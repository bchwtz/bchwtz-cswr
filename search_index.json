[["index.html", "Computational Statistics Hello World!", " Computational Statistics Benjamin Buchwitz 2022-05-13 Hello World! These are the course notes for an introductory course in the statistical programming environnement R. The course is called “Computational Statistics with R” and focusses on programming first and then introduces more statistical examples step by step. Prior programming knowledge is not required to sucessfully complete the course as every concept is being introduced along the way. However it is recommended that you attended at least one course in undergraduate statistics. To provide a hands on experience and to keep the learning pace high, we concentrate on applying statistical methods and do not in depth introduce the theoretical concepts again. "],["installation.html", "Installation", " Installation To get the most out of this course it is strongly suggested that you install R and RStudio. Programming as well as statistical analysis is learned by doing, rather than just by reading the provided examples. To actively run the provided code on your personal computer you can follow the installation steps below. Both, R and RStudio are available for Windows, Mac OS and Linux and should work on almost every computer. R Language (Base System) https://www.r-project.org/ R-Studio (IDE) https://www.rstudio.com/ "],["intro.html", "Chapter 1 The Basics", " Chapter 1 The Basics All of my friends who have younger siblings who are going to college or high school - my number one piece of advice is: You should learn how to program. – Mark Zuckerberg "],["r-and-the-r-project.html", "1.1 R and the R-Project", " 1.1 R and the R-Project R is a language and an environment for statistical computing and graphics. It is a GNU project that is similar to the S language and environment, which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and his colleagues. R can be considered a different implementation of S. There are some important differences, but much code written for S runs unaltered under R. R provides a wide variety of statistical (linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology and R provides an Open Source way to participate in that activity. "],["r-system-and-rstudio.html", "1.2 R System and RStudio", " 1.2 R System and RStudio RStudio is an integrated development environment (IDE) as well as a graphical user interface (GUI) for R. It includes a console, a syntax-highlighting editor that supports direct code execution as well as tools for plotting, debugging and workspace management. When you start RStudio for the first time it should look like this: Figure 1.1: RStudio The shown version of RStudio is 1.1.x, which was the latest one available when this script was written and revised. If you have not changed any of the default settings, your RStudio should look almost the same. The most important section is the big one on the left (1). This is the R console that connects your development environment with the R core. RStudio makes it simple, interactive and very convenient to use R. You just type your command into the bottom line of the console tab (1), hit Enter and your computer will execute it. Commands are your way to talk to the computer and that is why the console is often also called the command line interface (CLI). The &gt; is called the prompt and indicates that R is ready and waiting for your commands. If you type 1 + 2 into the command line, you should see the following after hitting Enter: 1 + 2 #R&gt; [1] 3 As we want reproducible results we need a more convenient and supporting place to work with our code. Scripts allow us to be an efficient programmer. Therefore you should bundle and work with your code in the Script Pane (2) and stop typing in the console. You can easily start a new script by clicking File \\(\\to\\) New File \\(\\to\\) R Script in the main menu bar. Or you hit Control + Shift + N (Windows) on the keyboard to open one automatically. As you can see R responds to your request with more than just the result. The [1] on the left side indicates that the line in which the response is written starts with the first value. This becomes quite useful when your commands become more complex so that R returns multiple values. The colon operator (:) creates a sequence between two given numbers by incrementing the first number by 1 (or -1 respectively if the bigger number is provided as the first argument) until the second number is reached. If we want R to give back the integers between 1 and 100 we simply type1:100 and get exactly what we have asked for: 1:100 #R&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #R&gt; [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #R&gt; [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 #R&gt; [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #R&gt; [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 #R&gt; [91] 91 92 93 94 95 96 97 98 99 100 The numbers in brackets now indicate that the first line starts with the first value of R’s answer to your command and the second line starts with a higher value (depending on the size of your RStudio window). When handling matrices and bigger vectors in your programs you will greatly appreciate this small feature, but for now, we can ignore it. To make the next lines of R code more readable this formatting is only referred to when I want you to notice them. The same applies for the &gt; at the beginning of each line. Therefore the following Code excerpts will not contain these signs, which will make it also much easier for you to copy them into your own console. To clearly differentiate between commands and responses I will indicate R’s output with R&gt; and color them differently so that our first example will look like this: 1 + 2 #R&gt; [1] 3 "],["first-steps-r-as-calculator.html", "1.3 First Steps: R as Calculator", " 1.3 First Steps: R as Calculator R is a powerful programming language for statistical algorithms and therefore you can also use it as a (very powerful) calculator for nearly every case you can imagine including calculations on arrays and complex matrix operations. Let us start with some basic examples: 3 + 5 * 2 #R&gt; [1] 13 R handles the sharp (hash) # symbol as an indicator for comments and in fact does not interpret it. You can and should (!) use it to comment on your code to express your thoughts and what you were trying to do while programming because it is necessary to understand your code. But let’s stay focused: 4 * -(5 - 1,5) #R&gt; Error: &lt;text&gt;:1:12: unexpected &#39;,&#39; #R&gt; 1: 4 * -(5 - 1, #R&gt; ^ Errors occur while programming. One of the major sources of errors is the syntax of a language. Syntax simply describes the required structure for commands. Every command has to follow certain structural rules in order to make R understand what you want it to do. In this case, R is designed to accept the point as decimal separator, otherwise, it will only prompt an error message while providing a short hint to the problem. You can then just retype the command or you can use the arrow-up key \\(\\uparrow\\) on your keyboard to access your last command and change it accordingly: 4 * -(5 - 1.5) # The changed command works. #R&gt; [1] -14 As stated before, with R you can do everything your calculator can do, from simple mathematical operations to more complex tasks. Some well known and often used functions are: 10 / 2 # Simple division #R&gt; [1] 5 5^2 # Exponentiation #R&gt; [1] 25 pi # Mathematical constant Pi #R&gt; [1] 3.141593 sin(pi / 6) # Trigonometry #R&gt; [1] 0.5 sqrt(64) # Square root #R&gt; [1] 8 sqrt(as.complex(-64)) # Complex square root #R&gt; [1] 0+8i log(1) # Natural logarithm (base e) #R&gt; [1] 0 sum(1:6) # Sum of numbers from 1 to 6 #R&gt; [1] 21 1:6 + 1:6 # Element-wise addition #R&gt; [1] 2 4 6 8 10 12 prod(1:3) # Product of multiple elements #R&gt; [1] 6 Trying to calculate something that is not defined, leads to a warning. If you try dividing by zero, R reminds you that it has no answer to this question as well as any other calculator. 0 / 0 # Not defined, NaN = Not a Number #R&gt; [1] NaN "],["objects-and-data-structures.html", "1.4 Objects and Data Structures", " 1.4 Objects and Data Structures You may have heard of objects or the concept of object-oriented programming. R is also considered to be object-oriented, which basically means that we have to deal with objects if we want to use it. You can imagine an object as a name that you can use to refer to stored data. Of course, there are rules for naming an object. Generally speaking, every symbol is possible as long as it consists of alphanumerical characters, numbers, and dots. Special characters like $, @, +, -, / or * are not allowed within names. To assign values to an object, we use the arrow &lt;-. Whenever you tell R to handle commands containing your object, it will replace it with the data saved inside. The most basic form of an object is a variable, containing only one value: foo &lt;- 5 # Arrow consists of &lt; (less than) and - (minus) foo #R&gt; [1] 5 RStudio will support you with multiple features that make your life as a programmer easier. If a new variable is generated, it will show you a sneak peek of its contents in the environment pane on the upper right corner. The environment pane (3) will show you all objects, you have created since starting RStudio. Variable or object names are case sensitive, which means that R differentiates between small and capital letters. Two objects with the same name, but different capitalization are regarded as two different objects: foo &lt;- 5 # Variable assignment Foo &lt;- 3 FOO &lt;- 1 foo + 1 # Variable evaluation #R&gt; [1] 6 Foo + 1 #R&gt; [1] 4 FOO + 1 #R&gt; [1] 2 As a practical proof that different capitalization produces different objects, we can also use R to compare the created objects. You can simply do this by using == as a comparison operator. Watch out, do not forget the second equality sign. When using only one, your command will be interpreted as an assignment and has, therefore, the same effect as using &lt;-. Although one equality sign can be used as an assignment operator, it should be avoided due to better readability of your code: foo == foo # Check for equality (==) #R&gt; [1] TRUE Foo == FOO #R&gt; [1] FALSE foo = FOO # Do not use &quot;=&quot; to assign values! foo # Using &quot;&lt;-&quot; is much better to read. #R&gt; [1] 1 Using variables in mathematical operations does not change their values. To change stored data, you have to overwrite it. R will do this without asking for your permission. To remove a variable completely from the memory you can use the function rm(). foo &lt;- 5 # Assign a value foo # Display assigned value #R&gt; [1] 5 foo &lt;- 20 # Overwrite the value foo #R&gt; [1] 20 rm(foo) # Remove/delete variable foo 1.4.1 Data Structure: Vectors Of course, we are not limited to storing simple numbers in R. In fact, there is no specialized data type for storing single numbers as in other programming languages. Stored single numbers are often called atomic vectors or one-element vectors. However, vectors can easily be used to store multiple numbers. foo &lt;- 1:6 # Create vector foo foo #R&gt; [1] 1 2 3 4 5 6 vec &lt;- c(0.5, 1, 2) # Combine values into a vector vec #R&gt; [1] 0.5 1.0 2.0 foo - 1 # Using vectors in calculations #R&gt; [1] 0 1 2 3 4 5 foo * foo # Equal to foo? #R&gt; [1] 1 4 9 16 25 36 # Watch out when using vectors foo + 1:3 # with different lengths! #R&gt; [1] 2 4 6 5 7 9 foo + 1:5 # Recycling #R&gt; Warning in foo + 1:5: longer object length is not a multiple of shorter object #R&gt; length #R&gt; [1] 2 4 6 8 10 7 As you may have noticed R does not follow the rules of matrix multiplications but uses element-wise execution instead. This means that R is applying the requested operation to each and every element of the vector. When multiplying two vectors of the same length R will always multiply the first element of the first vector with the first element of the second vector, then the second element of the first vector with the second element of the second vector and so on. When given two vectors of different length R will repeat the shorter vector until it matches the length of the longer one. Note that the shorter vector will only be repeated within this single calculation, R does not change the vector itself. If the length of the shorter vector is not a natural multiple of the length of the long one, R will perform a calculation by concatenating the shorter vector multiple times, until it reaches the same length as the longer one, and issues a warning with the calculation results. This behavior is called vector recycling. Element-wise operations are very useful, especially when it comes to handling data with a lot of observations. You can easily apply calculations which will only affect elements from the same observation when using vectors with the same length. If you want to know the length of a vector you can use the function length(). 1.4.2 Data Structure: Matrices Even though element-wise operations are useful sometimes you need matrix-algebra in your functions and R does of course support this, but there are special operators you have to use. There are different operators for every case you may need. You can, for example, calculate the inner product with the %*%-Operator and the outer product with the %o%-Operator. vec &lt;- 1:3 # Create vector vec length(vec) # Ask for length of vec #R&gt; [1] 3 vec2 &lt;- vec * vec # Element-wise multiplication vec2 #R&gt; [1] 1 4 9 scalar &lt;- vec %*% vec # Calculate inner product scalar #R&gt; [,1] #R&gt; [1,] 14 mat &lt;- vec %o% vec # Calculate outer product, same as vec %*% t(vec) mat # R indicates rows and #R&gt; [,1] [,2] [,3] #R&gt; [1,] 1 2 3 #R&gt; [2,] 2 4 6 #R&gt; [3,] 3 6 9 # columns for you When manipulating data, it may be useful to access specific columns, rows or elements of a matrix. You can simply use the row and column indices in brackets object[row, column] to access the desired data. Leaving one spot blank advises R to return all elements in this dimension. If you need to know the \\(n \\times m\\) dimensions of the matrix you can use dim() to get to know what you need. dim(mat) # Request matrix dimensions #R&gt; [1] 3 3 mat[1 , 1] # Accessing first element in first row #R&gt; [1] 1 mat[2 , ] # Second row of a matrix #R&gt; [1] 2 4 6 mat[ , 3] # Third column of a matrix #R&gt; [1] 3 6 9 The most common way of creating matrices is using the command matrix which takes an arbitrarily long vector and input. Obviously, you need to define the number of rows and columns for the matrix. By default, all the elements from the vector are filled into the matrix column by column. To change the standard behavior we can set the argument byrow = TRUE. matrix(1:9, nrow = 3, ncol = 3) # Filling matrix by column #R&gt; [,1] [,2] [,3] #R&gt; [1,] 1 4 7 #R&gt; [2,] 2 5 8 #R&gt; [3,] 3 6 9 matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE) # Filling matrix by row #R&gt; [,1] [,2] [,3] #R&gt; [1,] 1 2 3 #R&gt; [2,] 4 5 6 #R&gt; [3,] 7 8 9 You can also construct matrices out of vectors manually using rbind() for row vectors and using cbind() for column vectors. Transposing a matrix can be done with t() and calculating the determinant with det(): rowMat &lt;- rbind(vec, vec, vec) # Matrix with vec as row vector rowMat #R&gt; [,1] [,2] [,3] #R&gt; vec 1 2 3 #R&gt; vec 1 2 3 #R&gt; vec 1 2 3 colMat &lt;- cbind(vec, vec, vec) # Matrix with vec as column vector colMat #R&gt; vec vec vec #R&gt; [1,] 1 1 1 #R&gt; [2,] 2 2 2 #R&gt; [3,] 3 3 3 det(colMat) # Determinant of colMat #R&gt; [1] 0 As we have already talked about comparison operators and element-wise operations it is not surprising that there are similar operations for matrices and matrix elements. rowMat == t(colMat) # Element-wise matrix comparison, transpose colMat #R&gt; [,1] [,2] [,3] #R&gt; vec TRUE TRUE TRUE #R&gt; vec TRUE TRUE TRUE #R&gt; vec TRUE TRUE TRUE identical(rowMat, t(colMat)) # Matrix comparison #R&gt; [1] TRUE The function all.equal() seems to be an alternative to identical() as it returns the same value for our matrix comparison. But of course, there is a reason that both of these functions exist. This has to do with the internal representation of numeric values and we are going to talk about this in more detail in the next chapter. 1.4.3 Data Structure: Strings Data is more than just numbers and so there is a need for a structure specialized in storing sentences, words and characters, which are called strings. You can create string variables in the same way as vectors and matrices, but you need to place single or double quotation marks at the begin and end of each string. x &lt;- c(&quot;We&quot;,&quot;love&quot;) # Create variable containing two strings. x #R&gt; [1] &quot;We&quot; &quot;love&quot; y &lt;- &quot;statistics&quot; # Variable with only one string. y #R&gt; [1] &quot;statistics&quot; length(y) #R&gt; [1] 1 The universal length() function can also be used with string variables and it will give you the number of strings it contains and not the numbers of characters. There are many other commands for string variables e.g. dealing with putting strings together or taking them apart. The squared brackets we got to know when discussing vectors can also be used to access elements of a string. nchar(y) # Number of characters in the variable #R&gt; [1] 10 nchar(x) # Number of characters per element #R&gt; [1] 2 4 sentence &lt;- c(x, y) # Combining strings in a variable sentence #R&gt; [1] &quot;We&quot; &quot;love&quot; &quot;statistics&quot; sentence[3] # Access the third element #R&gt; [1] &quot;statistics&quot; 1.4.4 Data Structure: Lists and Data Frames Lists allow more complex combinations of different data types. You can see a list as a vector which can contain elements of different data types without losing information about their nature. Every sub-element in a list has an own name, like any variable or any object and can be accessed using the dollar sign $ as operator. list &lt;- list(num=1:3 , strg= &quot;abc&quot;) # Create a list with 2 elements list # Display the list #R&gt; $num #R&gt; [1] 1 2 3 #R&gt; #R&gt; $strg #R&gt; [1] &quot;abc&quot; list$num # Display element num within list #R&gt; [1] 1 2 3 str(list) # Overview over structure of the list #R&gt; List of 2 #R&gt; $ num : int [1:3] 1 2 3 #R&gt; $ strg: chr &quot;abc&quot; The str() command is a comfortable way of getting a quick overview of the data structure and it’s containing values of an object. It is a generic function and also works with other data structures such as simple vectors, much more complex data frames or even functions. Data frames are a specific incarnation of lists and ideal when working with larger data sets, as they allow us to combine all sorts of data in them, which won’t work well in a normal list and won’t work at all in a matrix. Each sub-element of the data frame is handled as its own column in our matrix-like structure of data. All sub-elements need to have the same length and each sub-element needs to be consistent in terms of its data type, which results in a data frame with exactly one datatype per column and a fixed number of rows for all columns. df &lt;- data.frame( Name=c(&quot;Homer&quot;,&quot;Marge&quot;,&quot;Bart&quot;,&quot;Lisa&quot;), Age=c(38 , 34 , 10 , 8), Sex=c(&quot;m&quot;,&quot;f&quot;,&quot;m&quot;,&quot;f&quot;), stringsAsFactors = FALSE) df # Display the data frame #R&gt; Name Age Sex #R&gt; 1 Homer 38 m #R&gt; 2 Marge 34 f #R&gt; 3 Bart 10 m #R&gt; 4 Lisa 8 f df$Name # Display only column with names #R&gt; [1] &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Lisa&quot; "],["functions.html", "1.5 Functions", " 1.5 Functions 1.5.1 Using Functions Programming reveals its value when bringing data and algorithms together. We already briefly talked about the different data storing concepts, now it is time to talk about “containers” for our algorithms, which we are going to call functions. We already encountered some basic functions like sum(), sqrt() and det(), which allow us to perform basic tasks with our stored data. The base version of R includes many other and much more powerful functions, even more can be added by installing extensions, called packages, from the internet. And of course, we can write functions on our own. A function is always defined by its name followed by a list of arguments or parameters. Most functions return a value, which can be a number, a matrix or a list. Using a function is pretty straightforward: just type in the name followed by parentheses with the data you want the function to use. Arguments can be numbers, vectors or even the output from other functions. This ability to handle the output of a function directly as an input for another function allows us to nest functions, which is also called linking. round(pi) #R&gt; [1] 3 mean(df$Age) #R&gt; [1] 22.5 round(mean(df$Age)) #R&gt; [1] 22 Functions can handle multiple arguments as input. This allows us to specify what we want and handle different cases within the same function. But first we need to know the different arguments a function can accept - and of course, there is a function to find out about that. This function is called args(). As input, it accepts the function that you want to know the arguments of. args(round) #R&gt; function (x, digits = 0) #R&gt; NULL round(pi, digits=2) #R&gt; [1] 3.14 round(pi, length=2) #causes an error #R&gt; Error in round(pi, length = 2): unused argument (length = 2) When using arguments not listed in the output of args(), it is quite obvious that the function can’t handle such input and produces an error. All listed arguments can be used just by assigning a value while calling the function, as it’s done above with the argument digits=2. The function works also when we do not provide a value for digits and that is why this argument is called optional. Optional arguments always have a default value like digits=0 for the function round(). Therefore, only the input to be rounded x is mandatory, so that you can not call round without providing a value for it. The majority of functions is able to accept multiple arguments as input and there are multiple ways to pass them over, meaning you can explicitly name the arguments and assign the data to them using the =. This allows you to completely mix up the order of the arguments. Alternatively, you can pass them to the function in the right order without directly naming or addressing them. In general, it is good practice to name each argument as this keeps your code clean and understandable. num &lt;- c(0.5, 0.25, 0.125, 0.0625) # Some arbitrary numbers round(x = num, digits = 2) # Function-call with named arguments #R&gt; [1] 0.50 0.25 0.12 0.06 round(digits = 2, x = num) # Function-call with mixed up named arguments #R&gt; [1] 0.50 0.25 0.12 0.06 round(num,2) # Function-call with unnamed arguments #R&gt; [1] 0.50 0.25 0.12 0.06 round(2, num) # Watch out! Using unnamed arguments #R&gt; [1] 2 2 2 2 # can lead to unintended results. For some reason, people who are new to programming, feel the need to clean up the console window. As there is no real-world need for this, there is not an easy to remember command, but if you are one of those people who like it really tidy, the next example is for you: cat(&quot;\\014&quot;) # Removes all content from console window In RStudio you can also use CTRL + L when you have placed the cursor in the console pane to clear the output area or click Clear in the global environment window. 1.5.2 Writing Functions Programming is applied problem-solving. To start with our own function, we need a problem to solve with our newly learned R skills. And here it is: Write a function named roll() that simulates rolling a pair of dice. To implement rolling a dice you may want to use the function sample() as the heart of your program. At first, we need the numbers located on a die. Luckily, we already know how to produce them, so this isn’t a problem at all. Next thing is to get familiar with the sample() function. We can easily do this by using args() and playing a little bit around. num &lt;- 1:6 args(sample) #R&gt; function (x, size, replace = FALSE, prob = NULL) #R&gt; NULL sample(num) #R&gt; [1] 1 5 2 4 3 6 sample(num, size = 1) #R&gt; [1] 6 sample(num, size = 2) # Argument size allows us to adjust how #R&gt; [1] 1 5 # many times the dice are rolled Using sample() only with our vector of numbers does not lead to the correct result, but the argument size allows us to adjust for the number of times a dice is rolled. We obviously need two returned values as we need one value per die, so size=2. When you run this line of code over and over again you may notice that the two returned values are never the same. If we are trying to increase the number of rolled dice, R points us directly to the reason: # sample(num, size = 7) #causes an error sample(num, size = 2, replace = TRUE) #R&gt; [1] 6 5 The behavior of the sample()-function takes us directly back to one of our first statistics lessons. Every time the function returns a value it removes it from the population/sample, so it can not be returned again. If we set the option replace to TRUE the previously withdrawn number is placed back in the sample and can be drawn again. Therefore this option allows us to create independent, random samples, which is exactly what we want. We just solved our first programming problem! Now we just need to wrap our code in a function so that we can call it using roll(). For this we need the function function(). my_function &lt;- function() { } # This is how a function is defined # We can simply put our working example in the function constructor, # give it a name and execute everything by calling the given name. roll &lt;- function() { num &lt;- 1:6 dice &lt;- sample(num, size = 2, replace = TRUE) return(dice) } roll() # We can now use our function #R&gt; [1] 1 1 The code between the parentheses is called the body of a function. The complete code runs when you require R to execute your function. You can see how I indented these lines. This does not affect R’s behavior but makes the code a lot more readable. R ignores every blank line and space, so you can use them to structure your code. The return() at the end of the function advises R to explicitly give back a value. Code that does not produce an output on the console by running it line by line is called a silent function. If you perform a calculation that also produces an output on the console it will be automatically returned if its the last line in your function, e.g. sum(dice) instead of return(dice) would return the combined result of the two rolled dices. Let us have a look at an overview of all the different parts of a function: Figure 1.2: Function Constructor and Parts of a Function 1.5.3 Solving Problems Our small roll()-function already has most of the described parts of the function constructor except for default values - in fact, it does not require any values to run the function, which makes it very inflexible. While programming, it is always better to solve general problems instead of very narrowly defined cases. So let’s adjust our problem description a little and produce a program that is more flexible. Write a function named roll() that simulates rolling a desired number of dices and allows the user to adjust the number of sides on the rolled dice. To solve this exercise we can use our existing roll()-function and modify it to meet the desired criteria. All we have to do is define the dependencies of the sample()-function in the body arguments of the function constructor. While doing this we can also define useful default values, so that we can still use the function without arguments. Let us use rolling a six-sided pair of dice for the default values. Our improved roll()-function now looks like this: roll &lt;- function( num = 1:6 , rolls = 2) { dice &lt;- sample(num, size = rolls, replace = TRUE) return(dice) } This function is now much more general and usable for a lot of different cases. Let’s have a brief look at how to use it. To show that the function delivers the same value when we call it with arguments and default arguments, we have to fix the random number generator. R uses this random number generator in the background to pick a value from our sample and will, therefore, deliver different results each time we call our function. Luckily we can lock the random number generator for a single execution using the set.seed() command with a freely selected numeric value as argument. Every time a routine relies on the random number generator and the seed is set to the same value R will produce the same result. set.seed(1) # Fix the Random Number Generator roll() # Execute modified function #R&gt; [1] 1 4 set.seed(1) roll(num = 1:6 , rolls = 2) # Function call with default Arg. #R&gt; [1] 1 4 # It is easy to increase the number of rolls or the number of sides # of the rolled dice using the argument section of the function. roll(num = 1:18 , rolls = 5) #R&gt; [1] 7 1 2 11 14 To execute multiple lines of code at once you can simply select them and hit the Run in the upper right corner of RStudios script pane. "],["packages.html", "1.6 Packages", " 1.6 Packages 1.6.1 Packages with Data Of course, we don’t need to invent everything ourselves. R comes with tons of great functions we can use for our analysis. One of the most important tasks in statistics is to perform a linear regression with one dependent and one or more independent variables. Load the dataset wage1 into R and use the lm()-function to fit a linear model to the data and calculate the effect of education on wage in the simple bivariate case. Summarize and interpret your findings considering that the data was recorded back in the 1970s in the USA. Naturally, the first step is to load the required data set into R. Luckily the data is simply available as a package that can be installed with install.packages(\"wooldridge\"). After installation and loading the package with library(wooldridge) we can see the whole documentation including variable descriptions by typing ?wage1. library(wooldridge) # Load the dataset dim(wage1) # Dimensions of the table #R&gt; [1] 526 24 names(wage1) # Variable names in the dataset #R&gt; [1] &quot;wage&quot; &quot;educ&quot; &quot;exper&quot; &quot;tenure&quot; &quot;nonwhite&quot; &quot;female&quot; #R&gt; [7] &quot;married&quot; &quot;numdep&quot; &quot;smsa&quot; &quot;northcen&quot; &quot;south&quot; &quot;west&quot; #R&gt; [13] &quot;construc&quot; &quot;ndurman&quot; &quot;trcommpu&quot; &quot;trade&quot; &quot;services&quot; &quot;profserv&quot; #R&gt; [19] &quot;profocc&quot; &quot;clerocc&quot; &quot;servocc&quot; &quot;lwage&quot; &quot;expersq&quot; &quot;tenursq&quot; As we can see the whole data set includes 526 observations and 24 variables. To get a first impression of the data we can type View(wooldridge::wage1). This will produce a viewer tab which can be used to view and inspect the data. Although it looks like an editable spreadsheet, the data can only be viewed and not changed. If you want to modify data you have to go back to the script view or command line and use commands to do this. If you are familiar with tools to handle spreadsheets like Excel you may think of quickly editing the data set over there and return to R afterward. If this came to your mind, just stop thinking about it now! R is much more powerful and way quicker than Excel and its lookalikes, so there is really no reason to chop up your workflow. As we will only calculate a simple bivariate model we don’t need all the data, so we extract what we need from the big set. In our simple case, these are the columns wage containing the hourly average wages in USD of the interviewed individuals and educ their respective years of education. So we basically have to extract the first two columns from the data set. wage &lt;- wooldridge::wage1$wage # Extract wage data (hourly wage in $) education &lt;- wooldridge::wage1$educ # Extract education data (education in years) # Initial data inspection summary(wage) # Summarize the variable wage #R&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #R&gt; 0.530 3.330 4.650 5.896 6.880 24.980 summary(education) # Summarize the variable education #R&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #R&gt; 0.00 12.00 12.00 12.56 14.00 18.00 Extracting the needed columns from the data frame is easy. We already discussed the $-Operator, which allows us to address named parts of an object. After extracting the data and giving the variables comprehensive names, we should gain a condensed overview of what we are using for our analysis. The summary() command is a generic function which works for many types of data and responds with a useful summary depending on the input. In our case summary() returns typical descriptive measures. As we now have a first feeling of the data, we can construct our linear model using the lm()-function. lm(wage ~ 1 + education) # Perform bivariate regression #R&gt; #R&gt; Call: #R&gt; lm(formula = wage ~ 1 + education) #R&gt; #R&gt; Coefficients: #R&gt; (Intercept) education #R&gt; -0.9049 0.5414 When simply executing the lm()-function R only returns the parameter estimates. That is far too less to evaluate if the model is appropriate or if the estimated effects are significant. We can use the summary()-function again to gain a deeper understanding of what we have calculated and how our model looks like. For convenience and re-usability it makes sense to store the results in an own object. model.uni &lt;- lm(wage ~ 1 + education) # Store results in variable summary(model.uni) # Summarize the fitted model #R&gt; #R&gt; Call: #R&gt; lm(formula = wage ~ 1 + education) #R&gt; #R&gt; Residuals: #R&gt; Min 1Q Median 3Q Max #R&gt; -5.3396 -2.1501 -0.9674 1.1921 16.6085 #R&gt; #R&gt; Coefficients: #R&gt; Estimate Std. Error t value Pr(&gt;|t|) #R&gt; (Intercept) -0.90485 0.68497 -1.321 0.187 #R&gt; education 0.54136 0.05325 10.167 &lt;2e-16 *** #R&gt; --- #R&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #R&gt; #R&gt; Residual standard error: 3.378 on 524 degrees of freedom #R&gt; Multiple R-squared: 0.1648, Adjusted R-squared: 0.1632 #R&gt; F-statistic: 103.4 on 1 and 524 DF, p-value: &lt; 2.2e-16 The summary view of the regression object model.uni gives us much more information than the lm()-function itself. This allows us to evaluate and interpret the model parameters as we have learned in our basic undergraduate statistic courses. 1.6.2 Packages with Functions There are thousands of functions in the R core, but sometimes this is not enough and you have the need to expand R’s functionalities. As you are not the only programmer out there many of the required additional functions already exist, written by users like you, professionals or professors all around the world. And they are giving them to you for free so that you can use them for any purpose you want. So before starting a new programming project always do proper research and look at what is already out there. Results from statistical works and analysis are often displayed using graphical tools. Producing plots of data to gain a quick aggregated overview of the data or using graphics to support your results is common, needed and useful. R is already equipped with tools to generate plots like plot() and hist() and RStudio makes them easy to use and comfortable to handle their outputs. An additional option that aims to produce strong, nice looking plots with less effort than the standard R toolset is the package ggplot2. Their creators state that the package “takes care of many of the fiddly details that make plotting a hassle (like drawing legends)”. It seems to be worth to add ggplot2 to our portfolio. We are looking at some basics and produce some nice plots here. But first, we need to install the package. As long as you are connected to the Internet it is quite easy to install new packages using the command line: install.packages(&quot;ggplot2&quot;) # Installing packages requires # a working internet connection That’s it, already. R takes care of the rest. It will visit the website, download the package and install it with all dependencies automatically and report the progress in the console. If you already know the name of a package and want to install it, you can do this by simply replacing the text in quotation marks. If you don’t know the packages name - we will discuss some resources besides your favorite search engine to find useful packages and additional help later. Before you can use the power of a freshly installed package you have to advise R to load the package (even if it is already installed). You can do this using the library()-function. If you are trying to execute a command from the package before loading it R will respond with an error. library(&quot;ggplot2&quot;) # Loads an already installed package into memory One of ggplot2’s most powerful functions is the ability to create quick (but nice looking) plots using qplot(). This is a generic function which produces its output dependent on its input, just like summary(). If you give qplot() a single vector it will produce a nice histogram, if you give it two vectors of equal length it will create a scatter plot. sample &lt;- rnorm( 1000 ) # Generate 1000 norm. dist. numbers head(sample, 10) # Preview the first 10 numbers #R&gt; [1] -1.539950042 -0.928567035 -0.294720447 -0.005767173 2.404653389 #R&gt; [6] 0.763593461 -0.799009249 -1.147657009 -0.289461574 -0.299215118 qplot(sample , binwidth = .1) # Plot generated sample as histogram As you can see, it is very fast forward to generate nice histograms out of single vectors. The argument binwidth = .1 defines the width of each cluster to aggregate the generated values into pillars. As you may have noticed I have not written 0.1. Lazy programmers (and who isn’t?) can get rid of the leading zero and start floating point numbers with the dot. Please notice that your histogram may look a little different as rnorm() also relies on the internal random number generator and of course you can use the function hist() instead to produce a similar plot. We will dive deeper into random numbers and distributions in one of the following chapters. Let’s try out what happens if we give qplot() two vectors. x &lt;- seq(-5, 5, by = .1) y &lt;- x^2 qplot(x,y) # Plot using ggplot2-package plot(x,y) # Plot using R&#39;s core function "],["getting-help.html", "1.7 Getting Help", " 1.7 Getting Help 1.7.1 Integrated Help System The flexible package system has a lot of advantages and puts the mind power of world-leading data scientists directly to your fingertips. When installing a new package or adding a new function which has been developed either by the R Core Team or by another developer you need to use the documentation to get used to the new functions, what they are doing in detail, how the package is used and what overlaps and problems might occur. This is exactly the reason why R has a build in help system. You can get useful information about every function you have at hand. Just place the question mark in front of the function name and hit enter or use the more formal function help(). # This is how to help yourself and find out how new or existing # functions work. Always read the help page before asking someone! help(sin) # Load documentation for a function ?sin # Shortcut to help() help(package=ggplot2) # Information about whole package A help page consists of different sections, each discussing a special aspect of the function. While the exact parts of a help page vary due to different purposes of different functions the following list is not exhaustive for all functions, but you can expect to see at least the following sections: Description: Provides a brief overview of what the function actually does or what it was intended to do. This section allows you to quickly grasp if the function is useful for your intended case. Usage: Shows a very short function call, often just enough to see required arguments. It’s good to get easy functions to work, for more advanced applications look at the dedicated example section. Arguments: Explains the arguments used or required by a function and gives an overview of the needed datatypes or in what way an argument manipulates the behavior of a function. Details: Provides background information on the behavior of the function and often mentions or briefly explains the theoretical concept underlying the function. Examples: Often the second focal point after the description section. This section provides working code examples which show in more or less detail how the function could be used in practice. Especially the combination with other functions and creation of sample data is a good source of inspiration for your own projects. If you don’t know what the exact name of a function is you can use the help.search()-function to search for a keyword in the whole documentation. The question mark shortcut for this search function is ?? and it’s automatically suggested by R if it can’t find the function you were looking for. 1.7.2 Help on the Internet R has a gigantic global supporter and fan base. Therefore you can find a lot about solving special problems, handling errors or information about packages by simply searching the web. Sometimes it may be difficult to search something related to R as R is at least to some people also the \\(18^{th}\\) letter of the alphabet and this may be confusing for your favorite search engine. Here are a few starting points which may help you on your programming journey: www.rseek.org www.stackoverflow.com www.r-project.org/mail.html "],["exercises.html", "Exercises", " Exercises EX 1 Write a function that outputs your name on the command line and generates an output similar to the one shown below. Maximiliane Mustermann Solution - Click to expand! myname &lt;- function(x){ print(&quot;Maximilian Mustermann&quot;) } EX 2 Install the forecast package, load it and use one arbitrary function from the package. EX 3 Write a function that generates an output similar to the one shown below. Hello world! (And all the people of the world) EX 4 Write a function that generates an output similar to the one shown below. * *** ***** ******* ********* * EX 5 Write a function that takes two numbers as arguments and outputs their respective sum. "],["data-types.html", "Chapter 2 Data Types", " Chapter 2 Data Types There are only 10 types of people in the world: those who understand binary, and those who don’t. – Anonymous We already talked about different data structures like vectors, matrices and data frames we can use in R. Now it’s time to dig a little deeper and strengthen our understanding of more basic data organization concepts before we come back to data types, high-level data structures and how to work with real-life data in R. "],["fixed-and-floating-point-number-representation.html", "2.1 Fixed and Floating Point Number Representation", " 2.1 Fixed and Floating Point Number Representation Before we look at the actual data types lets get a bit technical and explore how computers deal with numbers. The numbers we normally use are called decimal numbers, which are basically numbers with a base of \\(b = 10\\). Thus, we can deconstruct any real number \\(x \\in \\mathbb{R}\\) into an (infinite) sum of powers to our base \\(b\\). Let’s look at an example: \\[ 42.125 = x = 4 \\cdot 10^{1} + 2 \\cdot 10^{0} + 1 \\cdot 10^{-1} + 2 \\cdot 10^{-2} + 5 \\cdot 10^{-3}\\] While numbers in the decimal system can be conveniently processed by humans, computers work differently. In order to store them in memory, we have to perform a base switch with our chosen number \\(x\\) and rewrite it in binary notation, where \\(b = 2\\) and therefore only the digits 0 (absence of electrical current) and 1 (presence of electrical current) exist. We know that \\(42.125 = 32 + 8 + 2 + 0.125\\), which leads to the following: \\[ 42.125 = x = 1 \\cdot 2^{5} + 0 \\cdot 2^{4} + 1 \\cdot 2^{3} + 0 \\cdot 2^{2} + 1 \\cdot 2^{1} + 0 \\cdot 2^{0} + 0 \\cdot 2^{-1} + 0 \\cdot 2^{-2} + 1 \\cdot 2^{-3} \\] When we write the digits down it becomes clear that \\(42.125_{b = 10}\\) equals \\(101010.001_{b =2}\\). The representation we just discussed is called fixed point representation. Computers use a very similar version to handle real numbers called floating point representation. This is a combination of storing the algebraic sign, numbers in the binary system and moving the ‘decimal’ point while storing how many digits it has been moved and storing the algebraic sign. After this deconstruction our chosen number \\(x\\) in the decimal system looks like this: \\[ 42.125 = x = (-1)^{0} \\cdot 4.2125 \\cdot 10^{1}\\] All we have to do now is convert everything in binary again. For compatibility purposes we also need to come up with a standard, meaning how many binary digits (bits) we want to use for the sign, the actual number, and the exponent. The usual convention is 32 bit for single precision numbers divided into 1 bit for the sign, 8 bits for the exponent and 23 bits for the actual number (called mantissa) the convention for double precision numbers 64 bit with 1 bit for the sign, 11 bits for the exponent and 52 bits for the mantissa. By only having a fixed amount of bits to form a number it should be obvious that using such a coding comes at a cost. This cost is usually precision. The value domain covers only a specific area and quickly comes to its limits when it comes to irrational numbers. Having a basic idea of how a computer codes decimal numbers it should be clear that there is the need for different data types in R, which means we can now start to look at the elementary data types our software environment provides. If you want a deep dive on pitfalls when it comes to floating point arithmetic you should have a look at the article called What every Computer Scientist should know about floating-point Arithmetic which is freely available. "],["numeric.html", "2.2 Numeric", " 2.2 Numeric Atomic vectors, which are the base element of all higher order data structures can be of different types or modes. The mode is a mutually exclusive classification of objects according to their basic structure. The atomic modes are numeric, complex, logical, factors and character. A single object, even if not atomic, can only have a single mode. You can look up the mode using the command mode(). The atomic mode numeric can be divided into two types, one for natural numbers, called integer and one for floating point numbers called double. As of now the reasons why these two cases were separated should be obvious - it is quite easy and memory efficient to store a natural number as we do not need an exponent and the whole set of conventions we discussed earlier. Contrary to this, it is quite complex to store a floating point number, which consumes - dependent on our desired precision - more bits and therefore more memory. Let’s have a quick look at what R has to offer here: vec &lt;- 1:5 mode(vec) #R&gt; [1] &quot;numeric&quot; typeof(vec) #R&gt; [1] &quot;integer&quot; sequence &lt;- seq(1 , 5 , by=.5) typeof(sequence) #R&gt; [1] &quot;double&quot; # Mixing integers and doubles will produce a vector of type double mixed &lt;- c(1:5 , seq(1 , 5 , by=.5)) typeof(mixed) #R&gt; [1] &quot;double&quot; Generally, R stores numbers as double and will use integers only when easily applicable or if you specifically force R to treat a number as an integer using the as.integer() command. A vector of type integer uses less memory than a vector of type double with the same length. The reason for this is the internal representation of numbers we discussed at the beginning of this chapter. "],["complex.html", "2.3 Complex", " 2.3 Complex R can handle complex numbers. It is doubtful that you will come across them when doing simple data analysis, but likely if you are performing more advanced calculations for e.g. in time series analysis. You can simply create a complex vector by adding an imaginary term to a real number. num &lt;- sqrt(as.complex(-64)) typeof(num) #R&gt; [1] &quot;complex&quot; num &lt;- 1 + 2i typeof(num) #R&gt; [1] &quot;complex&quot; is.complex(num) #R&gt; [1] TRUE Re(num) # Returns real part of a complex number #R&gt; [1] 1 Im(num) # Returns imaginary part of a complex number #R&gt; [1] 2 To perform more calculations or obtain more information when dealing with complex numbers you can also use Mod() for the modulus and Arg() for the argument of a given complex number. "],["logical.html", "2.4 Logical", " 2.4 Logical Logicals can hold the value TRUE or FALSE and are often the output of comparisons. To fulfill your need for typing efficiency when programming R allows you to abbreviate TRUE and FALSE by T respectively F. x &lt;- TRUE x #R&gt; [1] TRUE 1 &gt; 5 #R&gt; [1] FALSE bool &lt;- c(T, F, T, F, T) bool #R&gt; [1] TRUE FALSE TRUE FALSE TRUE sum(bool) # ATTENTION: Automatic conversion to 1 (T) and 0 (F) #R&gt; [1] 3 When using TRUE and FALSE in calculations they are automatically converted into their underlying numerical representation. We are going to talk about this phenomenon called coercion in detail in the respective chapter. When it comes to logical values R provides a lot of operators to compare and evaluate objects - these are called logical operators. Code Description Syntax &gt; Greater than a &gt; b &gt;= Greater than or equal to a &gt;= b &lt; Less than a &lt; b &lt;= Less than or equal to a &lt;= b == Exactly equal to a == b != Not equal to a != b ! Logical negotiation (NOT) !a | OR (elementwise) a | b || OR (stepwise) a || b &amp; AND (elementwise) a &amp; b &amp;&amp; AND (stepwise) a &amp;&amp; b %in% Is element in group of elements a %in% c(a,b,c) xor() Exclusive or (XOR) xor(a,b) A lot of these logical operators can come in handy when structuring your code and using conditional statements to handle different events or cases in your program. If you don’t know how they are working you can just try them out while giving a and b different TRUE or FALSE values or you can construct a truth table in the following way. values &lt;- c(NA, FALSE, TRUE) names(values) &lt;- as.character(values) outer(values, values, &quot;&amp;&quot;) # Truth table for AND #R&gt; &lt;NA&gt; FALSE TRUE #R&gt; &lt;NA&gt; NA FALSE NA #R&gt; FALSE FALSE FALSE FALSE #R&gt; TRUE NA FALSE TRUE outer(values, values, &quot;|&quot;) # Truth table for OR #R&gt; &lt;NA&gt; FALSE TRUE #R&gt; &lt;NA&gt; NA NA TRUE #R&gt; FALSE NA FALSE TRUE #R&gt; TRUE TRUE TRUE TRUE Operations using logicals can sometimes be tricky and seem to be wrong or trying to fool you. So pay attention when dealing with them and try to get a sense of how they are evaluated. Here is an example of a tricky evaluation. To understand the following lines you should know what the exclusive or (xor) does and what differentiates it from the normal or. Trying to come up with a line of code to construct a truth table for this in R may help to understand the following: xor(T, T) == T | T #R&gt; [1] TRUE xor(T,T) #R&gt; [1] FALSE T | T #R&gt; [1] TRUE FALSE == TRUE # This is obviously wrong #R&gt; [1] FALSE Although it seems as R is making an error here, it does not. It evaluates our line of code in a strictly logical order and this differs from the appearance of the equation. Here is how R evaluates the parts of our expression: # Due to the definition of XOR the expression has to be FALSE xor(T,T) == T|T #R&gt; [1] TRUE xor(T,T) # 1. Step #R&gt; [1] FALSE xor(T,T) == T # 2. Step #R&gt; [1] FALSE xor(T,T) == T|T # 3. Step #R&gt; [1] TRUE F|T # Aggregation (same as 3. step) #R&gt; [1] TRUE # If you want R to behave as it seems on first look you have # to use parentheses: xor(T,T) == (T|T) # Evaluates each side individually, then compares #R&gt; [1] FALSE The following code illustrates the difference between | and || and shows why using || as operator for the logical OR can come in handy sometimes. Remember that the logical OR returns TRUE if at least one element is TRUE. When the first element is TRUE, the result is independent of the second element. rm(x) # Make sure variable x does not exist TRUE | x # Element x does not exist, so R returns an Error! #R&gt; Error in eval(expr, envir, enclos): object &#39;x&#39; not found TRUE || x # Element x does not exist, but the result can be determined #R&gt; [1] TRUE # without actually touching the object and || ensures that # the execution is aborted as soon as possible. "],["character.html", "2.5 Character", " 2.5 Character Character vectors store pieces of text from a single character to whole sentences. You can easily create a character vector by putting text in quotes and R will handle the rest for you. char &lt;- &quot;TRUE&quot; typeof(char) #R&gt; [1] &quot;character&quot; num &lt;- &quot;3.14&quot; # num * 3 # This produces an error as.numeric(num) * 3 #R&gt; [1] 9.42 An often occurring mistake is confusing numbers imported as characters with numerics. The conversion comes in handy when handling data and it is good advice to check datatypes if your calculations with important data look suspicious or won’t work at all. "],["factors.html", "2.6 Factors", " 2.6 Factors Factors are helpful to represent nominally and ordinally scaled variables. 2.6.1 Unordered Factors x &lt;- factor(c(&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;,&quot;no&quot;)) x #R&gt; [1] yes yes no yes no #R&gt; Levels: no yes table(x) #R&gt; x #R&gt; no yes #R&gt; 2 3 unclass(x) # Levels are automatically generated in alphabetical order! #R&gt; [1] 2 2 1 2 1 #R&gt; attr(,&quot;levels&quot;) #R&gt; [1] &quot;no&quot; &quot;yes&quot; x &lt;- factor(c(&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;yes&quot;,&quot;no&quot;), levels = c(&quot;yes&quot;,&quot;no&quot;)) unclass(x) # By setting levels explicitly the internal order can be defined. #R&gt; [1] 1 1 2 1 2 #R&gt; attr(,&quot;levels&quot;) #R&gt; [1] &quot;yes&quot; &quot;no&quot; x[1] == x[3] # Nominally scaled variables can be check for equality #R&gt; [1] FALSE 2.6.2 Ordered Factors An example for an ordered factor are ordinally scaled variables, which are often found in questionnaires, like the likert scale. likert &lt;- factor(c(2,3,1,5,2,4,5,2,3,3), levels = c(1,2,3,4,5), labels = c( &quot;strongly disagree&quot;, &quot;disagree&quot;, &quot;don’t know&quot;, &quot;agree&quot;, &quot;strongly agree&quot;), ordered = TRUE) likert #R&gt; [1] disagree don’t know strongly disagree strongly agree #R&gt; [5] disagree agree strongly agree disagree #R&gt; [9] don’t know don’t know #R&gt; 5 Levels: strongly disagree &lt; disagree &lt; don’t know &lt; ... &lt; strongly agree likert[1] &lt; likert[2] #R&gt; [1] TRUE likert[1] &gt; likert[2] #R&gt; [1] FALSE likert[1] + likert[2] #R&gt; Warning in Ops.ordered(likert[1], likert[2]): &#39;+&#39; is not meaningful for ordered #R&gt; factors #R&gt; [1] NA likert[1] == likert[5] #R&gt; [1] TRUE "],["missing-and-raw-data.html", "2.7 Missing and Raw Data", " 2.7 Missing and Raw Data Besides the atomic data types, R supports two more types. A missing or undefined value is indicated by NA which stands for non-available. This is in fact not a real data type, R considers this type a logical value. A lot of functions support handling data with NAs in the set and provide different options to use the respective dataset anyway. Watch out to not mistake NA for the reserved term NaN, which indicates erroneous calculations. num &lt;- c(1 , 2 , NA , 4 , 5) num + 3 #R&gt; [1] 4 5 NA 7 8 is.na(num) #R&gt; [1] FALSE FALSE TRUE FALSE FALSE sum(is.na(num)) # Number of NAs in dataset #R&gt; [1] 1 which(is.na(num)) # Location/Index of the NA #R&gt; [1] 3 mean(num) # Mean of the data can not be calculated #R&gt; [1] NA mean(num , na.rm=T) # Omitting the NA for caluclating mean #R&gt; [1] 3 R also supports RAW vectors meaning data stored in hexadecimal notation. The hexadecimal system is a companion from our well-known decimal system and the binary system we talked about at the beginning of this chapter. Hexadecimal numbers are numbers with base \\(b = 16\\). This may get handy when reading in files in binary formats. To find additional information about how to work with the type RAW see the corresponding help pages with ?raw. raw(3) # Create empty raw vector of length 3 #R&gt; [1] 00 00 00 x &lt;- as.raw(15) # Convert number to raw x #R&gt; [1] 0f typeof(x) #R&gt; [1] &quot;raw&quot; "],["coercion-attributes-and-class.html", "2.8 Coercion, Attributes, and Class", " 2.8 Coercion, Attributes, and Class 2.8.1 Coercion We have already seen that R coerces data types sometimes automatically. That makes it possible to calculate the sum of a vector consisting of logical values, which basically tells you how many elements with value TRUE are present. R has strict rules on how it behaves when coercing data types. If a string is present in a vector everything will be converted to strings. When there are only logicals and numbers in a vector R converts the logicals to their numeric value, so that every TRUE becomes 1 and every FALSE becomes a 0. The main corresponding goal here is to not lose information but conserve it at the cost of memory requirements and compatibility. A graphical representation is shown in the following figure. Figure 2.1: Coercion Cycle and Coercion Rules Data frames and lists can handle multiple data types in the same structure, but there is a good reason not to mix everything up in a huge data frame and this reason is Math. Using only vectors containing a single type of data is a big advantage, as it is easy to perform mathematical operations using matrices and vectors which couldn’t be done with a mixed type data structure and as they are so easy to store in memory these operations are fast. 2.8.2 Attributes and Names Attributes are R’s interpretation of metadata. They can be attached to any type of object but won’t affect your calculation or other operations and they won’t be displayed when you display the object. That makes it very convenient to store a description or any additional information you want to handover with your object. Of course, you can access the attributes within functions which allows you to perform special tasks if a data has given attributes. Here is how you assign and display attributes. x &lt;- 1:5 attributes(x) # A simple numeric vector has no attributes #R&gt; NULL As you can see above, the result that is being returned is NULL, which is neither equivalent to 0 nor NA. NULL is short for the NULL-Pointer which is a computer scientists term for an empty set \\(\\{\\varnothing\\}\\). Every time R returns NULL it just wants to express that there is nothing there. description &lt;- list(Description = &quot;Simple numerical vector&quot;) attributes(x) &lt;- description attributes(x) #R&gt; $Description #R&gt; [1] &quot;Simple numerical vector&quot; Another form of adding descriptive information to your data is by using the names() function. This is useful for datasets and you can imagine this as a header for your data. This is by far the most common way to enrich data in R. With names it is the same as with other attributes. They won’t affect the behavior of the vector meaning you can still perform all calculations names(x) #R&gt; NULL names(x) &lt;- c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;) x #R&gt; one two three four five #R&gt; 1 2 3 4 5 #R&gt; attr(,&quot;Description&quot;) #R&gt; [1] &quot;Simple numerical vector&quot; x &lt;- x^2 x #R&gt; one two three four five #R&gt; 1 4 9 16 25 #R&gt; attr(,&quot;Description&quot;) #R&gt; [1] &quot;Simple numerical vector&quot; names(x) &lt;- NULL # To delete names set them to NULL x &lt;- unname(x) # Alternative way to remove the names x #R&gt; [1] 1 4 9 16 25 #R&gt; attr(,&quot;Description&quot;) #R&gt; [1] &quot;Simple numerical vector&quot; 2.8.3 Class A class in R is a property assigned to an object that describes the type of stored data in some way. Classes are used to control how generic functions like summary() behave. It is not a mutually exclusive classification. If an object has no specific class assigned to it, such as a simple numeric vector, its class is usually the same as its mode. If you transform data e.g. from a vector to a matrix, R will automatically change the class attribute. num &lt;- 1:10 class(num) #R&gt; [1] &quot;integer&quot; typeof(num) == class(num) #R&gt; [1] TRUE mod &lt;- lm(wage ~ 1 + education) # Regression from previous chapter class(mod) #R&gt; [1] &quot;lm&quot; When summarizing the objects mod with the linear model inside and the numeric vector num one can easily see that different outputs, dependent on the class of the objects, are produced. The same mechanism can also be used to define own generic functions and dynamically and conveniently control their behavior. summary(num) #R&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #R&gt; 1.00 3.25 5.50 5.50 7.75 10.00 summary(mod) #R&gt; #R&gt; Call: #R&gt; lm(formula = wage ~ 1 + education) #R&gt; #R&gt; Residuals: #R&gt; Min 1Q Median 3Q Max #R&gt; -5.3396 -2.1501 -0.9674 1.1921 16.6085 #R&gt; #R&gt; Coefficients: #R&gt; Estimate Std. Error t value Pr(&gt;|t|) #R&gt; (Intercept) -0.90485 0.68497 -1.321 0.187 #R&gt; education 0.54136 0.05325 10.167 &lt;2e-16 *** #R&gt; --- #R&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #R&gt; #R&gt; Residual standard error: 3.378 on 524 degrees of freedom #R&gt; Multiple R-squared: 0.1648, Adjusted R-squared: 0.1632 #R&gt; F-statistic: 103.4 on 1 and 524 DF, p-value: &lt; 2.2e-16 "],["exercises-1.html", "Exercises", " Exercises From April 5, 2022 EX 1 Write down a truth table for the exclusive or. EX 2 What does double mean in the context of computer science? EX 3 What is TRUE + TRUE? Why? EX 4 Create the following data structure. #R&gt; x y z #R&gt; 1 1 c TRUE #R&gt; 2 3 b FALSE "],["data-manipulation.html", "Chapter 3 Data Manipulation", " Chapter 3 Data Manipulation You might not think that programmers are artists, but programming is an extremely creative profession. It’s logic based creativity. – John Romero We already introduced data structures, namely vectors, matrices, lists, and data frames and performed some basic tasks with them. R supports, of course, many more interesting and advanced actions to manipulate data in any desired way. "],["vectors.html", "3.1 Vectors", " 3.1 Vectors We have already seen that we can access a subset of a vector using brackets. This option is far more powerful than shown before when combining the brackets with other operators. vec &lt;- 1:10 vec[5] # Show fifth element #R&gt; [1] 5 vec[1:3] # Show first to third element #R&gt; [1] 1 2 3 vec[-1] # Exclude first element #R&gt; [1] 2 3 4 5 6 7 8 9 10 vec[-length(vec)] # Exclude last element #R&gt; [1] 1 2 3 4 5 6 7 8 9 R provides handy commands like any() and all() to manipulate data and perform fast logic tests. any(vec &gt; 5) #R&gt; [1] TRUE all(vec &lt; 10) #R&gt; [1] FALSE all(vec[-length(vec)] &lt; 10) #R&gt; [1] TRUE You can also use brackets to filter data and advise R to only return specific values which match your filter criteria. vec[vec &gt; 5] # Brackets can hold expressions to filter data #R&gt; [1] 6 7 8 9 10 vec[vec &gt; 7] &lt;- 0 # Setting all observations &gt; 7 to zero vec #R&gt; [1] 1 2 3 4 5 6 7 0 0 0 subset(vec, vec &lt; 5) # subset() can also be used to filter data #R&gt; [1] 1 2 3 4 0 0 0 Using which() it is possible to perform actions on indices which can be used to remove values from a vector for example. which(vec &lt; 1) # Get indices of zeros in vec #R&gt; [1] 8 9 10 vec[which(vec &lt; 1)] #R&gt; [1] 0 0 0 vec.positive &lt;- vec[-which(vec &lt; 1)] # Remove zeros from vec vec.positive #R&gt; [1] 1 2 3 4 5 6 7 As in the real world, there are always multiple solutions to a problem. If you want to remove the zeros from the shown vector vec there are additional ways to receive the same result as for vec.positive in a more efficient, but not always more clear way. Make sure to understand the following solutions in addition to the one shown using which(). vec[vec &gt; 0] # Nice, clean and efficient #R&gt; [1] 1 2 3 4 5 6 7 subset(vec,vec &gt; 0) # Same result using a function #R&gt; [1] 1 2 3 4 5 6 7 vec[-vec &lt; 0] # Also right, but not clear or fast #R&gt; [1] 1 2 3 4 5 6 7 Let us have a detailed look at how the third and maybe confusing solution is derived. You should not use it as the others shown are more clear and way nicer, but it illustrates nicely how weird a solution can be achieved. -vec #R&gt; [1] -1 -2 -3 -4 -5 -6 -7 0 0 0 -vec &lt; 0 #R&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE vec[-vec &lt; 0] #R&gt; [1] 1 2 3 4 5 6 7 "],["matrices-and-arrays.html", "3.2 Matrices and Arrays", " 3.2 Matrices and Arrays Matrices support the same operations as vectors and of course, you can formulate ambitious filtering commands to extract or manipulate data. mat &lt;- matrix(1:25, nrow = 5) 10 %% 3 # Modulo operation #R&gt; [1] 1 mat %% 2 == 1 #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] TRUE FALSE TRUE FALSE TRUE #R&gt; [2,] FALSE TRUE FALSE TRUE FALSE #R&gt; [3,] TRUE FALSE TRUE FALSE TRUE #R&gt; [4,] FALSE TRUE FALSE TRUE FALSE #R&gt; [5,] TRUE FALSE TRUE FALSE TRUE mat[which(mat %% 2 == 1)] #R&gt; [1] 1 3 5 7 9 11 13 15 17 19 21 23 25 Matrices as known from undergraduate mathematical courses usually have two dimensions. Sometimes there is the need for more dimensions and of course, R supports us with a construct that is similar to a more dimensional matrix - this structure is called an array. If you already played around with other programming languages you may insist, that an array does not need to have three or more dimensions, which is absolutely right. You can imagine array as a generic term for a (numeric) n-dimensional data-structure. A vector, therefore, can be called a one-dimensional array while matrices can also be called two-dimensional arrays. If you store data in a three-dimensional structure - the array is normally called a cube, but of course, we are not limited to three dimensions. Cubes of higher dimensions are usually called Hypercubes or just referred to as arrays. To printout, our n-dimensional structure on the two-dimensional command-line R gives back the array in slices. The command to create an array is array(). A &lt;- array(c(1:9 , 11:19 , 21:29), dim=c(3 , 3 , 3)) A # Display array #R&gt; , , 1 #R&gt; #R&gt; [,1] [,2] [,3] #R&gt; [1,] 1 4 7 #R&gt; [2,] 2 5 8 #R&gt; [3,] 3 6 9 #R&gt; #R&gt; , , 2 #R&gt; #R&gt; [,1] [,2] [,3] #R&gt; [1,] 11 14 17 #R&gt; [2,] 12 15 18 #R&gt; [3,] 13 16 19 #R&gt; #R&gt; , , 3 #R&gt; #R&gt; [,1] [,2] [,3] #R&gt; [1,] 21 24 27 #R&gt; [2,] 22 25 28 #R&gt; [3,] 23 26 29 dim(A) #R&gt; [1] 3 3 3 When entering higher dimensions the complexity level increases drastically and may get confusing. During our data science journey, we mostly use two-dimensional structures and therefore won’t explore the world of n-dimensional arrays here. "],["lists.html", "3.3 Lists", " 3.3 Lists Lists are somewhat similar to arrays because a list can contain another list and so on. This makes lists slightly more complicated than matrices. If we access an element within a list with the []-Operator we get another list as output. To access the value of a list (within a list) we must use the double bracket [[]]-operator. The single bracket element tells you which subelement of a list is displayed and the double bracketed indices give you the specific element. L &lt;- list(5:10,c(&quot;C++&quot;,&quot;R&quot;,&quot;Phyton&quot;),c(TRUE , FALSE)) L[1] #R&gt; [[1]] #R&gt; [1] 5 6 7 8 9 10 typeof(L[1]) #R&gt; [1] &quot;list&quot; typeof(L[[1]]) #R&gt; [1] &quot;integer&quot; L[[2]][2] # Accessing a specific element #R&gt; [1] &quot;R&quot; L[2][2] # Works only with double brackets #R&gt; [[1]] #R&gt; NULL "],["dataframes.html", "3.4 Dataframes", " 3.4 Dataframes Dataframes are only lists consisting of vectors of equal length, so all the list specifics also do apply for dataframes. Nevertheless, if we are operating in special matrix-like structures R supports us with a bunch of specialized functions that allow us to perform very fast manipulations. Some of the useful functions provided by R are merge(), apply(), sweep(), stack(), aggregate() and transform(). Merging rows and columns We already discussed the functions rbind() and cbind() to merge data, especially matrices by row or by column. But if handling more complex datasets this won’t get rid of duplicates, contained in each table, manually. Exactly for these cases the merge()-function was created. d &lt;- data.frame(ID=1:4,list( Name=c(&quot;Homer&quot;,&quot;Marge&quot;,&quot;Bart&quot;,&quot;Lisa&quot;), Age=c(38 , 34 , 10 , 8), Sex=c(&quot;m&quot;,&quot;f&quot;,&quot;m&quot;,&quot;f&quot;) )) e &lt;- data.frame(ID=c(1,4,3,2),list( Name=c(&quot;Homer&quot;,&quot;Lisa&quot;,&quot;Bart&quot;,&quot;Marge&quot;), Height=c(182 , 120 , 122 , 223), Weight=c(108 , 33 , 35 , 58) )) # Merge even works with unsorted dataframes and matches the datasets # fully automatically using common columns. merge(d,e) #R&gt; ID Name Age Sex Height Weight #R&gt; 1 1 Homer 38 m 182 108 #R&gt; 2 2 Marge 34 f 223 58 #R&gt; 3 3 Bart 10 m 122 35 #R&gt; 4 4 Lisa 8 f 120 33 To add rows in a similar and clever way there are packages available to take care of cases that can’t be solved with rbind(). One of these functions is called smartbind() from the package gtools. Apply functions to rows and columns A common, very fast and unbelievable useful helper is the function apply() which allows us to apply any function to every row or column of a dataframe. The function works in this way: apply(data, MARGIN=\\#, FUN=function() ). MARGIN indicates if the function defined in FUN should be applied to rows MARGIN=1 or columns MARGIN=2. FUN can be equal to any function, including self-written ones. mat &lt;- matrix(1:10,byrow=T,ncol=5) mat #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] 1 2 3 4 5 #R&gt; [2,] 6 7 8 9 10 apply(mat,MARGIN=1,FUN=mean) # Equivalent to rowMeans() #R&gt; [1] 3 8 apply(mat,MARGIN=2,FUN=sum) # Equivalent to colSums() #R&gt; [1] 7 9 11 13 15 This case is only to illustrate how apply() works. For the cases shown are specific functions like rowMeans() or colSums() available which are faster and should be used. In addition to apply() there are two more functions which can be used to apply the desired function on a list: lapply() and sapply(). They differ only in their output. The function lapply() outputs a list, while sapply() outputs a vector if that’s possible. They follow the exact same syntax as apply(). Sweep out Statistics in a Matrix The function sweep() sweeps out a summary statistics in a way defined by the argument FUN with subtraction as default operation. In addition to apply() this allows for very fast and completely vectorized manipulations. sweep(mat , MARGIN = 1 , STATS = c(1 , 10)) #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] 0 1 2 3 4 #R&gt; [2,] -4 -3 -2 -1 0 sweep(mat , 1 , apply(mat , 1 , mean)) #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] -2 -1 0 1 2 #R&gt; [2,] -2 -1 0 1 2 But sweep() is much more powerful than it seems on first sight. Dividing a column by its mean can also easily done with sweep by passing the argument FUN to it. mat &lt;- matrix(rep(1:5 , 2) , byrow = T , ncol = 5) mat #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] 1 2 3 4 5 #R&gt; [2,] 1 2 3 4 5 sweep(mat , 2 , colMeans(mat) , FUN = &quot;/&quot;) #R&gt; [,1] [,2] [,3] [,4] [,5] #R&gt; [1,] 1 1 1 1 1 #R&gt; [2,] 1 1 1 1 1 If you have problems using the sweep function you may want to convert your (sub-) dataframe to a numeric matrix, this can be done with data.matrix(). Concatenate all values from a dataframe To concatenate all values from multiple columns of a dataframe one can use the function stack() which outputs a dataframe with the stacked values while ignoring character columns. d.stacked &lt;- stack(d) # Stacks all numeric values of dataframe d d.stacked # Display the stacked data #R&gt; values ind #R&gt; 1 1 ID #R&gt; 2 2 ID #R&gt; 3 3 ID #R&gt; 4 4 ID #R&gt; 5 Homer Name #R&gt; 6 Marge Name #R&gt; 7 Bart Name #R&gt; 8 Lisa Name #R&gt; 9 38 Age #R&gt; 10 34 Age #R&gt; 11 10 Age #R&gt; 12 8 Age #R&gt; 13 m Sex #R&gt; 14 f Sex #R&gt; 15 m Sex #R&gt; 16 f Sex You may have noticed the column ind in the resulting output. This shows the origin of the data and makes the stacking fully reversible with the command unstack(), except for eventually lost string containing columns of the dataframe. Splitting dataframes while applying functions The function aggregate() allows us to split dataframes into subpopulations according to a provided measure and apply the desired function to each population. aggregate(d$Age,by=list(Sex=d$Sex),FUN=mean) #R&gt; Sex x #R&gt; 1 f 21 #R&gt; 2 m 24 Transformations without recreating dataframes The function transform() can easily be used to manipulate columns in a dataframe without the need to recreate the entire dataframe. # Adding two new lines to the dataframe d d &lt;- data.frame(d,list( Height=c(182 , 223 , 122 , 120), Weight=c(108 , 58 , 35 , 33) )) d # Display the extended dataframe #R&gt; ID Name Age Sex Height Weight #R&gt; 1 1 Homer 38 m 182 108 #R&gt; 2 2 Marge 34 f 223 58 #R&gt; 3 3 Bart 10 m 122 35 #R&gt; 4 4 Lisa 8 f 120 33 transform(d,Height=Height/100, BMI=Weight/(Height/100)^2) #R&gt; ID Name Age Sex Height Weight BMI #R&gt; 1 1 Homer 38 m 1.82 108 32.60476 #R&gt; 2 2 Marge 34 f 2.23 58 11.66321 #R&gt; 3 3 Bart 10 m 1.22 35 23.51518 #R&gt; 4 4 Lisa 8 f 1.20 33 22.91667 "],["strings.html", "3.5 Strings", " 3.5 Strings Strings are not only used to provide a description of your data. When dealing with more complex programs they become more and more useful. An often used case is creating variables while a program is running and using the created variables in the same instance to perform calculations without knowing the exact scheme of the variable names. This programming technique is called dynamic variable naming and we are going to explore this later in the course. But to be able to handle strings we need a couple of useful functions to deal with them. When creating functions that output calculations on the command line like the lm()-function does when calculating linear models, it is useful to manipulate the appearance in the output to create a better readable experience for the user. string &lt;- c(&quot;Statistics&quot;,&quot;and&quot;,&quot;calculus&quot;,&quot;are&quot;,&quot;wonderful!&quot;) string #R&gt; [1] &quot;Statistics&quot; &quot;and&quot; &quot;calculus&quot; &quot;are&quot; &quot;wonderful!&quot; noquote(string) #R&gt; [1] Statistics and calculus are wonderful! cat(string) #R&gt; Statistics and calculus are wonderful! Additional useful functions to manipulate the appearance of output are print(), format() and sprintf(). To concatenate strings into a single variable we commonly use the c()-function, if we really want to combine multiple strings into a single one we can use paste(). letters # Reserved word for all 26 small letters #R&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; #R&gt; [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; LETTERS # Reserved word for all 26 capitalized letters #R&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; #R&gt; [20] &quot;T&quot; &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; paste(letters[1:5],LETTERS[1:5],sep=&quot;+&quot;) #R&gt; [1] &quot;a+A&quot; &quot;b+B&quot; &quot;c+C&quot; &quot;d+D&quot; &quot;e+E&quot; paste(letters[1:5],LETTERS[1:5],collapse=&quot;&quot;,sep=&quot;&quot;) #R&gt; [1] &quot;aAbBcCdDeE&quot; If you want to create a bunch of variable or column names consisting of the same string, but different numbers you can simply pass that string and a vector of numbers to paste() and it will return the desired combinations, which then can be used to name the columns of a dataframe for example. paste(&quot;name&quot; , 1:3 , sep=&quot;&quot;) # Easy name or variable generation #R&gt; [1] &quot;name1&quot; &quot;name2&quot; &quot;name3&quot; To extract substrings or to split a string to a certain scheme R provides the functions substring() and strsplit(). If you want to search through a string you can use grep() to find the respective index of the searched string in a bigger string or vector of strings. To replace parts of a string R provides the commands gsub() to replace all occurrences and sub() to replace the first occurrence in the target string. If you are familiar with Linux or the world of Unix-based operating systems you may already be familiar with a lot of these string manipulating functions. In fact, most of these functions found in R can also be found in your favorite Linux shell as they originated from there. # To convert a String to upper or lower cases one can use the # functions toupper() and tolower(). identical(toupper(letters),LETTERS) #R&gt; [1] TRUE grep(&quot;K&quot;, LETTERS) # Returns index of searched string #R&gt; [1] 11 string &lt;- paste(rep(letters,2),collapse=&quot;&quot;,sep=&quot;&quot;) string #R&gt; [1] &quot;abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz&quot; gsub(&quot;o&quot;,&quot;X&quot;,string) #R&gt; [1] &quot;abcdefghijklmnXpqrstuvwxyzabcdefghijklmnXpqrstuvwxyz&quot; sub(&quot;o&quot;,&quot;X&quot;,string) #R&gt; [1] &quot;abcdefghijklmnXpqrstuvwxyzabcdefghijklmnopqrstuvwxyz&quot; If you have a more complex task like finding generic patterns in strings or simply want to find more than just simple predefined letters or words you may want to make yourself familiar with regular expressions. Regular expressions (regex) allow you to come up with a generic description of what should be searched and returned in textual data and they often come in handy when you want to extract specific parts of a textual dataset for further processing. As the syntax of regex is quite confusing when seen for the first time and textual data isn’t our main focus here we skip this part. Nevertheless, if you are interested in using them the built-in help system provides an excellent starting point when asked for ?regex. "],["dates-and-times.html", "3.6 Dates and Times", " 3.6 Dates and Times Dates and times in computer science can easily fill a bunch of books. There are different time formats, different data types and there are countless routines to handle, measure and manipulate time objects. So let us focus on the basics to implement basic features in your programs when it comes to times. In R we have two simple methods to obtain the current time. date() #R&gt; [1] &quot;Fri May 13 08:38:50 2022&quot; Sys.time() #R&gt; [1] &quot;2022-05-13 08:38:50 UTC&quot; Once we have a date we can encode and convert it to a POSIXlt-Object with the function strptime(). POSIX-Objects store the number of elapsed seconds since the 1. January 1970 - 00:00. Once we have a POSIXlt-Object it is easy to extract things like the month using the function months() or the day of the week using weekdays(). Sys.setlocale(&quot;LC_TIME&quot;,&quot;C&quot;) # Set regional parameters once #R&gt; [1] &quot;C&quot; strptime(&quot;15/mar/88&quot;,format=&quot;%d/%b/%y&quot;) #R&gt; [1] &quot;1988-03-15 UTC&quot; date &lt;- strptime(&quot;15/mar/88&quot;,format=&quot;%d/%b/%y&quot;) weekdays(date) #R&gt; [1] &quot;Tuesday&quot; months(date) #R&gt; [1] &quot;March&quot; As you can see in the code snippet above you need to pass a proper format argument to the function strptime() in order to decode or encode the given time accordingly. This may look like hieroglyphs at first sight but is indeed quite easy to learn. The following table provides an overview of the most useful codes. A more extensive reference can be found in the help system (?strptime) or in the Linux man pages, as the POSIX standard also originated from the Unix-universe. Code Input / Output %a and %A Abbreviated and full weekday name in the current locale. %b and %B Abbreviated and full month name in the current locale. %d Day of the month as decimal number (01-31). %e Day of the month with leading space for single decimal numbers (1-31). %H Hour as decimal number (00-23). %m Month as decimal number (01-12). %M Minute as decimal number (00-59). %S Seconds as decimal number including 2 leap seconds (00-61). %T Equivalent to %H:%M:%S. %U Week of the year as decimal number (00-53). %V Week of the year as decimal number as defined in ISO 8601 (01-53). %W Week of the year as decimal number (00-53, UK convention). %y Year without century (00-99). %Y Year with century (ISO 8601:2004). %F Equivalent to %Y-%m-%d (ISO 8601 date format). %D Date format such as %m/%d/%y (ISO C99 date format). When operating with POSIX-Objects we can perform lots of useful tasks which make handling time objects really convenient like comparing them and calculating the elapsed time between two events. Event1 &lt;- as.POSIXlt(&quot;1989-11-09&quot;) # Fall of Berlin wall Event2 &lt;- as.POSIXlt(&quot;1949-05-23&quot;) # Founding date of BRD Event1 - Event2 #R&gt; Time difference of 14780 days difftime(Event1, Event2, units=&quot;hours&quot;) #R&gt; Time difference of 354720 hours Event1 &gt; Event2 #R&gt; [1] TRUE At this point, we are quite well equipped with knowledge to handle and manipulate all common appearances of data, but programming is much more. Data is important but we can leverage its ability only when we have more powerful tools to automatize dealing with it. "],["exercises-2.html", "Exercises", " Exercises From April 12, 2022 EX 1 The function ´apply´ offers three main arguments ´X´, ´MARGIN´, ´FUN´. Please explain their purpose and possible parameterization options. EX 2 What is the idea behind the additional three dot argument ´…´ (dotdotdot) that can be set when using ´apply´? EX 3 Please explain the following three lines of code (line by line): x &lt;- cbind(x1 = 3, x2 = c(4:1, 2:5)) dimnames(x)[[1]] &lt;- letters[1:8] apply(x, 2, mean, trim = .2) EX 4 Explain the difference between the two function calls without executing them. paste(&quot;variable&quot;, 1:5, sep=&quot;&quot;) paste(&quot;variable&quot;, 1:5, collapse=&quot;&quot;, sep=&quot;&quot;) EX 5 What does the following code produce? Please derive the output and explain what happens before executing the code. mat &lt;- matrix(1:4, nrow=2) mat2 &lt;- mat %% 2 == 0 apply(mat2, 2, function(x){x[1]|x[2]}) EX 6 Calculate the average value of each column in the following matrix ´m´ after removing lines 10, 24 and 30-37. set.seed(5) m &lt;- matrix(sample(1:10^7), ncol=10, byrow=T) EX 7 Try to come up with an explanation of what the following code does and why it works. &#39;[&#39;(mat, 2, 2) "],["structure.html", "Chapter 4 Code Structure", " Chapter 4 Code Structure “Begin at the beginning,” the King, said gravely, “and go on till you come to the end; then stop.” – Lewis Carroll, Alice in Wonderland Like every other real programming language, R provides tools to structure your code based on conditional expressions meaning executing a part of the code dependent on a condition and loops to describe and perform similar tasks in a compact way. If you have no prior programming experience this may seem to be the hardest thing in the world, because you have to think of all the possible cases which occur in your situation to differentiate between them in your program. Additionally, you have to think in iterations, which means you have to find the common parts of your problems and put them into loops. But let us start at the beginning and look at conditional expressions. "],["conditional-expressions.html", "4.1 Conditional Expressions", " 4.1 Conditional Expressions If and Else The most basic conditional expression is an if-statement. It is used to make expressions contingent on a certain condition and in R it has the following syntax. if ( condition ) { code # Only executed if &#39;condition&#39; is TRUE } # A simple example x &lt;- 1 if (x &gt; 0) { cat(&quot;Hello World!&quot;) } #R&gt; Hello World! The condition is a set of commands that produce a single logical value namely TRUE or FALSE and the code in the body encapsulated by { } is only executed if the condition is TRUE. If the first condition is not true it is possible to include an else statement which should be evaluated if the first condition cannot be fulfilled. if ( condition ) { code # Only executed if &#39;condition&#39; is TRUE } else { alternate code # Evaluated if &#39;condition&#39; is FALSE } There may be cases where two logical values are not enough to implement what you want. In this case, you can combine as many if clauses as you want with the statement else if and catch the rest of the cases (if none of your conditions is fulfilled) with a single else clause. x &lt;- 314 if ( x &lt; 10 ) { cat(&quot;X is a small number!&quot;) } else if ( x &gt; 100 ) { cat(&quot;X is a big number!&quot;) } else { cat(&quot;X is neither a big nor a small number!&quot;) } #R&gt; X is a big number! When using logical operators it may be better to use double symbol notation &amp;&amp; rather than single symbol notation &amp; (same with logical OR). The reason for this is because the double symbol notation evaluates the condition step by step from left to right and aborts and returns a value if a FALSE is found. This makes the execution of the code much more efficient, faster and so the way to go. However one should carefully check the evaluated expressions in order to avoid errors which can easily sneak in here. There is a good reason that conditional expressions heavily rely on the usage of the word else. It seems that everything can be solved using ifs only. While this is (sometimes) possible it is not advisable. Normally conditional expressions are mutually exclusive choices so that code can easily get confusing or erroneous. Let us consider the following example which would work perfectly fine and make much more sense with added elses. if ( x &gt; 100 ){ cat(&quot;X is a big number!&quot;) } #R&gt; X is a big number! if ( x &gt; 10 ){ cat(&quot;X is a moderate number!&quot;) } #R&gt; X is a moderate number! if ( x &gt; 1 ){ cat(&quot;X is a small number&quot;) } #R&gt; X is a small number Another point to discuss is accuracy. When comparing objects in a setting such as the one discussed here, it is often better to use the functions isTRUE() or !isTRUE() rather than the combination of a logical operator and a logical value like T == and T != to evaluate a condition. This is due to the fact that conditional expressions, as well as conditional clauses, only work if there is a single (!!!) logical value provided, which can be enforced using functions that always return only a single value. One of the pitfalls are problems related to the precision of the calculations. Precision problems as the following one can also be solved using the function all.equal() which allows to set a tolerance argument and evens out the small precision problems in floating point calculations. num1 &lt;- 0.2 - 0.1 num2 &lt;- 0.3 - 0.2 num1 == num2 #R&gt; [1] FALSE all.equal(num1 , num2 , tolerance=10^-5) #R&gt; [1] TRUE As speed is one of the most important things about programming and vectorized commands are pretty fast, there is also a vectorized if/else version in R. ifelse( condition , ifTRUE , ifFALSE) # This is really fast! x &lt;- 1:3 ifelse( x &gt; 1 , x^2 , FALSE) #R&gt; [1] 0 4 9 Switch If you are using a lot of if/else constructs in your code this may get a little bit crowded and probably messy. R provides also a statement called switch() to return values contingent on a certain expression. This is also faster than combining lots of if-statements in your code but usually not as fast as the vectorized ifelse(). switch( statement , case1 , case2 , ... ) # Basic syntax # Example if &#39;statement&#39; is numeric x &lt;- 2 switch(x,&quot;one&quot;,&quot;two&quot;,&quot;three&quot;) #R&gt; [1] &quot;two&quot; # Example if &#39;statement&#39; is a string x &lt;- &quot;mean&quot; data &lt;- c(1 , 5 , 7.5) switch(x,&quot;sd&quot;=sd(data),&quot;median&quot;=median(data),&quot;mean&quot;=mean(data)) #R&gt; [1] 4.5 If the provided statement is a number then R returns the case with the matching index. If the statement is of type string, then R tries to match the statements and executes the code listed there. Using strings as a statement and formatting your code in a structured way you can receive the same appearance and feeling like you would with switch-case-statements in other programming languages. The following code block shows alternative formatting (just some additional curly braces, spaces, and newlines) that create a look similar to other programming languages such as C or Java. However, one should be careful with switch as the behavior is not always totally intuitive as the following example shows. usertype &lt;- &quot;caseXX&quot; switch(usertype, case1 = { 1 }, case2 = , case3 = {2.5}, 99) #R&gt; [1] 99 This maps case2 to the value defined in case3 and additionally supplies and unnamed default value 99 for cases that are not defined or cannot be matched. If there is more than one match, the first matching element is used. For more information, try ?switch. Despite being special in some cases switch is usually faster than a lot of if and else statements and keeps your code more organized. "],["loops.html", "4.2 Loops", " 4.2 Loops Loops are one of the most used and most important constructs while programming - they are somehow like the universal workhorse. Generally, a loop is a control structure which allows running specific lines of code over and over again until an exit condition is fulfilled or until a specified number of repetitions has been performed. While executing the same code several times the provided data can, of course, be alternated which makes loops very useful when performing a huge number of actions of the same type. For-Loop A for loop runs specified code a fixed number of times. Therefore, you have to know in advance how often the code in the body of the loop should be executed. The for loop has the following general form. for ( idx in vector){ # Basic syntax code } The typical iterator variable i that is commonly used in other programming languages should be avoided in R because it is already a reserved word for the imaginary unit when working with complex numbers. Common workarounds are using it (short for iterator), ii (just doubled to avoid using the reserved word) or idx (abbreviation for iteration index). x &lt;- 1:3 for ( idx in x) { print(idx) } While-Loop The while loop loops until the specified condition is false. It is often used when you do not know and can not calculate in advance how often the instructions will be executed. It has the following general form. while ( condition ){ # Basic syntax code } idx &lt;- 6 while ( idx &lt; 10 ){ print(idx) idx &lt;- idx + 2 } Repeat The repeat loop is somehow similar to the while loop. The while loop starts a new run every time the condition is evaluated as TRUE while the repeat loop will always begin a new run and will only stop if it is told to do so with the break command. This means you can explicitly control which part of the loop should be executed and when the execution should be stopped. counter &lt;- 0 # Counter variable to count loops repeat{ # Repeat does not need a condition x &lt;- rnorm(1) counter &lt;- counter + 1 if (x &gt; 2) { print(counter) print(x) break } } Break and Next We have already seen that break is able to abort the repeat-loop, but it is much more powerful and can break any of the discussed loops directly. Even if you can use break to abort loops on certain conditions this is a bad habit. It is always better to know when and how a loop should end and do this via the condition or by predefining the number of loops using the for loop. The next command is used to skip the following statements and if used in a loop it will increment the loop variable (idx in the for loop example). Here is a simple example to illustrate how both are working. for ( idx in 1:100) { x &lt;- rnorm(1) if (x &lt; 0) next print(x) if ( abs(x) &gt; 3) break # Break should normally be avoided } Generally one can say that loops in R are pretty slow and in most of the cases drastically slow down your code. Therefore, where possible loops should be avoided! In most of the cases it is much better (and faster!) to use vectorized functions like apply(), sapply() or tapply(). Special: Looping over non-numerical data R does not exactly support looping over non-numerical data, but it may be useful sometimes and of course, there are a few ways to accomplish this. The first one is using a function of R’s mighty apply()-family. In this case lapply() is the way to go if the performed and desired loops are independent of each other and it is allowed to process them in any order. Another option is using the function get(). In fact, the purpose is straight forward and sounds very simple. It takes a character string as an argument and returns the object of that name. Here is a short example of how to calculate the mean of three vectors. x &lt;- rnorm(10) y &lt;- rnorm(1000) z &lt;- rnorm(1000000) for ( idx in c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;)) { vec &lt;- get(idx) print(mean(vec)) } #R&gt; [1] -0.452227 #R&gt; [1] -0.02234161 #R&gt; [1] -0.001333537 This simple example may give you an idea of how powerful the get()-function is. Try to imagine how flexible and powerful a routine written with the combination of get() and paste() can be. This does, however, not cover all possible cases, for example, if you want to loop over the elements of a list by name, the above example using get() won’t work. list &lt;- list(x=x, y=y, z=z) for ( idx in names(list)){ print(list$idx) } #R&gt; NULL #R&gt; NULL #R&gt; NULL Of course there is a way to force R looping over the names and fetch the data. To achieve this you have to construct a string and then make R evaluate this string using the command eval(). for ( idx in names(list)){ string &lt;- paste(&quot;list$&quot;,idx,sep=&quot;&quot;) # Constructing command data &lt;- eval(parse(text=string)) # Evaluating command print(mean(data)) # Working with fetched data } #R&gt; [1] -0.452227 #R&gt; [1] -0.02234161 #R&gt; [1] -0.001333537 The shown example is, of course, arbitrary and the result could have been achieved easier using numeric values for idx and double brackets to address the i-th element in the list. Nevertheless, it shows Rs capabilities to loop over non-numeric content and gets really useful if you are dealing with more complex ways of addressing data. If you want to read R code from a file or are looking for another option to evaluate a concatenated string you can also use the source()-command. "],["exercises-3.html", "Exercises", " Exercises From April 26, 2022 99 Bottles of Beer In early 1994, when the internet was only used by a fraction of the people that are using it today, someone posted the full lyrics of the song 99 Bottles of Beer to a humor mailing list, which was also heavily used by programmers. This was seen as an enormous waste of bandwidth because the lyrics could have been easily posted using six lines of BASIC code. This small event was the occasion for Tim Robinson to develop and post his version of the song in C++ which inspired other programmers to post their version of the song which since then has been ported to nearly every programming language available and is one of the many classic yet funny stories from the early ages of the internet. The song has the following lyrics: 99 bottles of beer on the wall, 99 bottles of beer. Take one down and pass it around, 98 bottles of beer on the wall. \\[ \\vdots \\] 2 bottles of beer on the wall, 2 bottles of beer. Take one down and pass it around, 1 bottle of beer on the wall. 1 bottle of beer on the wall, 1 bottle of beer. Take one down and pass it around, no more bottles of beer on the wall. No more bottles of beer on the wall, no more bottles of beer. Go to the store and buy some more, 99 bottles of beer on the wall. [END] Write a function called bottlesong() that outputs the famous and complete lyrics of the song 99 Bottles of Beer. 4.2.1 Multiplication Table Write a function mulitplicationtable() that outputs the multiplication table including results from \\(1 \\cdot 1 = 1\\) up to \\(10 \\cdot 10 = 100\\) to the console. The function should rely on loops, results should be calculated at runtine and the time delay between the output of two consecutive lines to the command line should be user-definable. The following block shows an abbreviated example output. 1 x 1 = 1 1 x 2 = 2 1 x 3 = 3 ... 10 x 9 = 90 10 x 10 = 100 4.2.2 Diagonal Matrix Write a function that creates a symmetric matrix with dimension \\(n x n\\), that contains a value x on every element, except for the elements on the main diagonal which should contain value y. Wrap your code in a function diagmat() and make all mentioned values user-definable. The following block contains an example output: "],["software-development.html", "Chapter 5 Software Development", " Chapter 5 Software Development First, solve the problem. Then, write the code. – John Johnson Most programming projects start small and are not intended to grow. However, this is exactly what happens and after some time you are confronted with a large, complex and diffuse code base where nobody - including yourself - understands how the programs really work. Good coding style is like correct punctuation when using natural language. You can live without it, but it makes things a lot easier to understand and better to read. The reasons to advocate a good programming style are manifold: Errors: Good programming style helps to avoid errors, saves time when searching and correcting mistakes and reduces embarrassment that normally occurs if errors cannot be found. Collaboration: It occasionally occurs that problems are solved in teams rather than alone, a good programming style eases the handover and reduces introduction efforts when joining projects. It may also happen that someone (researcher, colleague) has to maintain or use your program. Good programming style eases the process of understanding dramatically. Publications: Supervisors for thesis or seminar papers as well as scientific journals usually want to know (implementations details) about data and estimation procedures to support transparency and reproducibility of scientific work. From an academic point of view, the quality of the provided code is often considered when grading assignments, theses or homework. Memories: Humans usually overestimate how much they remember and usually underestimate the necessity for future changes. Months after finishing or suspending a project, most people have forgotten the details and difficulties as well as dependencies when modifying the code. Your future ‘you’ will have the same problems as any colleague that has to use your code if you haven’t looked at it for a couple of months. Developing software is an intensive field of research, it has a lot in common with organization and project management, but usually is a lot more formalized. Many of the more popular Software Development Frameworks (SDF), like Rapid Application Development (RAD), Scrum, Extreme Programming (XP) or the Agile Unified Process (AUP) achieved attention and are actively adopted by other industries. All these frameworks, processes or however you want to refer to them have one thing in common and this is to ensure that a high-quality output (usually in terms of software) is being produced by one or more developers. While modern (agile) SDFs are usually used to manage teams and do not have distinct phases, more traditional approaches emphasize the idea of software development life cycle, where phases like planning, coding, documenting or testing occur sequentially rather than simultaneously. One of these process models is the software development waterfall, which clarifies the essential steps when writing software. Figure 5.1: Waterfall Software Development Process The basic idea of the process is to divide the whole development project into sequential phases, where overlap and ‘splashbacks’ are tried to be avoided. As in traditional project management, the emphasis is on planning time schedules, target dates, and budgets, while the implementation of an entire system is done at once. The transition to the next phase usually requires the approval of a customer or user and written project documentation is an explicit deliverable of each phase. The sequential nature and the resulting “inflexibility” in a pure waterfall model have been a source of constant criticism. In practice, the waterfall model has been superseded by more flexible and dynamic approaches and when working in teams a more agile SDF, e.g. Scrum, should be chosen as these eradicate many pitfalls found in the waterfall SDF. However, all of the following stages are always present and must be considered when developing software, regardless of the specific management approach. "],["planning.html", "5.1 Planning", " 5.1 Planning Especially in scientific environments software usually fulfills a strict purpose and solves an (academic) problem. The planning phase, therefore, captures the user requirements and aligns them with the prospective software architecture. The output of this phase usually is a document with the requirements and a plan for the software architecture. For good software, it is key to separate the actual coding from the planning phase. Usually, it is a good idea to start a new project with an empty sheet of paper and collect the theoretical and practical aspects to form a sketch of the possible solution. Do not think about technical details in this phase too much, but write down your ideas in pseudo code like load data, transform (scale and logarithmize) input, calculate regression coefficients, * analyze error terms*. Sub-problems that occur multiple times or code that needs to be executed more than once should be modularized and needs to be put into a function. This avoids inconsistencies and provides a single place to maintain functionalities and implement changes. Besides this, functions have additional advantages - they make the code more readable and facilitate testing and validation. When wrapping boring sections like data transformation in a function this can usually be done by a single line of code including easily visible arguments. Therefore, you can think of modules like building blocks to your solution, each block (function) should handle a single task and not more. In addition to solving one thing well, functions should be flexible and solve a sufficiently general problem. It is more advisable to make a function flexible and solve the n-dimensional case (and avoid hard-coding the 2-dimensional case), as your requirements may increase over time. The general steps and the building blocks of functions can be combined into an architecture that solves the specific problem. Proceeding this way, the problem is being divided into simpler steps and each sub-problem is solved individually (Principle: Divide and Conquer). It is always advisable to support the planning process with a graphical representation that allows to easily discover dependencies and the right sequence of commands. While computer and system sciences lead to a huge variety of “modeling languages” even simple graphical representations are often useful. A lot of problems and requirements have already been solved and implemented by other programmers and quite often there is no need to reimplement every little step on your own. It is fair and good practice to reuse, modify and recycle existing packages, libraries, functions or code fragments. “Whenever possible, steal code” is one of the many bits of advice frequently given to new programmers. While this is usually good advice and shortens the implementation and coding requirements it still comes at the cost of understanding and integrating someone else’s code into your project. While code often can be freely found in the web, it still comes with some requirements and liabilities which are usually bundled in the respective license that is linked or distributed with the code. Regardless of the license under which the software is distributed the usage of someone else’s ideas or intellectual property - especially in scientific projects - needs to be cited. If you use R packages they conveniently provide a reference that can be cited and is accessible with the command citation(\"packagename\"). If citation() is used without an argument it returns the reference for the R Base System. citation() #R&gt; #R&gt; To cite R in publications use: #R&gt; #R&gt; R Core Team (2022). R: A language and environment for statistical #R&gt; computing. R Foundation for Statistical Computing, Vienna, Austria. #R&gt; URL https://www.R-project.org/. #R&gt; #R&gt; A BibTeX entry for LaTeX users is #R&gt; #R&gt; @Manual{, #R&gt; title = {R: A Language and Environment for Statistical Computing}, #R&gt; author = {{R Core Team}}, #R&gt; organization = {R Foundation for Statistical Computing}, #R&gt; address = {Vienna, Austria}, #R&gt; year = {2022}, #R&gt; url = {https://www.R-project.org/}, #R&gt; } #R&gt; #R&gt; We have invested a lot of time and effort in creating R, please cite it #R&gt; when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for #R&gt; citing R packages. Integrating other functions, packages or code fragments means to organize and handle how different functions hand over data, which means one has to think about the data format and use standard data formats wherever possible. Choosing or creating a data format is not to be mistaken by choosing a file extension like .txt,.csv or .ppt but defines the structure of your list, data.frame or comparable data-structure present in R. Choosing a standard, therefore, refers to choosing the type of object that captures, organizes and stores data, estimation results or other output. This is important for several reasons as bad designs of the data structures make the coding more difficult and the program less readable. Often one needs to consider that empirically derived results are almost never perfect or satisfactory after a first run and need to be recalculated on different subsets or transformations of the data. A designed data structure needs to combine all results including the envelope and meta information, like function arguments, etc. Last but not least a clever and compact design of the resulting data structure can affect the actual execution speed of a program significantly. Choosing a standard or convention is especially important. Imagine when using a matrix to store individual observations for e.g. multiple time series it makes sense to store the observations in rows and the variables/time series in columns. Additionally, one needs to think about missing values in the data (e.g. calendarial effects like holidays) and how to handle them. "],["coding.html", "5.2 Coding", " 5.2 Coding Writing code that a computer can understand is actually quite easy - we have even written a simple function in the very first unit of this course. Good code, however, is written for humans to read and only incidentally for machines to execute. It, therefore, is good practice to write code with a reader in your mind (e.g. your supervisor for a thesis, a colleague or your grandparents). Most of the problems tackled in scientific projects can as well be solved with simple solutions. Advanced logic and aiming for short solutions (e.g. one-liner) may make code hard to read and even harder to understand. The objectives to head for when writing software are therefore clarity and brevity. Speed is important, but there is no necessity to tweak out every little bit of performance (in terms of speed and memory) and this should only be the chosen path if it is really necessary. To ensure readability you should adopt standard code layouts. While normal analyses and simple programming exercises usually can be pretty well organized with a few R files (.R), bigger projects normally rely on more advanced structures such as R Project files (.Rproj) and Packages. Being an introductory course we focus on simple organization schemes and refer to the appendix (for a very short introduction) and the literature. Figure 5.2: Exemplary simple Project Skeleton Apart from simple analyses, most projects require some software development and own functions that are used to analyze a respective dataset. Custom functions should be separated from the pure analysis if they reach a critical mass (which is usually the case), so that it is good advice to separate the code base into two files, like in the figure above. The following chapters discuss the layout of these files before introducing and discussing some additional principles and things to consider when filling the files with actual code. 5.2.1 Standard Program Structure The standard program structure contains the actual analysis (contrary to own functions) and should contain the following elements: Headline: The Name of the Program Preamble: A short text explaining the purpose and basic principles of the program, what it does and how it is intended to be used; including a license statement if necessary. Author: Name and contact information of the author(s), the (release) date and the version of the software. If multiple versions exist, the version information in the form of a changelog can be handed out. Program: The actual program divided into parts or blocks. Dependencies: Here external dependencies like libraries or a custom function file should be loaded. If the function file would only contain a few short function definitions it makes obviously no sense to create an own file for them. As long as the readability of the function file is not affected the function definitions could also be included here. Constants: Constant values that your program relies on and values that parameterize the methods used below should be user changeable, bundled and included in the second block, directly after loading external dependencies. Problem Solving: Here the actual problem solving takes place. Of course, this part can be divided into multiple parts or blocks. Output: The output generated by your analysis usually plots, tables or PDFs should be created and exported in the last block of the program. # Example of Standard Program Structure # This is an example skeleton of a Standard Program Structure that can be used # to structure own programs and analyses. It was written purely for educational # purposes and is part of the lecture notes for the course &#39;Computational # Statistics using R&#39;. # Author: Benjamin Buchwitz (benjamin.buchwitz@ku.de) # Date: 01.01.1970 # Version Changelog: # v0.1 - Initial Release # v0.2 - Major Improvements in Speed and Stability ################################################################################ ## PART I: Load Libraries and external Dependencies ################################################################################ # Loading installed packages ------- library(ggplot2) library(forecast) # Loading own functions ------------ setwd(&quot;~/projectName&quot;) # Set working directory to project folder source(&quot;projectName_functions.R&quot;) ################################################################################ ## PART II: Define Constants ################################################################################ a &lt;- 0.5 # Initial calibration Parameter n &lt;- 100 # Length of simulated Data T.eff &lt;- 80 # Value used in the Model x &lt;- rnorm(n) # Data; standard normally distributed y &lt;- rnorm(n) + 5 # Data; mean shifted to 5 ################################################################################ ## PART III: Problem Solving ################################################################################ res &lt;- CalculateSampleCovariance(x, y, verbose=FALSE) ... ################################################################################ ## PART IV: Output Results ################################################################################ # Save estimation results ---------- saveRDS(res, &quot;projectName_results.rds&quot;) # Create publication ready plots --- pdf(&quot;plotexample.pdf&quot;) plot(x, y, main=&quot;An awesome Plot&quot;) dev.off() 5.2.2 Standard Function Structure The second file contains the functions and should have the following structure. Headline: The Name of the Program that the functions belong to Preamble: A short text explaining the purpose and basic principles of the functions, what they do and how they are intended to be used; including a license statement if necessary. This can be the same text as in the file described above. Function Definitions: Actual definitions of the functions. Each function definition should retain the following structure: First Line: Function Constructor and Name of the function Preamble: The preamble of a function fulfills similar purposes as the preamble of the file that contains the analysis or function itself. It contains a headline that very briefly describes what the functions do. If necessary an additional text describes the function and its extended purposes or way of implementation. After that all Arguments of the function are introduced and described. Finally, the output, including the corresponding data structure is described. Body: The Code that actually solves the Problem. # Example file with Standard Function Structure # This is an example skeleton that contains a Standard Function Structure. The # functions defined here are needed to for the Standard Program Structure above. # It was written purely for educational purposes and is part of the lecture # notes for the course &#39;Computational Statistics using R&#39;. # Author: Benjamin Buchwitz (benjamin.buchwitz@ku.de) # Date: 01.01.1970 # Version Changelog: # v0.1 - Initial Release # v0.2 - Major Improvements in Speed and Stability ################################################################################ ## Function Definitions ################################################################################ #&#39; Computes the sample covariance between two vectors. #&#39; #&#39; This function calculates the ordinary sample covariance between two vectors, #&#39; it is intended to be either used by a user directly and is also used in #&#39; some other functions, e.g. when calculating the correlation coefficient. #&#39; #&#39; @param x One of two vectors whose sample covariance is to be calculated. #&#39; @param y: The other vector. x and y must have the same length, greater than #&#39; one, with no missing values. #&#39; @param verbose Logical. If TRUE, prints sample covariance; if not, output #&#39; is suppressed. Default is TRUE. #&#39; #&#39; @return The sample covariance between x and y. CalculateSampleCovariance &lt;- function(x, y, verbose = TRUE) { n &lt;- length(x) # Error handling if (n &lt;= 1 || n != length(y)) { stop(&quot;Arguments x and y have different lengths: &quot;, length(x), &quot; and &quot;, length(y), &quot;.&quot;) } if (TRUE %in% is.na(x) || TRUE %in% is.na(y)) { stop(&quot; Arguments x and y must not have missing values.&quot;) } covariance &lt;- var(x, y) if (verbose) cat(&quot;Covariance = &quot;, round(covariance, 4), &quot;.\\n&quot;, sep = &quot;&quot;) return(covariance) } ... 5.2.3 Naming Convention While the aforementioned Standard Program Structure and the Standard Function Structure regulate what should be presented when developing code it is still unclear how functions, arguments, objects, classes or files should be named. Naming things is considered one of the more tricky parts in computer science. As it may be complicated to change e.g. a function name after it has been heavily used in other functions one needs to adopt a clear and vivid naming scheme. This also ensures that named functions, classes, etc. speak for themselves and is essential to produce readable and self-explaining code. The following list shows some popular naming conventions that all more or less frequently occur in several programming languages. Of course, not all developers that produce and distribute R software follow the same naming conventions. The function names given in the list below are R functions to illustrate how and where the respective naming scheme can be found within R. The shown list including some additional discussion can be found in the R Journal. alllowercase: All letters are lower case and no separator is used in names consisting of multiple words as in searchpaths or srcfilecopy. This naming convention is common in MATLAB. Note that a single lowercase name, such as mean, conforms to all conventions but UpperCamelCase. period.separated: All letters are lower case and multiple words are separated by a period. This naming convention is unique to R and used in many core functions such as as.numeric or read.table. underscore_separated: All letters are lower case and multiple words are separated by an underscore as in seq_along or package_version. This naming convention is used for function and variable names in many languages including C++, Perl, and Ruby. lowerCamelCase: Single word names consist of lower case letters and in names consisting of more than one word all, except the first word, are capitalized as in colMeans or suppressPackageStartupMessage. This naming convention is used, for example, for method names in Java and JavaScript. UpperCamelCase: All words are capitalized both when the name consists of a single word, as in Vectorize, or multiple words, as in NextMethod. This naming convention is used for class names in many languages including Java, Python, and JavaScript. When working in professional software development teams or joining existing projects one usually has to adapt to the defined naming convention. However, when developing alone one should still follow some rules. A good set of rules to follow are these ones: A Personal Naming Convention This naming convention applies to variables, functions, and R files. Files that contain R code are always saved with the extension .R. All names are self-explaining English words, usually not abbreviated and at least 3 letters long. I use the singular even if numerous entries are expected. Example: price, duration, strike. Names for objects and functions use the lowerCamelCase naming scheme with the modifications listed below. Exceptions to (1) and (2) are widely used single-letter symbols and widely used acronyms. For both, I use only uppercase letters. Examples: X, Y in regression analysis or FFT, GDP. Uppercase letters concatenate words. The following rules apply to concatenated names: – Names of variables start with the most general term and end with the most specific (the latter usually denoting a certain method). Examples: optionPriceFFT, optionPriceAnalytic, optionStrike. – Names of functions start with the name of the project, followed by the object of the function, what it does and finally the specific method. Examples: wishartOptionPricingFFT, wishartOptionPricingCosfft, wishartOptionPlotting. To avoid confusion, I use the prefix “my”. This may be necessary if one variable is used locally to calculate a variant of a model or if the name conflicts with an existing R command. Example: myLinearModel, myDate. Use common Sense and be CONSISTENT! When you reach a point, where the naming convention fails do not use it in this special case and check if it needs to be adapted and improved. Recommendations following: Gruber (2013) - Solving Economics and Finance Problems with MATLAB. 5.2.4 Rules &amp; Best Practices The following presents some rules and best practices that are widely adopted by programmers and have been proven useful: Comment your code. Each line of a comment should begin with the comment symbol # and a single space. Comments should explain the why, not the what. Check twice if the comment is clear and if it is really needed. Rely on commented lines of ######, #==== or #---- to break up your file into easily readable chunks. While writing your code you can collapse these sections in RStudio so that you do not have to scroll over them every time. If you want to implement a feature later or if you assign the responsibility to implement something to a team member choose a consistent style for TODOs throughout your code. # TODO(username): Explicit description of action to be taken. Use White Space and indentation to structure your code, but avoid the tabular key (RStudio helps you by translating every tab into 2 spaces automatically). Aim for a code width of 80 characters or less, so that your code can be printed if necessary and stays fully visible in text editor windows with moderate width (avoid the necessity to scroll vertically). RStudio provides support for this by displaying a thin grey line in the code editor, that can be enabled via the menu under Tools -&gt; Global Options Dialogue. Choose the Code pane and select the Display tab to checkmark the “show margin column” option. It is pretty easy to find a case for which we have not discussed a rule yet. Due to the sheer complexity of the topic, we, therefore, rely on some popular examples to follow. Many professional developers have spent and effort to conceptualize complete style-guidelines, some with complete naming schemes and some without one. The following links will point to the most common style guides for R and you are encouraged to choose one (in addition to the things already discussed) when developing software. Note that none of these style guides is an official R standard and that there is (and possibly never will be) no such thing. The Tidyverse Style Guide Google’s R style guide Hadley Wickham’s Style Guide Bioconductor’s Coding Standards Colin Gillespie’s R Style Guide "],["documenting.html", "5.3 Documenting", " 5.3 Documenting Software that is intended to be used needs to be documented. During the coding chapter, we actually covered a few important steps that already contain documentation of a program. Generally, the provided documentation for a program can be divided into code documentation and user documentation. Examples for the code documentation are the comments that explain the implemented steps directly in the code, while the user documentation usually consists of long term documentation such as a vignette or accompanying book to explain what the developed software does. 5.3.1 Code Documentation (Short-form Documentation) Code documentation is sometimes also referred to as short-form documentation. As mentioned previously it supports readers of your code, mainly while using the code. It is intended to briefly explain arguments to functions and basic working principles. R in combination with various packages support you by documenting your code. The documentation style using the modified comment #' is implemented by the package roxygen2. It allows creating a help page directly from the function preamble, that is later accessible via the R help system ?. This is useful and saves a lot of work as this part of the documentation can be written at the same moment as the software itself. However, to be available via the ? operator, it is required that your code will be delivered to users as an R package. While we have not covered that yet, it is still beneficial to form a habit of documenting your code this way. The following table provides a short overview of some tags that you can (and should) use when documenting your code. Tag Explanation @param Describe function parameter @return Indicate what the function returns @examples Possible usage example of the function @seealso Function that may be relevant to understand @author Author of the respective function @keywords Contextual topic of the function @references Web or academic reference to the function However, the roxygen2 package is not limited to these tags but provides excessive options that are especially useful when developing R packages. A brief overview of capabilities and options for the roxygen engine is available in a Cheat Sheet. 5.3.2 User documentation (Long-form Documentation) Long-form documentation describes the program, which can consist of a single or multiple functions, in more detail. It provides insights on what the program does and is the right place to elaborate on implementation details or explain the statistical or mathematical theory behind your routine. Long form documentation is usually called a vignette and can be anything from a short technical note over a journal article to a complete book. It can, therefore, serve as user documentation, allowing and explaining possible customization options and describe everything in more detail than the accompanying code documentation. Available documentation for an installed package can be viewed using the function vignette(). vignette(package=&quot;forecast&quot;) Vignettes in package ‘forecast’: JSS2008 Automatic Time Series Forecasting: the forecast Package for R (Hyndman &amp; Khandakar, JSS 2008) (source, pdf) For example, the vignette for the forecast package is a journal article that was published in the Journal of Statistical Software (JSS). It will be downloaded when installing the package or, as the JSS is an open access journal, is also available to be downloaded here. A good way to write long-form documentation is using RMarkdown, which is a specialized version of Markdown - a plain text formatting system. R Markdown hast multiple advantages: it is strictly focussed on content, can intermingle text and results from code executions and is tightly integrated into RStudio. It is, therefore, the optimal solution for documenting programming challenges, solving assignments or writing theses that rely on R. The syntax employed by Markdown is simple and straightforward and it allows you to produce many kinds of output such as HTML, pdf or even Microsoft Word documents. "],["testing.html", "5.4 Testing", " 5.4 Testing Software testing is an investigation conducted to provide stakeholders with information about the quality of the tested code. Its goal is to generate an independent view of the software that allows understanding of how the developed product can affect its (business) environment. In professional environments, this allows finding bugs and verifies that the software product is fit for production purposes. Software testing involves the execution of a software component (e.g. a small function) or system component (e.g. a large function or a whole package) and evaluates at least one property of interest. This can, for example, be the requirements that have been noted when planning the program. Testing usually checks the behavior of the software under all kinds of inputs, evaluates the response time and efficiency and simulates conditions in practical uses. As the number of possible tests is almost infinite, professional developers employ test strategies that intend to find errors, unexpected results or general misbehavior. As all discussed steps in this chapter, testing is an iterative process. Fixing one error can lead to previously hidden deeper bugs or can even create new ones. As this is not a course on software engineering, we limit our testing framework to the following rules of thumb that can serve as a guideline after writing our first programs. Test in an organized and modular way, ideally by making a plan. Check every function separately. Test every scenario at least twice, especially when you have modified your function or fixed an error. Find and test boundary values. Test for different and deliberately false input. Do not only look for errors, often warnings reveal much more subtle problems, that need to be resolved. Document what you test. A more comprehensive way of testing in an automated fashion is provided by the testthat package, which is the most popular unit testing solution in R. However, as this is an introductory course we do not cover automated testing in more detail. "],["maintaining.html", "5.5 Maintaining", " 5.5 Maintaining Software systems evolve and need adaptation as well as fixes to continuously deliver results for their users. Maintenance allows preserving the value that the software provides over time. Generally, maintenance is any change to the software after it has been used for production purposes for the first time. The key software maintenance issues are both managerial and technical ones. Usually, maintenance tasks include error fixing, capabilities extensions, removing obsolete capabilities, and process or functionality optimizations. Productive software requires strict versioning to allow users as well as developers to keep track of the changes. While simple programs that are being written for simple analyses or software that implements research prototypes often only require a single “release”, this is obviously different for business and professional applications. Naturally, maintenance in a research context is not that intensively discussed, and as this is an introductory course we abstain from further explanations here. However, maintaining the value a software delivers has to be kept in mind for bigger and continuous projects. "],["exercises-4.html", "Exercises", " Exercises From May 3, 2022 Description R script main program main_program_aus_tourism function selectedData() selectedData function plotTs() plotTs function summarizeRegion() summarizeRegion "],["vectorization.html", "Chapter 6 Vectorization", " Chapter 6 Vectorization Talk is cheap. Show me the code. — Linus Torvalds While we already intesively talked about loops we have experienced and discussed some of their drawbacks. Especially in R loops are incomprehensible slow. Additionally they require to handle index or iterator values, which may lead to confusion. Luckily R provides an alternative that eradicates these problems and allows to eventually deprecated the usage of loops in R in favor of vectorized functions. If you understand the nuts and bolts of vectorization in R, the following functions allow you to write shorter, simpler, safer, and faster code. "],["apply-family.html", "6.1 *apply-Family", " 6.1 *apply-Family In the previous sections we already introduced some vectorized functions without explicitly metioning their rationale and that they are vectorized. The following functions all belong to the so called *apply-Family. Vectorization in R requires a thorough understanding of the available data structures, as the following functions iterate (automatically) over different slices of data strucutes and perform (loop-wise) repetitions on the data slices from vectors, matrices, arrays, lists and dataframes. More specifically, the family consists of apply(), lapply(), sapply(), vapply(), tapply(), mapply(), rapply(), and eapply(). The following snippet gives a very short overview over the functions we are going to discuss in more detail. All of these functions are provided by the R base system and are thus located in the library base. # The *apply-Family of Functions in the Library base base::apply Apply Functions Over Array Margins base::lapply Apply a Function over a List or Vector base::sapply Simplified Version of lapply base::vapply Safer Version of sapply base::tapply Apply a Function Over a Ragged Array base::mapply Apply a Function to Multiple List or Vector Arguments base::rapply Recursively Apply a Function to a List base::eapply Apply a Function Over Values in an Environment apply We already introduced apply and used it to apply a function to the rows or columns of a matrix, in the same fashion as functions like rowMeans or colMeans calculate a specific values for either a row or a column of a matrix. Generally speaking apply operates on (two dimensional) arrays, a.k.a. matrices. To get started we create a sample data set consisting of a matrix with 20 cells partitioned into five rows and four columns. mat &lt;- matrix(c(1:10, 21:30), nrow = 5, ncol = 4) mat #R&gt; [,1] [,2] [,3] [,4] #R&gt; [1,] 1 6 21 26 #R&gt; [2,] 2 7 22 27 #R&gt; [3,] 3 8 23 28 #R&gt; [4,] 4 9 24 29 #R&gt; [5,] 5 10 25 30 To mimic the functionality of rowSums, we now can use the apply function to find the sum over all elements of each row as follows. apply(X=mat, MARGIN=1, FUN=sum) #R&gt; [1] 54 58 62 66 70 Notice that the funciton call to apply takes three arguments, where X is the data, MARGIN corresponds eiter to the rows as they are the first dimension of the data or to the columns, which correspond to the second dimension. FUN is the funciton that should be applied on the specified margin of the data. Note that the function in the snippet below is passed without parantheses (sum instead of sum()). Remember that in R everything is a vector. Therefore, a matrix can be seen as a collection of line vectors when you cross the matrix from top to bottom (along MARGIN=1), or as a list of column vectors, spanning the matrix left to right (along MARGIN=2). The code in the above R chunk therefore translates directo to the instruction to: “apply the function sum to the matrix mat along the rows”. Ursuprisingly this leads to a vector containing the sums of the values of each row. Mathematically speaking we would expect a column vector here, while R outputs a line vector. As R does not differentiate here while outputting these on the console this makes no difference for this case. The following picture illustrates the process. lapply While matrices are an important and often used data structure they are not the only one. Quite often data comes as list and it may be a reasonable purpose to apply a function to every (sub-) element of a given list. As lists have no dimensions (see dim), the application of apply fails. Thefeore if you want to apply a specific function to every element of a list you have to use a list compatible version of apply, the lapply-function. The syntax is quite comparable to our usual apply, which can be seen when executing ?lapply. Using our toy example with the previously introduced matrix, we construct a list by spliting mat by row. Applying the function sum to this list should now result in the same values as the previous application of apply on mat. lst &lt;- split(mat, 1:nrow(mat)) # Split mat by row lst #R&gt; $`1` #R&gt; [1] 1 6 21 26 #R&gt; #R&gt; $`2` #R&gt; [1] 2 7 22 27 #R&gt; #R&gt; $`3` #R&gt; [1] 3 8 23 28 #R&gt; #R&gt; $`4` #R&gt; [1] 4 9 24 29 #R&gt; #R&gt; $`5` #R&gt; [1] 5 10 25 30 Due to the flexibility and ubiquitousness of lists, lapply can be widely used and e.g. also works on dataframes in addition to lists. Additionally it is compatible with vectors, where the second most important part about lapplycomes into place. Regardless if the data input X is a list, a dataframe or a vector, the returned data is always a list, which can be seen in the code below. lapply(X=lst, FUN=sum) #R&gt; $`1` #R&gt; [1] 54 #R&gt; #R&gt; $`2` #R&gt; [1] 58 #R&gt; #R&gt; $`3` #R&gt; [1] 62 #R&gt; #R&gt; $`4` #R&gt; [1] 66 #R&gt; #R&gt; $`5` #R&gt; [1] 70 The following image shows the proces and illustrates how lapply works. As seen above the results are identical to the ones delivered by apply and the returned data structure is (as expected) a list. sapply The function sapply takes the same inputs and behaves in the exact same mannor as lapply, but tries to simplify the result so that it returns an appropriate data structure instead of always returning a list. Applied to our example from above sapply returns a numeric vector. sapply(X=lst, FUN=sum) #R&gt; 1 2 3 4 5 #R&gt; 54 58 62 66 70 sapply can be fored to behave exactly like lapply and also return a list by setting the argument simplify to FALSE. res1 &lt;- lapply(X=lst, FUN=sum) res2 &lt;- sapply(X=lst, FUN=sum, simplify = FALSE) identical(res1, res2) #R&gt; [1] TRUE class(res2) # sapply also returns a list if forced to act like lapply #R&gt; [1] &quot;list&quot; The simplification performed by sapply can also be applied manually. R offers the commands unlist or simplify2array, that perform similar simplification operations. The code below shows that results obtained by lapply are identical to the ones generated by sapply, after passing them to the function simplify2array. res3 &lt;- sapply(X=lst, FUN=sum) res4 &lt;- simplify2array(lapply(X=lst, FUN=sum)) identical(res3, res4) #R&gt; [1] TRUE While the manual simplification is possible, it should be strictly avoided. Using the built in capabilities of the functions makes the code more readable and may be more robust. It may cover additional cases that may not be covered when own functions or manual processes are used and therefore does not fail surprisingly. vapply vapply is similar to sapply and therefore somehow identical to lapply, but it requires to specify what type of data is expected as return value. Therefore vapply supports the additional argument FUN.VALUE, that allows to specify the expected return value. For the example used above we expect l/s/vapply to return a single numeric value for each list value, therefore FUN.VALUE = numeric(1). vapply(X=lst, FUN=sum, FUN.VALUE=numeric(1)) #R&gt; 1 2 3 4 5 #R&gt; 54 58 62 66 70 If the value specified by FUN.VALUE and the actual value returned by vapply do not match an error is returned. vapply(X=lst, FUN=sum, FUN.VALUE=character(1)) #R&gt; Error in vapply(X = lst, FUN = sum, FUN.VALUE = character(1)): values must be type &#39;character&#39;, #R&gt; but FUN(X[[1]]) result is type &#39;integer&#39; vapply(X=lst, FUN=sum, FUN.VALUE=numeric(2)) #R&gt; Error in vapply(X = lst, FUN = sum, FUN.VALUE = numeric(2)): values must be length 2, #R&gt; but FUN(X[[1]]) result is length 1 Deciding which of these three functions lapply, sapply or vapply to use is obviously highly dependent on the context. While lapply always provides consistent results, the usage of sapply often helps to avoid annoiying transformations. When the input suffers from some inconsistencies vapply is the way to go, as it easily allows for checking special data types or even more complex data structures. tapply While every considered *apply-function up to now only supports one data input, tapply supports two of them, where the additional argument resembles an INDEX or grouping variable. tapply splits the provided data by the grouping values and applies the specified function to these created groups. The values for INDEX can be constructed based on factor levels, which means the provided values need to be a factor or must work when (automatically) coerced to a factor. The following code transforms our sample data mat from the other examples to a data frame with three columns. The first columns conatins the actual values that we previously found within mat, the second column indicates the column index where a specific value was placed in mat and the thrid coolumn inicates the row index, where the value was located. df &lt;- data.frame(value=as.vector(mat), # Transform example mat into a data row=rep(1:5, times=4), # frame that contains row and column col=rep(1:5, each=4)) # indices in addition to the value. head(df, n=3) # First 3 rows of the data frame #R&gt; value row col #R&gt; 1 1 1 1 #R&gt; 2 2 2 1 #R&gt; 3 3 3 1 tail(df, n=3) # Last 3 rows of the data frame #R&gt; value row col #R&gt; 18 28 3 5 #R&gt; 19 29 4 5 #R&gt; 20 30 5 5 The value in the top left corner (first row and first column) of mat is 1. Corresponding column indices are therefore col=1 and row=1. This tupel forms the first row of the created dataframe df. All other values from mat are handled in the same way. As mat is a 5 by 4 matrix, we get a dataframe with 20 rows. We can now use tapply, the values from within mat and one of the column or row indices as grouping INDEX to calculate the sum of all values that belong to a specified group. Previously we calculated the sum of all values from a specified row. To obtain these exact same results, we can use the following function call. tapply(X=df$value, INDEX = df$row, FUN = sum) #R&gt; 1 2 3 4 5 #R&gt; 54 58 62 66 70 mapply mapply stands for ‘multivariate’ apply. Its purpose is to be able to vectorize arguments to a function that is not usually accepting vectors as arguments. mapply applies a function to multiple Input arguments. The Inputs can either be lists or vectors. For a small example we define the following three vectors. n &lt;- rep(x = 15, n = 10) m &lt;- rep(x = 0, n = 10) s &lt;- seq(from = 1, to = 100, length.out = 10) The following example generates \\(n = 15\\) (n) normally distributed random numbers with mean \\(\\mu = 0\\) (m) and varying standard deviation \\(\\sigma\\) (s).While the arguments for n are for all iterations equal to 15 and for m equal to 0 the values for s differ for each group of 15 numbers. y &lt;- mapply(FUN = rnorm, n = n, mean = m, sd = s) colnames(paste0(&quot;s=&quot;,s)) #R&gt; NULL round(y, digits = 4) #R&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #R&gt; [1,] -0.3197 -1.3505 -39.5232 -40.4332 -2.2562 -15.3775 -33.4822 8.7811 #R&gt; [2,] 0.6714 4.8064 -24.6356 -59.4359 -47.6597 16.7118 107.1783 -9.6354 #R&gt; [3,] -0.8737 -2.3748 18.8879 61.5044 -39.0888 108.6574 43.8071 79.2238 #R&gt; [4,] 0.4372 -4.4322 -5.9077 58.1222 77.2545 33.2104 -27.6363 44.2322 #R&gt; [5,] -0.0837 -10.1182 -40.5401 39.1807 89.6296 -162.4549 -89.1815 -1.1865 #R&gt; [6,] -0.9776 12.3341 -22.5402 -51.5201 -36.8089 46.6530 -0.6849 -22.3735 #R&gt; [7,] 0.1972 9.0089 14.8953 18.0268 14.8789 -101.1431 -16.9997 -2.7931 #R&gt; [8,] -0.3204 -5.3583 -2.7345 -4.7797 95.4754 -34.1848 102.8748 -97.3877 #R&gt; [9,] -0.1080 -5.3385 -2.8679 -12.4431 -19.3939 -54.0886 40.5622 -118.5427 #R&gt; [10,] -0.3231 30.2885 11.4056 15.8892 -7.6271 40.5288 -49.4943 -72.5640 #R&gt; [11,] 0.7047 13.1733 -9.5396 5.4634 -20.7525 -34.0843 -23.2182 -13.9135 #R&gt; [12,] -0.4510 -21.8347 30.2582 -40.7978 65.2406 -12.3291 -0.2253 -89.4798 #R&gt; [13,] -0.1394 -7.7416 3.3178 12.1550 39.0508 62.9464 -35.0932 50.0533 #R&gt; [14,] -0.3987 -13.7062 -11.7581 1.6372 -61.2028 -53.6670 4.1789 -87.3442 #R&gt; [15,] 1.1819 7.8985 -10.9405 -32.0110 6.5984 5.7072 -4.0933 -50.7509 #R&gt; [,9] [,10] #R&gt; [1,] -26.8273 100.5772 #R&gt; [2,] 108.8673 -16.6370 #R&gt; [3,] 51.3577 140.1667 #R&gt; [4,] 38.4638 -26.8911 #R&gt; [5,] -49.1832 14.1799 #R&gt; [6,] 242.8926 54.7768 #R&gt; [7,] -9.1407 26.0456 #R&gt; [8,] -68.0724 51.5007 #R&gt; [9,] 83.3629 -15.0771 #R&gt; [10,] 84.3996 -130.0417 #R&gt; [11,] 65.7816 -6.5586 #R&gt; [12,] 100.2040 -0.2008 #R&gt; [13,] 92.6045 -123.7849 #R&gt; [14,] 36.4904 32.3467 #R&gt; [15,] 8.2663 97.7759 The outcome is illustated using the following plot. plot(NA, ylim = range(y), xlim=c(1,10), ylab = &quot;Random Number&quot;, xaxt = &#39;n&#39;, xlab= &quot;&quot;, main = &quot;Normally Distributed Random Numbers&quot;) mtext(&quot;with varying Standard Deviation&quot;) z &lt;- sapply(1:10, function(x,y){points(x = rep(x,length(y[,x])), y = y[,x])}, y = y) mapplycan be used to behave like nested for loops as the iterators can simply be anticipated and all iterator combinatinos can be generated in advance, e.g. with expand.grid. The following code shows how to iterate over all cells of a $5 10 $ matrix using mapply. nrow &lt;- 5 ncol &lt;- 10 m &lt;- matrix(1:(nrow*ncol), nrow = nrow, ncol = ncol) ic &lt;- expand.grid(1:nrow, 1:ncol) afun &lt;- function(r,c,m){m[r,c]^2} mapply(afun, r = ic[,1], c = ic[,2], MoreArgs = list(m)) #R&gt; [1] 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 #R&gt; [16] 256 289 324 361 400 441 484 529 576 625 676 729 784 841 900 #R&gt; [31] 961 1024 1089 1156 1225 1296 1369 1444 1521 1600 1681 1764 1849 1936 2025 #R&gt; [46] 2116 2209 2304 2401 2500 rapply rapply is recursive version of lapply, that applies a function recursively to every list element. l &lt;- list(a = list(a1 = 1:10 , a2 = 20:30), b = list(b1 = 1:100, b2 = 5)) rapply(object = l, f = sum) #R&gt; a.a1 a.a2 b.b1 b.b2 #R&gt; 55 275 5050 5 eapply eapply applies a function to every (visible = not hidden) element in an environment. env &lt;- new.env() env$a &lt;- 1:10 env$b &lt;- exp(-3:3) env$c &lt;- c(TRUE, FALSE, FALSE, TRUE) eapply(env, mean) #R&gt; $a #R&gt; [1] 5.5 #R&gt; #R&gt; $b #R&gt; [1] 4.535125 #R&gt; #R&gt; $c #R&gt; [1] 0.5 "],["technical-background.html", "6.2 Technical Background", " 6.2 Technical Background The paradigm of vectorization is closedly linked to functional programming and one can find a few reasons (e.g. execution speed in some cases), to make use of them. A very comparable set of functions an some additional patterns are implementd in the purrr package that is part of the tidyverse. "],["parallelization-using-vectorized-functions.html", "6.3 Parallelization using vectorized functions", " 6.3 Parallelization using vectorized functions A very convenient method of parallelization is by utilizing the parallel package that provides parallelized versions of functions from the *apply-family. However as it uses a technique called forking the usage of these function is limited to operating systems that are based on Unix such as Linux or MacOS. library(parallel) num.cores &lt;- detectCores() num.cores #R&gt; [1] 3 As you can see this document was generated on a computer with 3 computing cores. Depending on the specific processor these may either be real cores or logical cores. The number only differs if a real core can handle two tasks (threads) at once and thus behaves like two actual cores. In the case of my computer the CPU has 3 real cores. regnames &lt;- names(mtcars[,-1]) regoptions &lt;- rep(list(c(T,F)),length(regnames)) regselector &lt;- as.matrix(expand.grid(regoptions)) colnames(regselector) &lt;- regnames modellist &lt;- apply(regselector,1,function(x){as.formula(paste(c(&quot;mpg ~ 1&quot;, regnames[x]), collapse = &quot; + &quot;))}) system.time(res.sequential &lt;- lapply(modellist, function(f){lm(f,data=mtcars)})) #R&gt; user system elapsed #R&gt; 1.444 0.030 1.497 system.time(res.parallel &lt;- mclapply(modellist, function(f){lm(f,data=mtcars)},mc.cores=num.cores)) #R&gt; user system elapsed #R&gt; 1.202 0.357 0.911 rsquared.sequential &lt;- sapply(res.sequential, function(x){summary(x)$r.squared}) rsquared.parallel &lt;- sapply(res.parallel, function(x){summary(x)$r.squared}) table(rsquared.sequential == rsquared.parallel) #R&gt; #R&gt; TRUE #R&gt; 1024 summary(rsquared.sequential) #R&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #R&gt; 0.0000 0.7899 0.8325 0.8106 0.8499 0.8690 summary(rsquared.parallel) #R&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #R&gt; 0.0000 0.7899 0.8325 0.8106 0.8499 0.8690 "],["exercises-5.html", "Exercises", " Exercises From May 10, 2022 Description R script function determiningLinMod() determiningLinMod "],["applications---graphics.html", "Applications - Graphics", " Applications - Graphics "],["word-puzzle.html", "Chapter 7 Word Puzzle", " Chapter 7 Word Puzzle An evil word puzzle is a puzzle game where you have to find a special word. The whole puzzle consists solely of letters from within the word itself and does sometimes not even include the word you are looking for. "],["requirements.html", "7.1 Requirements", " 7.1 Requirements Write a program evilpuzzle() that generates an evil word puzzle (optionally with or without the word) and make sure that every word with at least 3 letters can be used to generate a puzzle with desired but meaningful dimensions. "],["solution.html", "7.2 Solution", " 7.2 Solution Let us start to solve the Excercise by generating a matrix full of letters from the desired word. We can easily do this using the sample()-command. If we want to fill the matrix one by one we can do this easily with two nested for-loops we just discussed, but the matrix()-command is much faster and the better option here. word &lt;- c(&quot;K&quot;,&quot;A&quot;,&quot;Y&quot;,&quot;A&quot;,&quot;K&quot;) cols &lt;- 15 rows &lt;- 10 # Generate word matrix with loops (very inefficient and slow!) mat &lt;- matrix(NA,nrow=rows,ncol=cols) for ( idx in 1:rows) { for (jdx in 1:cols) { mat[idx,jdx] &lt;- sample(word,size=1) } } # Generate word matrix vectorized (the way to go!) mat &lt;- matrix(sample(word,replace=T,size=rows*cols), nrow = rows, ncol = cols) We now have the matrix mat that contains letters from our word KAYAK in totally random order. The bigger the matrix gets via the cols and rows arguments the more likely is it that it contains the word itself up to a few times. Therefore, we have to check for that and correct this when necessary. Logically there are a couple of approaches to solve the task of controlling if mat contains our word more often and of course it is possible to directly check for this while generating the matrix. Another option is to decompose the matrix by rows and columns and check if the resulting and/or the reversed string contains our word. A more loop-based solution is going through the letters one by one and check if starting with the actual position the word can be found either horizontally, horizontally reversed, vertically or vertically reversed. Every time the word is found, the corresponding part is replaced with new random letters drawn from the word. As we can not be sure that we have not accidentally created the word by replacing the letters in an earlier position we have to go once again through the matrix after finishing our first iteration so that we can be sure that the word is not included anymore. So our first step towards the solution is to create a complete word free matrix. Using loops this can be translated in two nested for-loops running through rows and columns of mat and a while-loop that encapsulates the nested loops and takes care of starting over when we have replaced the occurring word in the matrix. In this case, our skeleton looks like this: check &lt;- TRUE # Make sure the while loop is entered while ( check == TRUE ){ check &lt;- FALSE # Assuming one iteration is enough we can leave the # loop after the current iteration. for ( row in 1:nrow(mat)){ # Loop running through rows for (col in 1:ncol(mat)){ # Loop running through columns # Code to solve the actual problem! } # End for (columns) } #End for (rows) } #End while With that basic setup we run through the entire matrix one element at a time. We can easily adress the element using the running variables row and col from the for-loops using them as indices in brackets. Depending on the word length we can now calculate which submatrix of mat has to be checked for our word. Namely these areas are the following ones: # Obtain ranges to check for the word mat[row,col:(col+length(word)-1)] # Horizontal (left to right) mat[row,(col-length(word)+1):col] # Horizontal reverse (right to left) mat[row:(row+length(word)-1),col] # Vertical (top to bottom) mat[(row-length(word)+1):row,col] # Vertical reverse (bottom to top) As it makes sense not to wrap everything in if-else-clauses we can simply put the indices which we have to check in lists and use them out of another loop to compare them with the actual word. Wrapping them in a list so that we can dynamically address them is easily done like so: rows &lt;- list(row, # List for row indices row, row:(row+length(word)-1), (row-length(word)+1):row) cols &lt;- list(col:(col+length(word)-1), # List for column indices (col-length(word)+1):col, col, col) Now we can loop through the list entries and perform our comparison and then act accordingly, which can easily be done by using an if-clause and checking if the submatrix we are looking at is identical with our word. Sadly this is not enough. As we dynamically calculate the indices for the submatrix when using our lists rows and columns we have not yet checked if the indices exist. For example, when looking at the top left element in the matrix (mat[1,1]) we can not check if the indices from bottom to top and from right to left correspond to our word, because these indices do not exist and are negative numbers in our list. In this case, R responds with an error: mat[1,(1:-5)] #R&gt; Error in mat[1, (1:-5)]: only 0&#39;s may be mixed with negative subscripts The same happens when we look at the bottom right element of the matrix and try to examine the cases horizontally and vertically as these do not exist. That means before overwriting an occurrence we have to check for 5 things. Are all calculated row indices bigger than zero and really in the matrix? Are all calculated column indices bigger than zero so that they can really be addressed? Are all calculated row indices smaller than the maximum row-dimension of the matrix and therefore not out of mats boundaries? Are all calculated column indices smaller than the maximum column-dimension of the matrix so that they are really present in the matrix? Does the submatrix equal the word we are looking for? Due to the properties of the &amp;&amp;-Operator, we can perform all these comparisons in one if clause including the check if the submatrix equals the word. Even though the indices in the matrix do not exist in the matrix (be sure to understand why this does not work with the single AND operator (&amp;) which leads to the following code including the overwriting with a fresh randomly selected set of letters from our word: for ( idx in 1:length(rows)){ # Could have used length(cols) instead # If all rows and cols are in matrix and not out of bound and # the range equals the word resample this section of the matrix. if ( all(rows[[idx]] &gt; 0) &amp;&amp; # Check 1 all(cols[[idx]] &gt; 0) &amp;&amp; # Check 2 all(rows[[idx]] &lt;= nrow(mat)) &amp;&amp; # Check 3 all(cols[[idx]] &lt;= ncol(mat)) &amp;&amp; # Check 4 identical(mat[rows[[idx]], cols[[idx]]],word)) { # Check 5 # Overwrite the submatrix with fresh sampled letters mat[rows[[idx]],cols[[idx]]] &lt;- sample(word, replace=T, size=length(word)) # After replacing we have to check the whole matrix again, # there we make sure to reenter the while loop! check &lt;- TRUE # Feature to count how many occurrences have been replaced wordcounter &lt;- wordcounter + 1 } #End if } #End for When the while-loop has terminated we are left with a clean matrix that surely does not contain our word in the checked dimensions. If you want to be really evil you can printout the puzzle, give it to some friends and let them search for the word. If you only want to create a hard puzzle you can choose a random location to overwrite the letters there with the word. All that’s left to do is wrapping the existing code in a function with the requested name evilpuzzle(). If you want to improve the routine a little bit and make it more flexible you can implement some switches using if-else-clauses to control if the puzzle should be evil or super evil, meaning containing the search word or not and after placing the word within mat crosscheck if that only leads to a single occurrence and cannot be assembled into multiple hits when trying to solve the puzzle. If that is not enough you can also implement checking for diagonal occurrences of the word. The function corresponding to the exercise after assembling the discussed bits may look like this: evilpuzzle &lt;- function(word = c(&quot;T&quot;,&quot;E&quot;,&quot;S&quot;,&quot;T&quot;), cols = 15, rows = 10, inclword=TRUE) { # Check if provided word is longer than 3 characters if ( sum(nchar(word)) &lt; 3 ) { stop(&quot;The provided word has to be longer than 3 characters!&quot;) } # Initialize wordcounter &lt;- 0 # Generate word matrix vectorized mat &lt;- matrix(sample(word,replace=T,size=rows*cols), nrow = rows, ncol = cols) check &lt;- TRUE # Make sure the while loop is entered while ( check == TRUE ){ check &lt;- FALSE # Assuming one iteration is enough we can leave the # loop after the current iteration. for ( row in 1:nrow(mat)){ # Loop running through rows for (col in 1:ncol(mat)){ # Loop running through columns rows &lt;- list(row, # List for row indices row, row:(row+length(word)-1), (row-length(word)+1):row) cols &lt;- list(col:(col+length(word)-1), # List for column indices (col-length(word)+1):col, col, col) # &lt;-- Indent removed for better overview outside of IDE! for ( idx in 1:length(rows)){ # Could have used length(cols) instead # If all rows and cols are in matrix and not out of bound and # the range equals the word resample this section of the matrix. if ( all(rows[[idx]] &gt; 0) &amp;&amp; # Check 1 all(cols[[idx]] &gt; 0) &amp;&amp; # Check 2 all(rows[[idx]] &lt;= nrow(mat)) &amp;&amp; # Check 3 all(cols[[idx]] &lt;= ncol(mat)) &amp;&amp; # Check 4 identical(mat[rows[[idx]],cols[[idx]]],word)) { # Check 5 # Overwrite the submatrix with fresh sampled letters mat[rows[[idx]],cols[[idx]]] &lt;- sample(word, replace=T, size=length(word)) # After replacing we have to check the whole matrix again, # therefore we make sure to reenter the while loop! check &lt;- TRUE # Feature to count how many occurrences have been replaced wordcounter &lt;- wordcounter + 1 } #End if } #End for # &lt;-- Indent removed for better overview outside of IDE! } # End for (columns) } #End for (rows) } #End while # Message to the user how often the word has been replaced cat(&quot;The word&quot;, paste(word,sep=&quot;&quot;, collapse=&quot;&quot;), &quot;was replaced&quot;, wordcounter, &quot;times! \\n&quot; ) # Add word to clean matrix if desired (only horizontally) if ( inclword == TRUE) { row &lt;- sample(1:nrow(mat),size=1) col &lt;- sample(1:(ncol(mat)-length(word)),size=1) mat[row,col:(col+length(word)-1)] &lt;- word } # Return the finished matrix to the user return(mat) } #End function The function evilpuzzle() now suits the desired needs and is a promising candidate to fulfill the needs described in the exercise. However, there is much room for improvement as the program only supports inserting the word horizontally and as it does not support inserting it differently the user is not able to select in which way it should be represented in the matrix. As stated before the chosen approach is not very efficient as the word is compared with the submatrix for all four orientations and for every element in the matrix. Further following improvements are left to the user: The performed comparison of the submatrices with the word is not necessary for every element of the matrix. If the element we are looking at in a certain iteration does not equal the first letter of word there is no need to compare the whole submatrices. Therefore we can implement such a simple test to improve the efficiency of our program. One major flaw of the program is that the word (if inserted in the matrix), is only inserted horizontally. The optimal solution would allow the user to select between inserting it at a random or desired orientation either horizontally, horizontally reversed, vertically or vertically reversed. A bonus would be to make the selection of the orientation of the words also random to allow a user to generate a puzzle for themselves. Right now the program only supports searching vertically or horizontally word occurrences and ignores the option of diagonal combinations. Therefore more checks can be implemented to make the puzzle even harder. The four diagonal cases should also be considered in the options for inserting the word in the matrix. Inserting the word in the matrix can lead to multiple possible combinations. Therefore a control mode should be programmed that counts the occurrences in the matrix and corrects multiple occurrences in a suitable way so that the word can only be found once in the final puzzle. There are many other and much more efficient ways to solve this exercise - we just have thrown together what we learned so far. If you can not get enough you can try generating the word puzzle in a totally different way, the string section can solve as source as inspiration. After this little exercise, you should have a good feeling of how loops and conditional expression can be used to actually do something (more or less) productive and you should have beaten the concepts we just discussed. "],["blockchain.html", "Chapter 8 Blockchain", " Chapter 8 Blockchain Cryptocurrencies, as we know them today, were introduced to the world with a whitepaper entitled “Bitcoin: A Peer-to-Peer Electronic Cash System” in 2008 by Satoshi Nakamoto. The paper promises nothing more than a decentralized, incorruptible database of monetary transactions that revolutionizes the concept of trust by eradicating third parties like banks, clearing agencies or lawyers. From there on the idea started to grow, spread widely and create enormous expectations so that the underlying technology - the blockchain - reached its peak of inflated expectations just before dropping hard with the bitcoin currency into the trough of disillusionment. While many papers have been written that discuss potential use cases and disruptive scenarios for the blockchain, very few actually understand the underlying working principles. While the basic idea is quite intuitive and easy to sell, the question of how blockchains actually work on a technical level is a bit harder to understand. Programming and developing a small demo blockchain is, however, the best way to understand the underlying technology and is a great programming exercise. The goal of any blockchain is to store data in a decentralized and secure (meaning not manipulatable) way. In the case of cryptocurrencies, this data is usually multiple financial transactions between two parties. Obviously, the general principle is not limited to this use-case. As the word block-chain suggests, the construction principle is a chain of (data) blocks. However to understand what chains these portions of data we need to explain the concept of hash functions first. In order to know why and how people are participating in the creation of blockchains and why a blockchain focusses on security, the concept of a proof-of-work (PoW) algorithm needs to be discussed. "],["hash-functions.html", "8.1 Hash Functions", " 8.1 Hash Functions A hash function is any function that maps data of arbitrary size onto data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. The most common application of hash functions is in cryptography. A cryptographic hash function allows verifying whether the input data maps onto a given hash value. However, when the input is unknown it is very difficult or (almost) impossible to reconstruct it from a given hash value. The following figure exemplarily illustrates the working principle of hash functions. The four inputs are mapped to one of the outputs. While the inputs show varying lengths (and can even be whole documents indicated by the word processor logo) the output is always a decimal number ranging from zero to eight (3-bit hash value). Figure 8.1: Hash Function A common example of hash functions besides blockchains is storing passwords. When a user creates a user account his password is not kept in clear text, but the hash values are the only piece of information about the password saved in the data basis. Even if an attacker or hacker gains access to the database and manages to steal a copy of the information he can not log into a user’s account, because the hash value is worthless to him and cannot be used to reconstruct the password. This is exemplified by the following R code, which uses the SHA-256 algorithm to create hash values. Contrary to the simple example the hash value is not 3 but 256 bits long. The shown hash values are coded as hexadecimal values. library(digest) # Load package &#39;digest&#39; that implements hash functions accessGranted &lt;- function(passwd){ hash &lt;- digest(passwd, algo = &quot;sha256&quot;) databasis &lt;- c(&quot;27f16d4aece13cf0443ebfd7deb7be198d0aa6adb637ca37b3c72f4fb2e01d7e&quot;, &quot;f92733254bac134a89313b489af49a20f8d0f76543824f503361ce3f92ca4d25&quot;, &quot;67f9a8c826a4e3d958d265b999030e85632cf2a2f65781cab08dfbe338f434f1&quot;) access &lt;- hash %in% databasis return(access) } accessGranted(passwd = &quot;uberpassword&quot;) # Guess No. 1 #R&gt; [1] FALSE accessGranted(passwd = &quot;administrator&quot;) # Guess No. 2 #R&gt; [1] FALSE # Not even using a &#39;stolen&#39; hash value as password helps accessGranted(passwd = &quot;27f16d4aece13cf0443ebfd7deb7be198d0aa6adb637ca37b3c72f4fb2e01d7e&quot;) #R&gt; [1] FALSE Obviously, the main feature of cryptographic hash functions is their irreversibility, which is strongly dependent on the actual algorithm used to create the hash value. Making and breaking hash functions is an active area of research which has revealed many vulnerabilities and lead to some hashing algorithms being insecure. The algorithm used in the examples above is called “SHA-256” and is considered secure as of today (13.05.2022). Other popular algorithms such as MD5or the algorithms used for storing Windows XP Passwords LM-Hash and NTLM-Hash are compromised and insecure, meaning that the hash value can be used to recover the password. Exercises Can you find a password that causes the function accessGranted to return TRUE? The ten most widely used passwords are 123456, 123456789, qwerty, password, 111111, 12345678, abc123 and 1234567. Which of these password produces the following SHA-256 hash: 27f16d4aece13cf0443ebfd7deb7be198d0aa6adb637ca37b3c72f4fb2e01d7e "],["blocks.html", "8.2 Blocks", " 8.2 Blocks After clarifying the basic concepts let us discuss the actual blockchain data. As mentioned earlier a block in the case of cryptocurrencies contains monetary transactions. However, as use cases vary so can the type of data stored into the blockchain. Using transactional data is only one example, it is, however, the most popular one. Additionally, to the transaction data a block needs to contain some additional information: Index: A number identifying the position of the block in the chain. Timestamp: The time at which the block was generated. Previous Hash: Hash value calculated over the last block. Hash: Hash value of the current block. Difficulty: Central parameter for the proof of work algorithm. Nonce: A number that can be randomly set; required for the proof of work algorithm. Let us create the first block of the chain. This is usually called the genesis block. As it cannot contain a hash over previous blocks, the value for previousHash is set to NULL. genesisBlock &lt;- structure(list( index = 1, timestamp = Sys.time(), data = &quot;Genesis&quot;, previousHash = NULL, difficulty = 0, nonce = 1, hash = NULL),class=c(&quot;block&quot;,&quot;list&quot;)) For a nice printout on the console and a compact form of the following, we define the following printing function for our block. To keep the output compact even if long data objects are stored into the block, we suppress displaying the data by default. print.block &lt;- function(x, showdata=F){ cat(&quot;:::::::::: BLOCKCHAIN BLOCK&quot;,x$index,&quot;:::::::::: \\n&quot;) cat(&quot;Timestamp:&quot;, x$timestamp,&quot;\\n&quot;) cat(&quot;Previous Hash:&quot;, x$previousHash,&quot;\\n&quot;) cat(&quot;Difficulty:&quot;, x$difficulty,&quot;\\n&quot;) cat(&quot;Hash:&quot;, x$hash,&quot;\\n&quot;) if(showdata){ cat(&quot;Data:&quot;,&quot;\\n&quot;) print(x$data) } } Using a simple function, that concatenates the information from the given block and calculates a hash value we can complete the information needed for the block. hashBlock &lt;- function(block){ block$hash &lt;- digest(c(block$index, block$timestamp, block$data, block$previousHash, block$difficulty, block$nonce), &quot;sha256&quot;) return(block) } genesisBlockHashed &lt;- hashBlock(genesisBlock) genesisBlockHashed #R&gt; :::::::::: BLOCKCHAIN BLOCK 1 :::::::::: #R&gt; Timestamp: 1652431135 #R&gt; Previous Hash: #R&gt; Difficulty: 0 #R&gt; Hash: 08171d9343895c8dbc39d4120c154c946c6936a69a29884cbc9573a41f07103f "],["proof-of-work-pow.html", "8.3 Proof of Work (PoW)", " 8.3 Proof of Work (PoW) A proof of work algorithm is an economic measure designed to prevent service abuses, by presenting a puzzle (computational challenge) to the service requester. Solving this puzzle is usually difficult and/or time-consuming but a particular service is only granted if the right solution to the puzzle is presented. Therefore, a proof of work mechanism is an effective way to avoid too many requests to a service. The mentioned puzzle can be for example based on a random process with low probability so that a lot of trial and error is required before a valid solution can be found. However, while there are many random processes with low probability a proof of work needs to have a valid and verifiable solution. The most widely used proof of work system is called Hashcash and implemented into the Bitcoin protocol. Simple PoW Example For explanatory purposes, we start by using a simple proof of work system that is easy to understand and easily implementable. Remember that the work to obtain the proof must be feasible on the side of the service requester, but easy to check for the service provider. For our example, we require a service requester to find a number that is divisible by 99 and a given and previously known proof number, which can be done with the following two functions. # Calculate a new proof of work proofOfWork &lt;- function(lastProof){ proof &lt;- lastProof + 1 while (!isProofValid(proof, lastProof)){ proof &lt;- proof + 1 # Increment proof by 1 until a valid solution is found. } return(proof) } # Check if a proof of work is valid isProofValid &lt;- function(proof, lastProof){ return(proof %% 99 == 0 &amp;&amp; proof %% lastProof == 0) } P1 &lt;- 1 # Initial Proof P2 &lt;- proofOfWork(lastProof = P1) # Second Proof P3 &lt;- proofOfWork(lastProof = P2) # Third Proof P4 &lt;- proofOfWork(lastProof = P3) # Fourth Proof c(P1, P2, P3, P4) #R&gt; [1] 1 99 198 396 While the example hopefully illustrates the principle, it must be kept in mind that this is not a proper proof of work algorithm. It can be easily seen that the sequence of proof numbers follows an identifiable pattern and thus can be calculated directly ( and not spending as much CPU time as desired for the solution). pvec &lt;- 1 # Initial Proof for(idx in 1:10){ pvec &lt;- append(pvec,proofOfWork(pvec[idx])) } plot(x=1:length(pvec),y=pvec, xlab=&quot;Steps of Proofs&quot;, ylab=&quot;Proof Number&quot;) Additionally to the predictability of the proof numbers in the simple algorithm our example has numerous additional disadvantages that disqualify it for productive use. In the case of the Hashcash algorithm, the challenge is actually much more randomized and based on hash functions. The challenge is to generate hash values that contain a certain amount of leading zeros. Additionally, there needs to be a mechanism to account for growing computing power, so that new solutions to provided puzzles are found in approximately equal time slices. This in turns means that the difficulty of the puzzles needs to be adjustable. Combining PoW and Hashing A good proof of work algorithm for cryptocurrencies should not only be adjustable in terms of its difficulty but should also be random, so that it is not possible to predict its outcome and save computing power. It is therefore comparable to a lottery. A more in-depth example for a proof of work algorithm is shown in the following code snippet. proofOfWork &lt;- function(block){ hashedBlock &lt;- hashBlock(block) targetHashBeginning &lt;- paste0(rep(0, block$difficulty),collapse = &quot;&quot;) while( !(substr(hashedBlock$hash,1,block$difficulty) == targetHashBeginning) ){ hashedBlock$nonce &lt;- hashedBlock$nonce + 1 hashedBlock &lt;- hashBlock(hashedBlock) } return(hashedBlock) } For a practical example let us use the genesis block that we just created as the starting point for our own blockchain and feed it to the new proofOfWork function after setting the difficulty to a moderate value. system.time(proofOfWork(genesisBlock)) #R&gt; user system elapsed #R&gt; 0.013 0.000 0.013 genesisBlock$difficulty &lt;- 3 system.time(proofOfWork(genesisBlock)) # Execute again with increased difficulty #R&gt; user system elapsed #R&gt; 0.496 0.029 0.525 Exercises Please explain the principle of the new and more complex proofOfWork function. "],["chaining-blocks.html", "8.4 Chaining Blocks", " 8.4 Chaining Blocks Due to the fact that the hash for each block considers not only the core information like index, timestamp and the data but also the hash of the previous block, applying the discussed process repeatedly leads to a chain of blocks. This chain is built on all information from all previous blocks due to the hashing process, meaning that one can only calculate the valid hash if one knows the correct and very easily verifiable hash from the block before. This leads to an unbreakable chain that proves the integrity of all information, changing prior information requires to recalculate all hashes. Let us put together what we learned and build our first block chain starting with the already generated genesis block. genesisBlockHashed #R&gt; :::::::::: BLOCKCHAIN BLOCK 1 :::::::::: #R&gt; Timestamp: 1652431135 #R&gt; Previous Hash: #R&gt; Difficulty: 0 #R&gt; Hash: 08171d9343895c8dbc39d4120c154c946c6936a69a29884cbc9573a41f07103f Let us assume that we received a single transaction request that needs to be included in the second block of the blockchain. The following transaction mimics the first bitcoin transaction, where 10.000 bitcoins were used to buy two pizzas at a Papa Johns restaurant. The next transaction is just another example that we can include in the third block to extend our chain a little more. transaction.b2 &lt;- data.frame(Sender=&quot;Papa&quot;, Receiver=&quot;John&quot;, Amount=10000) transaction.b2 #R&gt; Sender Receiver Amount #R&gt; 1 Papa John 10000 transaction.b3 &lt;- data.frame(Sender=&quot;Larry&quot;, Receiver=&quot;Tim&quot;, Amount=500) transaction.b3 #R&gt; Sender Receiver Amount #R&gt; 1 Larry Tim 500 Using the function addBlock we can add the transaction to our blockchain. addBlock &lt;- function(chain, newdata=NULL, difficulty=NULL){ # Get Information from the existing chain blkidx &lt;- sapply(chain, function(x)x$index) # Set difficulty conditional on user input if(is.null(difficulty)){ dfc &lt;- sapply(chain, function(x)x$difficulty) dfc &lt;- dfc[which.max(blkidx)] } else { dfc &lt;- difficulty } # Generate new block newBlock &lt;- structure(list( index = max(blkidx) + 1, timestamp = Sys.time(), data = newdata, previousHash = chain[[which.max(blkidx)]]$hash, hash = NULL, difficulty = dfc, nonce = 1),class=c(&quot;block&quot;,&quot;list&quot;)) # Calculate Proof of Work and add block to the chain newBlockHashed &lt;- proofOfWork(newBlock) chain &lt;- append(chain, list(newBlockHashed)) return(chain) } chain &lt;- list(genesisBlockHashed) # Intialize the Blockchain chain &lt;- addBlock(chain, newdata = transaction.b2) # Add the transation data chain &lt;- addBlock(chain, newdata = transaction.b3) # Add the transation data chain # Block Chain with 3 blocks #R&gt; [[1]] #R&gt; :::::::::: BLOCKCHAIN BLOCK 1 :::::::::: #R&gt; Timestamp: 1652431135 #R&gt; Previous Hash: #R&gt; Difficulty: 0 #R&gt; Hash: 08171d9343895c8dbc39d4120c154c946c6936a69a29884cbc9573a41f07103f #R&gt; #R&gt; [[2]] #R&gt; :::::::::: BLOCKCHAIN BLOCK 2 :::::::::: #R&gt; Timestamp: 1652431136 #R&gt; Previous Hash: 08171d9343895c8dbc39d4120c154c946c6936a69a29884cbc9573a41f07103f #R&gt; Difficulty: 0 #R&gt; Hash: d3aabc1cd9b9a889a3f4736376a0a826845141d4570a625003583e77f8275efb #R&gt; #R&gt; [[3]] #R&gt; :::::::::: BLOCKCHAIN BLOCK 3 :::::::::: #R&gt; Timestamp: 1652431136 #R&gt; Previous Hash: d3aabc1cd9b9a889a3f4736376a0a826845141d4570a625003583e77f8275efb #R&gt; Difficulty: 0 #R&gt; Hash: 341fdd24617918d1f28b7cecad225a6c906cd265486e94aaa0e390cdf0a61fa9 "],["security.html", "8.5 Security", " 8.5 Security While it should now be already pretty clear that the data in the blockchain is not manipulatable without recalculating all hashes in the chain, we can look at a few examples of what happens when we manipulate information. Considering the discussed principles, we consider two possible manipulation attacks. First one can manipulate the data stored in the blockchain directly. The second approach that we analyze is the case where someone just inserts a block in the chain. We utilize the following two functions that use the hashes to check the integrity of the information of a block and analyze the linkage of the chain. verifyBlock &lt;- function(block){ rehashedBlock &lt;- hashBlock(block) validHash &lt;- rehashedBlock$hash == block$hash return(validHash) } verifyChain &lt;- function(chain){ actualHashes &lt;- unlist(sapply(chain, function(x)x$hash)) previousHashes &lt;- unlist(sapply(chain, function(x)x$previousHash)) hashesLinked &lt;- previousHashes == actualHashes[-length(actualHashes)] if(!all(hashesLinked)){ message(&quot;The provided chain is not linked at block index &quot;, which(!hashesLinked),&quot; (link manipulation)!&quot;) return(all(hashesLinked)) } blockVerification &lt;- sapply(chain,verifyBlock) validChain &lt;- all(blockVerification) if (any(!blockVerification)){ message(&quot;The chain broke at block index &quot;, which(!blockVerification),&quot; (block manipulation)!&quot;) } return(validChain) } verifyBlock(chain[[2]]) # Check block 2 of the chain #R&gt; [1] TRUE verifyChain(chain) # Check the entire chain #R&gt; [1] TRUE Manipulation 1 As the entire information of the blockchain is freely available on the internet and everybody can download it, everybody can try to manipulate it as well. # Manipulate amount for first transaction in block 2 evilchain &lt;- chain evilchain[[2]]$data$Amount[1] &lt;- 9999 verifyBlock(evilchain[[2]]) #R&gt; [1] FALSE verifyChain(evilchain) #R&gt; The chain broke at block index 2 (block manipulation)! #R&gt; [1] FALSE Manipulation 2 Let us now inject a block that contains transactional data to steal money. evilBlock &lt;- structure(list( index = 2, timestamp = Sys.time(), data = data.frame(Sender=&quot;Everyone&quot;, Receiver=&quot;Me!&quot;, Amount=5), previousHash = chain[[1]]$hash, difficulty = 0, nonce = 1, hash = NULL),class=c(&quot;block&quot;,&quot;list&quot;)) evilBlockHashed &lt;- hashBlock(evilBlock) evilchain &lt;- c(chain[1],list(evilBlockHashed),chain[3]) # Substitute block 2 verifyBlock(evilchain[[2]]) #R&gt; [1] TRUE verifyChain(evilchain) #R&gt; The provided chain is not linked at block index 2 (link manipulation)! #R&gt; [1] FALSE These examples hopefully help to clarify why the blockchain cannot be easily manipulated. Additionally to the already discussed measures, the blockchain is shared over a network. While it would be easy to manipulate a blockchain and recreate the linkage by re-hashing every block, this disregards the other participants in the system. When a new block is required, the necessary information is sent out to the network and every single participant. Everybody interested in participating can now try to solve the proof of work that is similar to the algorithm we implemented. The process of “finding” a new block by solving the computational puzzle is actually called mining and the goal in case of bitcoin is to generate one new block every 10 minutes (600 seconds). The participant who first solves the puzzle and submits a correct solution to the network is rewarded with bitcoins. Mining new blocks, therefore, creates new bitcoins. New blocks are sent out to all nodes in the network so that everyone has an up to date copy of the blockchain and can verify the results himself. This ensures that the power to create new blocks and bitcoins is and stays decentralized. While there are some more aspects regarding the security of the chain that deal with special properties of and attacks tailored to the network these techniques often exploit imbalances of the distribution of computing power in the network and are not discussed further. "],["wrap-up.html", "8.6 Wrap up", " 8.6 Wrap up In this session, we created a very small example of a (locally) functional blockchain. Obviously, all explanations are tailored to the goal of getting more in depth of the underlying technology and demystifying the whole thing. The programmed routines outline the basic principles that are also present in the real cryptocurrencies, however, to develop a fully functioning blockchain for production purposes one needs much more components like a client-server infrastructure, encryption of communication, wallets and much more. "],["forecasting.html", "Chapter 9 Forecasting", " Chapter 9 Forecasting Forecasting is an important and common task in businesses to schedule production, arrange transportation, allocate personnel or financial resources and provide guidance for planning processes. Generally, the terms Forecasting, Planning, and Goals should be distinguished, but are commonly intermingled in practice. Forecasting is a process that tries to predict future events or figures as accurately as possible. All given information at the time of forecast creation (at the forecast origin) should be used and known future events should be incorporated into the forecast generation process. Goals are defined targets that the organization, division or person would like to achieve. While goals should be defined realistically and therefore based on forecasts and derived as output of (a not over-ambitious) planning process, this is not always the case. Goals can (and often are) defined due to organizational pressure or established for political reasons, without convincing strategies of how to achieve them. Planning is the process that connects goals and forecasts and involves figuring out paths and actions to align forecasts and goals. This can either be by adjusting goals, but usually focusses on measures that increase performance figures. As a side-effect, forecasts generated by a suitable method, start to pick up trends visible in new data so that the forecasts (hopefully) converge towards the established goals. Management usually distinguishes between short-term, medium-term and long-term tasks, which in term means that modern organizations require forecasts for all three horizons as part of the decision preparation and making process. Dependent on the business context of the company short-term forecasts can be used for scheduling and allocation tasks, which often also requires forecasts for demand or sales figures. Medium-term activities aim to determine future requirements in terms of materials, machinery, etc. and must be adaptive to the commercial environment. Long-term forecasts are usually incorporated in the planning of strategic scenarios and must be made with respect to environmental factors, shifts in the market landscape and additional external effects. Predicting future events, sales or demand figures grows in difficulty with the time offset from the day of forecast generation. Academia has led to many approaches that can be used for all - short, medium and long-term horizons - but for the sake of simplicity, we concentrate on methods that can be used to generate short- to medium-term forecasts. "],["time-series-data.html", "9.1 Time Series Data", " 9.1 Time Series Data Before we can start forecasting some data, we need to talk about the time series data-structure that can be represented in R and is available via the ts()-function. A time series is a series of data points ordered along a time-axis. Most commonly used are time with data points that are equally spaced, which is referred to as a sequence in discrete-time. While there are also datasets and forecasting methods for non-equidistant series (aka continuous time-series), the vast majority of applications, especially in business administration, concentrates on data with equidistant observations and in discrete-time. The following code snippet shows how to simulate a time series based on a linear trend with intercept. This is essentially the same concept as linear regression but reversed. First, the coefficients for the trend line are set, then the true line is calculated and afterward overlaid with a standard normally distributed error. set.seed(1) # Fix the RNG b0 &lt;- 0.5 # Intercept Coefficient b1 &lt;- 0.7 # Slope Coefficient x &lt;- 1:25 # Abscissa Data (Time Index) signal &lt;- b0 + b1 * x # Generate the Signal noise &lt;- rnorm(length(x)) # Generate the Noise series &lt;- ts(signal + noise) # Combine Signal and Noise plot(series) However, obviously, time series data can be more complex and also exhibit a special time structure. While the time series properties are not explicitly defined, one can easily imagine a series that has a more complex structure than just a time index as above. Time series data without an underlying structure is assigned a frequency value of 1, which can, for example, stand four hourly data. Adding more complex frequency information to the data allows adding calendarial structure, monthly data, meaning a time series with a single data point for each month, for example, has frequency = 12. An example of a monthly time series is the AirPassengers series, that shows the monthly total of international airline passengers from 1949 to 1960. Some additional information for this series is available via ?AirPassengers. AirPassengers #R&gt; Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #R&gt; 1949 112 118 132 129 121 135 148 148 136 119 104 118 #R&gt; 1950 115 126 141 135 125 149 170 170 158 133 114 140 #R&gt; 1951 145 150 178 163 172 178 199 199 184 162 146 166 #R&gt; 1952 171 180 193 181 183 218 230 242 209 191 172 194 #R&gt; 1953 196 196 236 235 229 243 264 272 237 211 180 201 #R&gt; 1954 204 188 235 227 234 264 302 293 259 229 203 229 #R&gt; 1955 242 233 267 269 270 315 364 347 312 274 237 278 #R&gt; 1956 284 277 317 313 318 374 413 405 355 306 271 306 #R&gt; 1957 315 301 356 348 355 422 465 467 404 347 305 336 #R&gt; 1958 340 318 362 348 363 435 491 505 404 359 310 337 #R&gt; 1959 360 342 406 396 420 472 548 559 463 407 362 405 #R&gt; 1960 417 391 419 461 472 535 622 606 508 461 390 432 plot(AirPassengers) Obviously, the process that is required to simulate a time series comparable to AirPassengers is not just a single linear trend as the series clearly shows peaks for the summer months each year. This data feature is called seasonality and the process that yields the respective time series (without the error term) is called a Data Generation Process (DGP). Forecasting in a technical sense is essentially applying methods to identify this DGP by separating the observed data into signal and noise. The statistical and mathematical description of the DGP can then be used to extrapolate the deterministic structure of the data. Forecasts generated on the basis of this idea deliver future values, that are most likely to be realized in the future (to the best of the knowledge at the time of generation). "],["forecasting-process.html", "9.2 Forecasting Process", " 9.2 Forecasting Process While a forecasting process can be arbitrarily complex, every forecaster should pass through the following five steps: 1) Problem Definition, 2) Information Gathering, 3) Explanatory Analysis, 4) Configure and Estimate Models, 5) Generate and Evaluate Forecasts. While all of these steps are equally important, in some academic or practical applications the first two are defined by the concrete forecasting task - it is, however, still very valuable to challenge given problem definitions, assumptions or data, especially when one does not have insights into every substep of the stages one and two. The following figure illustrates the process. Figure 9.1: Simple Forecasting Process Step 1: Problem definition. Often this is the most difficult part of forecasting. Defining the problem carefully requires an understanding of the way the forecasts will be used, who requires the forecasts, and how the forecasting function fits within the organization requiring the forecasts. A forecaster needs to spend time talking to everyone who will be involved in collecting data, maintaining databases, and using the forecasts for future planning. Step 2: Gathering information. There are always at least two kinds of information required: (a) statistical data, and (b) the accumulated expertise of the people who collect the data and use the forecasts. Often, it will be difficult to obtain enough historical data to be able to fit a good statistical model. In that case judgmental forecasting can be used. Occasionally, old data will be less useful due to structural changes in the system being forecast; then we may choose to use only the most recent data. However, good statistical models will handle evolutionary changes in the system so that there is not necessarily a need to throw away good data. Step 3: Preliminary (exploratory) analysis. Always start by graphing the data. Are there consistent patterns? Is there a significant trend? Is seasonality important? Is there evidence of the presence of business cycles? Are there any outliers in the data that need to be explained by those with expert knowledge? Step 4: Choosing and fitting models. The best model to use depends on the availability of historical data and the way in which the forecasts are to be used. It is common to compare two or three potential models. Each model is itself an artificial construct that is based on a set of assumptions (explicit and implicit) and usually involves one or more parameters which must be estimated using the known historical data. However due to the focus of this section we focus on one statistical model and compare this with judgemental forecasts generated by the course participants. Step 5: Using and evaluating a forecasting model. Once a model has been selected and its parameters estimated, the model is used to make forecasts. The performance of the model can only be properly evaluated after the data for the forecast period have become available. Additionally the real accuracy of a method can only be assesed on data that is not incorporated in the modelling process; we therefore always test on unseen data. There are also organizational issues in using and acting on the forecasts. When using a forecasting model in practice, numerous practical issues arise such as how to handle missing values and outliers for which additional tools are needed. Source: The description of the forecasting process can be found in: Hyndman, R.J., &amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. "],["real-data.html", "9.3 Real Data", " 9.3 Real Data If the download does not work for some reason you can obtain the files manually from the course notes and use the function readRDS to load them into R. Description Download Link YT Subscriber Bibis Beatuy Palace DOWNLOAD YT Subscriber Tseries DOWNLOAD plot(tseries.subs$TotalSubscribers/10^6, x=tseries.subs$date, ylim=c(0,max(max(tseries.subs$TotalSubscribers), max(bibi.subs$TotalSubscribers))/10^6), ylab=&quot;Total Subscribers&quot;, xlab=&quot;Time&quot;, main=&quot;Total Subscribers on Youtube&quot;, type=&quot;l&quot;) mtext(&quot;Units in Million&quot;) grid(nx=0, ny=NULL) s &lt;- sapply(tseries.subs$date[grepl(tseries.subs$date,pattern = &quot;01-01&quot;)], function(x){abline(v=x,col=&quot;grey&quot;,lty=&quot;dotted&quot;)}) lines(x=bibi.subs$date, y=bibi.subs$TotalSubscribers/10^6,col=&quot;red&quot;) legend(&quot;topleft&quot;, legend=c(&quot;T-Series&quot;, &quot;Bibis Beauty Palace&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=1, cex=0.8, inset=.05) While both curves look fairly smooth the day-to-day changes for the last month show that the data fairly fluctuates over time with a roughly constant absolute growth. n &lt;- 31 t.diff &lt;- ts(diff(tail(tseries.subs$TotalSubscribers, n))) b.diff &lt;- ts(diff(tail(bibi.subs$TotalSubscribers, n))) par(mfrow=c(2,1),mar=c(3,4,2,1)) plot(y=t.diff, x=tail(tseries.subs$date,n-1), type=&quot;b&quot;,ylab=&quot;Subscribers&quot;, main=&quot;Day-to-Day Subscriber Growth&quot;) abline(h=0,lty=&quot;dotted&quot;,col=&quot;grey&quot;) mtext(&quot;T-Series&quot;) plot(y=b.diff, x=tail(bibi.subs$date,n-1), type=&quot;b&quot;,ylab=&quot;Subscribers&quot;, col=&quot;red&quot;) abline(h=0,lty=&quot;dotted&quot;,col=&quot;grey&quot;) mtext(&quot;Bibis Beauty Palace&quot;) "],["judgemental-forecasting.html", "9.4 Judgemental Forecasting", " 9.4 Judgemental Forecasting Forecasting using judgment is common in practice and in many cases the only feasible option. The major advantage is that judgemental (human-generated) forecasts can be obtained even though there is a complete lack of historical data, a new product is being launched, or a new competitor enters the market. Practically speaking one also often encounters situations where the dataset is incomplete or becomes available with huge delays (e.g. quarterly delays for macroeconomic data). Studies have shown that domain knowledge and actual data improve the quality of these human-generated forecasts. To have sufficient forecasts available we are now going to create a judgemental forecast that we compare an automatically generated one. Sheets and information for the generation of the judgemental forecasts are provided in the accompanying lecture and will be made available below. "],["exponential-smoothing.html", "9.5 Exponential Smoothing", " 9.5 Exponential Smoothing Exponential smoothing was proposed in the late 1950s (Brown, 1959; Holt, 1957; Winters, 1960), and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry. \\[ L_t = \\alpha \\cdot y_t + (1-\\alpha)\\cdot(L_{t-1}+T_{t-1}) \\\\ T_t = \\gamma \\cdot (L_t - L_{t-1}) + (1-\\gamma) \\cdot T_{t-1} \\] When all values are given and calculated one can easily generate forecast using the following forecasting function for future time horizons \\(h\\). \\[ f_t(h) = L_t + T_t \\cdot h \\] However as the dependence between a value and the previous value is recursive and can easily be seen from the formulas above, the values for the Level \\(L_t\\) and the Trend \\(T_t\\) reach at some point a time index of \\(t=0\\), which is obviously not observed. These values at \\(t=0\\) are called initial values and need to be set using an appropriate strategy. While there are many possible ways (including complicated procedures for estimating the initial values) a straight forward and sufficiently performant procedure to set the values is given by: \\[ L_0 = y_1 \\\\ T_0 = y_2 - y_1 \\] In order to generate a forecast we now already have some data and performed some explanatory analysis. To generate a forecasting model we now need to implement the method in R so that we do not have to calculate everything by hand. Afterward, we need to apply the function that we wrote and calculate all the level and trend values. However, in order to calculate these values, we need the smoothing constants \\(\\alpha\\) and \\(\\gamma\\). However, in practice, these are unknown, which is why we need to estimate them. Arguing in the same way as we do for regression, the chosen values for \\(\\alpha\\) and \\(\\gamma\\) should configure our exponential smoothing value in a way that it fits best to the data, which we can measure using an error measure such as the Mean Squared Error (MSE). After obtaining the best values for the smoothing constants, we can finally apply exponential smoothing to calculate level and trend values and afterward use the forecasting function to extrapolate the data for future horizons, which is called forecasting. 9.5.1 Implementing HES In order to have a simple example let us implement and illustrate the whole process using a very short and artificially generated time series instead of the long youtube subscribers series. This facilitates development and the explanation of the concepts. Our example time series consists of only four observations: yt &lt;- ts(c(195,198,200,203), frequency=1) yt #R&gt; Time Series: #R&gt; Start = 1 #R&gt; End = 4 #R&gt; Frequency = 1 #R&gt; [1] 195 198 200 203 Implementing Holt’s Exponential Smoothing Method can be done directly by translating the given equations into R code. These smoothing equations are applied iteratively. In addition to the Level and Trend \\[ y_t = \\hat{y}_t + \\hat{e}_t \\\\ \\hat{y}_t = \\hat{L}_t + \\hat{T}_t \\] holtExponentialSmoothing &lt;- function(yt, alpha, gamma){ # Setting initial Values l0 &lt;- yt[1] t0 &lt;- yt[2] - yt[1] # Intialize vectors for iterative calculations level &lt;- l0 trend &lt;- t0 # Calculate fit for (idx in 2:(length(yt)+1)){ level[idx] &lt;- alpha * yt[idx-1] + (1-alpha) * (level[idx-1] + trend[idx-1]) trend[idx] &lt;- gamma * (level[idx]-level[idx-1]) + (1-gamma) * trend[idx-1] } # Calculate and Return results fitted &lt;-(trend+level)[-length(trend)] residuals &lt;- yt - fitted return(list(fitted=fitted,residuals=residuals,yt=yt, trend=trend,level=level, alpha=alpha, gamma=gamma, l0=l0, t0=t0)) } 9.5.2 Finding best alpha and gamma values Following the line of arguments that we already discussed we are essentially searching for a MSE that we can calculate based on the residuals of a given model. \\[ MSE_{\\alpha,\\gamma} = \\frac{1}{T} \\sum_{i=1}^{T} \\hat{e}_i^2 \\] We are now looking for the combination of \\(\\alpha\\) and \\(\\gamma\\) that minimizes the MSE in dependence of the smoothing constants that we can configure. While there are many possible optimization routines available, most of them are algebraically complex and would need additional theoretical knowledge in order to understand and use them. Therefore, we are applying a more direct search routine, which is called grid search. Grid searching routines try a combination of plausible values and calculate the error measure of interest for all desired combinations of the values. From a computational perspective, this approach is not very efficient but works reasonably well for our purpose. The following code snippet performs this parameter search and returns over a parameter space with a comparably small resolution. By increasing the resolution the process takes considerably longer, but the considered parameters are more precise in return. resolution &lt;- 10 parameterspace &lt;- seq(0,1,1/resolution) mseMat &lt;- matrix(NA, nrow=length(parameterspace), ncol=length(parameterspace)) for(adx in parameterspace){ for(gdx in parameterspace){ rdx &lt;- round(adx*resolution + 1, 0) # Recalculate Row index cdx &lt;- round(gdx*resolution + 1, 0) # Recalculate Col index mod &lt;- holtExponentialSmoothing(yt=yt, alpha=adx, gamma=gdx) mseMat[rdx,cdx] &lt;- mean(mod$residuals^2) } } rownames(mseMat) &lt;- paste0(&quot;a=&quot;,parameterspace) colnames(mseMat) &lt;- paste0(&quot;g=&quot;,parameterspace) round(mseMat,3) #R&gt; g=0 g=0.1 g=0.2 g=0.3 g=0.4 g=0.5 g=0.6 g=0.7 g=0.8 g=0.9 #R&gt; a=0 12.500 12.500 12.500 12.500 12.500 12.500 12.500 12.500 12.500 12.500 #R&gt; a=0.1 9.396 8.965 8.557 8.171 7.805 7.460 7.134 6.827 6.538 6.266 #R&gt; a=0.2 7.186 6.581 6.044 5.568 5.150 4.784 4.467 4.195 3.964 3.769 #R&gt; a=0.3 5.625 4.997 4.479 4.059 3.724 3.464 3.269 3.128 3.035 2.980 #R&gt; a=0.4 4.531 3.958 3.527 3.214 2.999 2.863 2.790 2.765 2.777 2.815 #R&gt; a=0.5 3.770 3.285 2.959 2.757 2.649 2.610 2.620 2.664 2.727 2.799 #R&gt; a=0.6 3.245 2.857 2.630 2.520 2.491 2.515 2.572 2.646 2.725 2.805 #R&gt; a=0.7 2.892 2.594 2.452 2.413 2.440 2.504 2.587 2.679 2.772 2.866 #R&gt; a=0.8 2.666 2.451 2.380 2.397 2.467 2.564 2.677 2.798 2.929 3.072 #R&gt; a=0.9 2.540 2.401 2.393 2.464 2.579 2.721 2.884 3.069 3.282 3.532 #R&gt; a=1 2.500 2.435 2.493 2.624 2.803 3.020 3.277 3.584 3.958 4.421 #R&gt; g=1 #R&gt; a=0 12.500 #R&gt; a=0.1 6.011 #R&gt; a=0.2 3.609 #R&gt; a=0.3 2.958 #R&gt; a=0.4 2.872 #R&gt; a=0.5 2.875 #R&gt; a=0.6 2.881 #R&gt; a=0.7 2.962 #R&gt; a=0.8 3.233 #R&gt; a=0.9 3.830 #R&gt; a=1 5.000 The round when recalculating the row and column indices rdx and cdx ensures that we only get real indices and do not run into precision errors, when the resolution parameter is increased. Given the calculated Mean Square Error Values for all combinations, we can now select the best parameter combination, by filtering the parameterspace vector based using the MSE values. These values lead to the lowest MSE for the training set. They, therefore, represent the values that lead to the best adaption of the method to the data (as the error is minimized). We now assume that the parameter combination that creates the best fit to the data is also good when extrapolating the time series to future values and leads to small error values there as well. alpha.best &lt;- parameterspace[which.min(apply(mseMat, 1, min))] alpha.best #R&gt; [1] 0.8 gamma.best &lt;- parameterspace[which.min(apply(mseMat, 2, min))] gamma.best #R&gt; [1] 0.2 Write the function hesGridSearch that performs the paramter estimation conditional on the time-series yt and with a user defineable resolution argument. ## Solution to Excercise hesGridSearch &lt;- function(yt, resolution = 10){ parameterspace &lt;- seq(0,1,1/resolution) mseMat &lt;- matrix(NA, nrow=length(parameterspace), ncol=length(parameterspace)) for(adx in parameterspace){ for(gdx in parameterspace){ rdx &lt;- round(adx*resolution + 1, 0) # Recalculate Row index cdx &lt;- round(gdx*resolution + 1, 0) # Recalculate Col index mod &lt;- holtExponentialSmoothing(yt=yt, alpha=adx, gamma=gdx) mseMat[rdx,cdx] &lt;- sqrt(mean(mod$residuals^2)) } } rownames(mseMat) &lt;- paste0(&quot;a=&quot;,parameterspace) colnames(mseMat) &lt;- paste0(&quot;g=&quot;,parameterspace) alpha.best &lt;- parameterspace[which.min(apply(mseMat, 1, min))] gamma.best &lt;- parameterspace[which.min(apply(mseMat, 2, min))] return(c(alpha=alpha.best,gamma=gamma.best)) } REMARK: Please keep in mind that the examples in this course are designed to illustrate a simplified version of the forecasting process and to get an impression of a quite successful forecasting method. However, real-world applications are usually much more complex. While this is true for most methods themselves this also applies to the parameter estimation and optimization processes. From a practical perspective, all aspects of statistical learning such as bagging, boosting, cross-validation and constructing ensembles are also incorporated in the model building process but are not covered here. 9.5.3 Generating Forecasts The process of generating the actual forecasts conditional on the estimated parameters is now fairly easy. Given the forecasting formula above one only needs the last fitted level and last fitted trend which are combined to produce a linear forecast. The function forecastHES below additionally considers the time series properties (tsp) of the original time series in the model object (mod$yt) and changes the timeline of the forecast so that it can be later plotted conveniently in the same plot. params &lt;- hesGridSearch(yt, resolution=10) params # Parameters with lowest (in-sample) MSE #R&gt; alpha gamma #R&gt; 0.8 0.2 mod &lt;- holtExponentialSmoothing(yt=yt,alpha=params[1],gamma=params[2]) forecastHES &lt;- function(mod, H){ fc &lt;- tail(mod$level,1) + tail(mod$trend, 1) * 1:H fc &lt;- ts(fc,start=tsp(mod$yt)[2]+1, end=tsp(mod$yt)[2]+H,frequency = frequency(mod$yt)) return(fc) } fc &lt;- forecastHES(mod, H = 3) fc #R&gt; Time Series: #R&gt; Start = 5 #R&gt; End = 7 #R&gt; Frequency = 1 #R&gt; [1] 205.3977 207.8910 210.3844 plot(yt, ylim=c(190,215),xlim=c(1,7),type=&quot;b&quot;, main=&quot;Example Time Series and Forecast&quot;) lines(fc, col=&quot;red&quot;,type=&quot;b&quot;) grid() "],["exercises-8.html", "Exercises", " Exercises Use the defined functions and generate forecasts for either the bibi.subs or the tseries.subs data. "],["practical-application.html", "9.6 Practical Application", " 9.6 Practical Application We already looked at some real data and talked about the forecast objective and the respective problem definition (Step 1). By Scraping the data from the web we also completed the information gathering phase (Step 2) and conducted a short explanatory analysis by plotting the data (Step 3), discussing potential trends and looking at day-to-day subscriber changes. This leaves two remaining steps of the forecasting process for the practical application, namely Configure and Estimate Models (Step 4) and Generate and Evaluate Forecasts (Step 5). firstforecastdate &lt;- as.Date(&quot;2019-06-04&quot;) # Date of the last lecture bibi.train &lt;- bibi.subs[bibi.subs$date &lt; firstforecastdate, ] bibi.test &lt;- bibi.subs[bibi.subs$date &gt;= firstforecastdate, ] tseries.train &lt;- tseries.subs[tseries.subs$date &lt; firstforecastdate, ] tseries.test &lt;- tseries.subs[tseries.subs$date &gt;= firstforecastdate, ] Before we proceed and actually produce forecasts that then can be evaluated, we split the sample into two portions, a training set, and a test set. The training set is used to perform the modeling and estimate parameters and the test set contains data that we want to forecast and puts it aside for the evaluation. You can interpret the value of the firstforecastdate as the first value after a simulated or pretended forecast origin. Calculate H=8 forecasts for the youtube subscriber data using only the observations from the training set bibi.train. The actual forecast generation can now easily be performed in three distinctive steps. First, the time series needs to be constructed based on the downloaded subscriber data. Afterward, the best parameters for the exponential smoothing model can be determined (Tip: Increase the resolution to 100 or 1000 when estimating the parameters on your notebook). Finally, the eight step-ahead forecasts can be generated using the estimated parameters. # Load the data and convert to time series yt &lt;- ts(bibi.train$TotalSubscribers) # Find Parameters and mode data params &lt;- hesGridSearch(yt, resolution=10) mod &lt;- holtExponentialSmoothing(yt=yt,alpha=params[1], gamma=params[2]) # Calculate the Forecasts H &lt;- 8 fc &lt;- forecastHES(mod, H=H) fc #R&gt; Time Series: #R&gt; Start = 1075 #R&gt; End = 1082 #R&gt; Frequency = 1 #R&gt; [1] 5644250 5644680 5645110 5645539 5645969 5646399 5646829 5647259 The following plot based on the daily subscriber changes shows that the model fits the data fairly well. The black line and circles show the observed subscriber changes, while the red line shows how good the model with the best parameters, determined by grid search replicate the time series. plot(diff(mod$yt),type=&quot;b&quot;) lines(diff(mod$fitted),col=&quot;red&quot;) "],["forecast-evaluation.html", "9.7 Forecast Evaluation", " 9.7 Forecast Evaluation By translating the formula of the MSE into R code we receive a simple function that directly outputs the error measure based on a vector of predictions and observations. MSE &lt;- function(pred, obs){ return(mean((obs-pred)^2)) } After getting the respective observations from the test set (depending on when the data was downloaded the test set contains more ‘future’ observations and needs to be trimmed accordingly), we can, therefore, calculate the performance for Holt’s Exponential Smoothing Model. # Get the first H observations from the test set. obs &lt;- head(bibi.test$TotalSubscribers, H) mse.hes &lt;- MSE(pred=fc,obs=obs) # Calculate the out-of-sample MSE mse.hes #R&gt; [1] 62630.29 When analyzed alone the MSE is not interpretable. It can not be said directly if 6.263029210^{4} is a high or low value without a reference. However, technically speaking, most of the things that you learned in your undergraduate statistics course also applies to the MSE. Being a squared error measure it honors when deviations from the original data are small, while high differences are punished (by squaring obviously). The unit of the MSE shown is Subscribers\\(^2\\), which is not directly interpretable (and also not very meaningful). To get a feeling of how good the HES-Model actually performs, we compare it with the judgemental forecasts generated in the course. The corresponding .csv file contains all generated forecasts and can be downloaded here: Description Download Link Judgemental Forecasts SS2019 DOWNLOAD After downloading the respective file/s to your working directory you can load them into R using the function read.csv2. As .csv files contain only text, the dates in the file need to be converted manually to be proper dates. studentfc &lt;- read.csv2(&quot;fc2019.csv&quot;,stringsAsFactors = F) studentfc$date &lt;- as.Date(studentfc$date) Before we formally evaluate the forecasts it is always a good idea to graphically analyze the performance and see if the forecasted values correspond to our intuition. We, therefore, select the last few values from the observations (up to the pretended forecast origin) and extend the plot by the student forecasts (red), the HES forecasts (blue) and the real observed values (green). y &lt;- tail(bibi.train$TotalSubscribers,20) x &lt;- 1:(length(y)+H) plot(x=x,y=c(y,rep(NA,H)), ylim=c(min(y), max(studentfc[,-c(1,2)])), type=&quot;l&quot;, xlab=&quot;&quot;, ylab=&quot;Subscribers&quot;) x.fc &lt;- (length(y)+1):(length(y)+H) invisible(apply(studentfc[,-1], 2, lines, col=&quot;red&quot;, type=&quot;l&quot;, x=x.fc)) lines(y=obs,x=x.fc,col=&quot;green&quot;,type=&quot;b&quot;) # Actual amount of subscribers lines(y=fc,x=x.fc,col=&quot;blue&quot;,type=&quot;l&quot;) # Forecast generated by HES The plot shows that the majority of the student forecasts pick up the general momentum of the series and present reasonable extensions. Outliers in these judgemental forecasts are present and some forecasts seem to be substantially lower than the series. The HES forecast is (by construction) linear, while this is not necessarily the case for judgemental forecasts. However, in addition to some of the student forecasts the HES forecast is relatively close to the observed values. To get a more precise and objectified evaluation we calculate the MSE for all forecasts. mse.all &lt;- c(hes=mse.hes,apply(studentfc[,-1],2,MSE,obs=obs)) mse.all #R&gt; hes ggs16494 wws18129 sls16662 wws16618 wws22365 #R&gt; 6.263029e+04 2.799490e+13 5.429494e+05 5.207438e+05 6.632625e+07 3.414909e+06 #R&gt; wws20866 wws22320 wws22358 wws19675 wws22321 wws22376 #R&gt; 7.469459e+06 2.641179e+05 7.016785e+05 1.343939e+06 5.863438e+04 5.669594e+05 #R&gt; wws22337 wws18345 #R&gt; 8.159274e+05 7.731969e+05 The scientific notation above makes it a little complicated to read the output and determine a winner, so let us add the rank and sort the output before putting it in a table. This makes it easy to see that the best forecast is produced by wws22321. The difference in MSE between the best student-generated forecast and the HES forecast is 3995.917215. RANK MSE wws22321 1 58634.38 hes 2 62630.29 wws22320 3 264117.88 sls16662 4 520743.75 wws18129 5 542949.38 wws22376 6 566959.38 wws22358 7 701678.50 wws18345 8 773196.88 wws22337 9 815927.38 wws19675 10 1343939.38 wws22365 11 3414909.38 wws20866 12 7469459.38 wws16618 13 66326253.00 ggs16494 14 27994904294847.75 "],["web-apps.html", "Chapter 10 Web Apps", " Chapter 10 Web Apps To illustrate how web apps work we choose to visualize some well known, but for most people not very intuitive properties of the sample mean \\(\\bar{X}\\). While it is quite obvious that the expected value of the sample mean \\(E(\\bar{X}) = E(\\frac{1}{n}\\sum_{i=1}^{n}X_i) = \\mu\\) is equal to the expected value of \\(X_i\\), their variances differ. Let us illustrate these properties using a plot. First, we are going to generate 1000 random draws \\(X_i\\) from a standard normal distribution \\(N(\\mu=0,\\sigma^2=1)\\) and plot a histogram to exemplify their probability density. We calculate the sample mean of five consecutive draws (n &lt;- 5) and compare the probability density of these 200 (N) calculated means to the probability density of the initial draws. In order to compare the differences more clearly, we are also adding a density line for the empirical normal distribution. # Make results comparable set.seed(100) # Draw random numbers and calculate N means based on n draws n &lt;- 5 N &lt;- 200 rngmat &lt;- matrix(rnorm(n*N), nrow = n, ncol=N) meanvec &lt;- apply(rngmat, 2, mean) # Plot histograms MASS::truehist(rngmat, col=rgb(0,0,1,1/6),xlim=c(-5,5),ylim=c(0,1),xlab=&quot;&quot;) par(new=T) # Necessary to combine two histograms in one plot MASS::truehist(meanvec,col=rgb(1,0,0,1/4),xlim=c(-5,5),ylim=c(0,1),xlab=&quot;&quot;) # Add empirical probability density as line xseq &lt;- seq(-5,5,length.out = 1000) y &lt;- sapply(xseq, function(x){dnorm(x, mean(rngmat),sd(rngmat))}) ymean &lt;- sapply(xseq, function(x){dnorm(x, mean(meanvec),sd(meanvec))}) lines(xseq, ymean, type=&quot;l&quot;,col=&quot;red&quot;) lines(xseq, y, type=&quot;l&quot;, col=&quot;blue&quot;) Obviously, the shown plot is dependent on the number of draws that are used for every calculation of a mean n and the total number of means that we want to calculate N. Additionally, we can make the user decide whether he wants the density lines in the plot and if the random numbers are drawn using a seeded random number generator for which a seed can be provided. Every time one of these four inputs changes the plot needs to be recreated. A very convenient way to make our small program accessible by users that have little or no experience using R, we can transform our script into a web app using the shiny-package. A shiny app can be easily build using three simple components: a page constructor, a server script, and app constructor. The page constructor creates the user interface meaning it provides a mechanism for users to input values for variables and displays results. The server script takes the inputs from the user interface, performs calculations and returns the output for the user interface. The app constructor links the user interface and the server script and coordinates everything. The general structure of a shiny app in the required form is as shown in the following snippet. library(shiny) # Define UI for application that draws a histogram ui &lt;- fluidPage( titlePanel(&quot;Title&quot;), sidebarLayout( sidebarPanel( ## Input ), mainPanel( ## Output ) ) ) server &lt;- function(input, output) { ## Actual R Script } shinyApp(ui = ui, server = server) # Shiny Example for the CSWR Course library(shiny) # Define UI for application that draws a histogram ui &lt;- fluidPage( # Application title titlePanel(&quot;Random Samples and their Mean&quot;), # Sidebar with a slider input for number of bins sidebarLayout( sidebarPanel( sliderInput(&quot;N&quot;, &quot;Number of samples:&quot;, min = 10, max = 100, value = 50), sliderInput(&quot;n&quot;, &quot;Number of draws per sample:&quot;, min = 1, max = 100, value = 1), checkboxInput(&quot;showdensity&quot;, &quot;Show Density?&quot;, value = FALSE), numericInput(&quot;seed&quot;, &quot;Seed&quot;, value=NULL, min = 0, max = 10000, step = 1) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) ) # Define server logic required to draw a histogram server &lt;- function(input, output) { output$distPlot &lt;- renderPlot({ if(!is.na(input$seed)) set.seed(input$seed) rngmat &lt;- matrix(rnorm(input$n*input$N), nrow = input$n, ncol=input$N) meanvec &lt;- apply(rngmat, 2, mean) xseq &lt;- seq(-5,5,length.out = 1000) y &lt;- sapply(xseq, function(x){dnorm(x, mean(rngmat),sd(rngmat))}) ymean &lt;- sapply(xseq, function(x){dnorm(x, mean(meanvec),sd(meanvec))}) ymax &lt;- ceiling(max(c(y,ymean))) ymax &lt;- ifelse(is.finite(ymax),ymax, 1) MASS::truehist(rngmat, col=rgb(0,0,1,1/6),xlim=c(-5,5),ylim=c(0,ymax),xlab=&quot;&quot;) if(input$showdensity) lines(xseq,y,type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2) par(new=T) MASS::truehist(meanvec,col=rgb(1,0,0,1/4),xlim=c(-5,5),ylim=c(0,ymax),xlab=&quot;&quot;) if(input$showdensity) lines(xseq,ymean,type=&quot;l&quot;,col=&quot;red&quot;,lwd=2) }) } # Run the application shinyApp(ui = ui, server = server) "],["homework-assignment.html", "Homework Assignment", " Homework Assignment Please hand in the answers to the following exercises by uploading a PDF in Ilias before the deadline expires. The PDF should contain solutions, explanations and the code itself and needs to be produced using RMarkdown. This Homework Assignment is an individual task and passing is required to complete the course. The deadline for submitting your solutions over Ilias is 24. April 2020 - 9:00 am. 1. Simple Calculations (10%) \\[ 2^{15} ,\\quad \\frac{20}{\\sqrt{\\pi}} ,\\quad {10 \\choose 5} ,\\quad \\ln{\\frac{2+4}{4+3}} ,\\quad \\sqrt[3]{-27},\\quad \\sin(270^\\circ) \\] \\[ \\log_5(6) ,\\quad \\sum_{i=0}^{5}{5} ,\\quad \\sum_{i=3}^{5}{i \\; !} ,\\quad \\prod_{i=1}^{107}{i},\\quad 1-0.000000000000000001 \\] How many digits does the result (overall number, not only after decimal point) from the second last calculation (Product) have? Is the result from the last calculation identical to \\(1-0.000000000000000001\\) ? Why or why not and what are the implications? 2. Poker Game (40%) You have a regular set of poker cards. The order of the face cards is as follows (numeric value of card in parantheses): \\[ 2 &lt; 3 &lt; \\ldots &lt; 9 &lt; 10 &lt; \\text{Jack}\\,(11) &lt; \\text{Queen}\\,(12) &lt; \\text{King}\\,(13) &lt; \\text{Ace}\\,(14)\\] The order of the colors is: \\[ \\text{Diamonds}\\,\\diamondsuit &lt; \\text{Clubs}\\,\\clubsuit &lt; \\text{Hearts}\\,\\heartsuit &lt; \\text{Spades} \\,\\spadesuit\\] Write a function poker(), where each of the two players receives five cards of the described deck. The player with the highest card wins the game. Your function should return the winner of the game only. Hint: It may be easier to use the values instead of the names of the face cards. Manipulate your function from 1. and create the function poker_p1() so that Player 1 has a 60% chance of getting a King and therefore wins more often. Make yourself familiar with the function replicate(). Apply your knowledge to the original and manipulated function and play the implemented games one million times. Visually prove that Player 1 has a higher chance of winning when using function poker_p1() using a suitable visualization. Include the function name and your matriculation number in the main title of your plots. OLS Regression Within R we can easily fit regression models using the lm() function, which not only gives us the plain coefficients but a lot of additional output. Please write a function my.lm() that performs a linear regression and returns results in the form of summary(lm(y~1+x1+x2+...)). When writing functions, please document all steps in detail to show your procedure and make your programming comprehensible. Before you start, make sure that you are familiar with the associated concepts and provide correct interpretations of your results. Implement your function my.lm(). The following steps are the cornerstones of the implementation and may be your guideline while developing the function. Estimate the regression coefficients. Calculate the variance of the estimated coefficients. Perform hypothesis tests to check whether the calculated coefficients are significant, outputting \\(t\\)-value as well as \\(p\\)-value. Calculate additional metrics that are included in the output like \\(R^2\\), \\(R_{adj}^2\\) and the summary statistics of the residuals. Perform the F-Test and calculate the necessary elements to interpret the results. Wrap everything in a function that uses a vector \\(y\\) and a matrix or vector \\(x\\) as input. Make the necessary parameters adjustable for the user and format the output so that it looks like the one from summary(lm()). Hint 1: You do not need to support the formula notation (~) in your function. Hint 2: If you are not totally sure about interpreting results, you should consult a textbook. A good one to start is Regression Analysis by Example from Chatterjee and Hadi. To prove that your regression function works corretcly, compare it to an output generated by lm() with at least 3 regressors (e.g. using the data from the wooldridge::wage1 dataset) and interpret the resulting coefficients 4. Bug Bounty (5%) Participate in the bug bounty program, either by submitting an error found in the available course notes or by suggesting a substantial improvement to the course notes. "],["programming-project.html", "Programming Project", " Programming Project Please chose one of the provided topics and hand in the answers to the programming project by uploading a PDF in Ilias before the deadline expires. The PDF should contain the theoretical background, the complete and documented implementation, explanations and all code (in the appendix) that is required to reproduce your results. The PDF needs to be produced using RMarkdown. This Project is an individual task, will be graded and determines your final grade. The deadline for submitting your solution is 21. June 2020 - 23:59. "],["topic-1-web-scraping.html", "Topic 1: Web Scraping", " Topic 1: Web Scraping Data scraping, data harvesting, or data extraction is the process of extracting information from websites in an automated fashion. The program that downloads the content and extracts the desired piece of information is usually referred to as bot or crawler. The scraping process consists of two parts: fetching and extracting. The first step, fetching, is the downloading of a page. Therefore, web crawling is the main component of web scraping, to fetch pages for later extraction and processing. When a page has been downloaded (and saved) the second step, the information extraction can take place. This involves taking some parts out of the downloaded page and prepare it in a way that it can be used for another purpose, e.g. an analysis. The goal of this task is to scrape data such as cryptocurrency time series, job descriptions or car offers from a given website and calculate, present as well as interpret some descriptive statistics in an appealing way (for example using Shiny). The key learning is to develop a crawling algorithm that is capable of downloading content from the given site. Each of the following websites can be used only once in the course. If you are interested in a listed website please send me a message (benjamin.buchwitz@ku.de) stating the website and your identifier/matriculation number. Websites are assigned on a first-come-first-serve basis. However, you can also propose a website, that will then be added to the list. When coding please keep in mind that you can and should not crawl the entire content from your chosen page. Please limit the number of crawled instances to an absolute maximum of 100.000 entries (less is also ok - if in doubt talk to me). Site Type Student https://guide.michelin.co.jp/ Restaurants wws22603 https://www.rottentomatoes.com/ Movies wws23848 https://www.stepstone.de/ Jobs https://www.mobile.de/ Car / Motorbike https://www.kickstarter.com/ Crowdfunding Getting started Literature: Simon Munzert, Christian Rubba, Peter Meißner, and Dominic Nyhuis. 2014. Automated Data Collection with R. A Practical Guide to Web Scraping and Text Mining. Chichester: John Wiley &amp; Sons. Accompanying website: http://r-datacollection.com/. "],["topic-2-forecasting.html", "Topic 2: Forecasting", " Topic 2: Forecasting In autoregression models, the variable of interest is forecasted using a linear combination of past values of the variable itself. The term autoregression indicates that it is a regression of the variable against itself. The \\(AR(p)\\)-Model for a time series \\(\\{y_t\\}\\) is given by: \\[ y_t = c + \\phi_1 \\cdot y_{t-1} + \\ldots + \\phi_p \\cdot y_{t-p} + \\epsilon_t\\] If \\(\\{y_t\\}\\) is instationary it may be preferable to estimate the model on the basis of the first differences of the time series which is given by \\(z_t = y_t - y_{t-1}\\) (details in the referenced literature). The goal of this task is to compare the performance of the \\(AR(p)\\) model on the 100.000 Series of the M4 Time Series Dataset. Therefore it is required to develop a function that estimates a model of a given order and calculates a required amount of forecasts. These forecasts should be compared against the real data from the M4 holdout by means of the Mean Absolute Scaled Error (MASE). Please report results for stationary and instationary \\(AR(p=1)\\) to \\(AR(p=10)\\) models (20 models in total) for the whole M4 dataset. Additionally you can also include an stationary and instationary \\(AR(p=0)\\) model to model the data as a simple and trivial benchmark for the more compelx models (yielding 22 models in total). Dataset: M4 Competition Website: https://www.mcompetitions.unic.ac.cy/the-dataset/ Getting started Literature: Hyndman, R.J., &amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. Accompanying website: http://OTexts.com/fpp2. "],["topic-3-algorithm-implementation-data-analysis.html", "Topic 3: Algorithm Implementation &amp; Data Analysis", " Topic 3: Algorithm Implementation &amp; Data Analysis This topic is a more flexible choice. The basic idea is to select a dataset and a corresponding method to model and predict the outputs. The Dataset and the methodology is free to choose (in consultation with me via benjamin.buchwitz@ku.de). The goal of this task is to implement the method on your own (not using a package) and develop a deeper understanding of the method while performing an analysis with the selected dataset. I strongly recommend choosing a simple method which is often challenging (e.g. simple regression tree for Boston Housing or simple classification tree for Twitter Classification). Each dataset method pair can only be chosen once and the task is allocated on a first-come-first-serve basis. Dataset Methodology Student Red Wine Quality tbd wws16885 Twitter Classification Dataset tbd ww Boston Housing Dataset Regression Tree Getting started Literature: Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer. "],["topic-4-custom-topic.html", "Topic 4: Custom Topic", " Topic 4: Custom Topic You are still free to suggest a custom topic that you are interested in. If you want to do so please reach out to me to clarify the details and get you listed below. Project Description Student COVID-19 Visualization + Forecasting (https://data.europa.eu) wws22488 "],["bug-bounty.html", "A Bug Bounty", " A Bug Bounty "],["leaderboard.html", "Leaderboard", " Leaderboard Name Number of Submissions Credit (BC) wws22593 11 \\(11 \\cdot2^{0}\\) wws22547 7 \\(7 \\cdot2^{0}\\) wws18024 5 \\(5 \\cdot2^{0}\\) mgs15616 2 \\(4 \\cdot2^{0}\\) wws22554 3 \\(3 \\cdot2^{0}\\) wws15344 2 \\(2 \\cdot2^{0}\\) wws21073 1 \\(1 \\cdot2^{0}\\) wws16924 1 \\(1 \\cdot2^{0}\\) wws22488 1 \\(1 \\cdot2^{0}\\) "],["what-is-this.html", "What is this?", " What is this? A bug bounty program is a crowdsourcing initiative that rewards individuals for discovering and reporting errors. It is frequently used by big companies such as Google, Facebook or Microsoft to attract hackers and security researchers to test their websites and platforms for security vulnerabilities. However, this bug bounty program here is inspired by the one initiated by Donald E. Knuth, where he rewards people when they find errors in his books. Every course and class member is encouraged to participate in the CSWR Bug Bounty program and report typographic errors, code problems or logic flaws in this course websites. Reporting an error is easy and can be done by sending a screenshot and a brief explanation of the error via email to benjamin.buchwitz@ku.de. The person who first submits an error receives the reward, that ranges from 2^0 to 2^7 Bug Credits (BC) and will be added to the leaderboard. Typographic errors are always rewarded with 2^0 BC each. Bugs in the R code or flaws in algorithmic logic will be rewarded with respect to their severity and complexity and can be rewarded with up to 2^7 BC. Reported errors will be corrected within 48h so that the content of this site improves continuously. "],["test-exam.html", "B Test Exam", " B Test Exam Description Download Link Test Exam SS 2019 DOWNLOAD "],["additional-exercises.html", "B.1 Additional Exercises", " B.1 Additional Exercises B.1.1 Exercise I The following function makemat was found in the file function.final.final2.R as part of an old undocumented project. Therefore its purpose is unknown. Being a young and ambitious R developer you were asked to answer the following questions. set.seed(124) num &lt;- round(runif(9, min = 0, max = 0.6), 2) num #R&gt; [1] 0.05 0.25 0.31 0.24 0.13 0.18 0.35 0.29 0.55 kat &lt;- c(1, 2, 3) val &lt;- c(0.2, 0.4, 0.6) mat &lt;- matrix(num, ncol = 3, nrow = 3) for ( i in 1:prod(dim(mat)) ){ for ( j in val){ state &lt;- (mat[i] &lt; j) if (state){ mat[i] &lt;- kat[j == val] } } } mat[(mat % 2) == 1] &lt;- 1 mat While executing and trying to load the function above the following message appears. Explain why the message is returned and correct the error in the provided source code. Error: unexpected input in \"mat[(mat % 2) == 1] &lt;- 1\" Make yourself familiar with the provided code and briefly describe in a generic way what the code does (line-by-line, one full iteration of the outer loop). Ensure that your description is short but precise and allows the reader to gain a deep understanding of the data manipulation process. Write down the content of the variable mat after fixing the error and executing the code above. "],["exercise-ii.html", "B.2 Exercise II", " B.2 Exercise II The following three code snippets were found in the file misc.support.final.final2.R as part of an old undocumented project. Therefore their purpose is unknown. Being a young and ambitious R developer you were asked to explain the mysterious content of the following three snippets. ### CODE SNIPPET 1 x &lt;- 100 counter &lt;- 0 while( x &gt; 0){ x &lt;- x/2 counter &lt;- counter + 1 } counter Briefly describe what the code snippet above does (line-by-line). Please give your opinion on the provided code and comment on time of execution. ### CODE SNIPPET 2 res &lt;- list() for (i in 1:10){ res[[i]] &lt;- sample(seq(from=0,to=1,by=0.2)) } sum(sapply(res,FUN = sum)) length(res) typeof(res[[1]][1]) Please write down what the last three lines of code snippet 2 return when executing the code above. ### CODE SNIPPET 3 a &lt;- array(sort(rep(1:3,4)),dim=c(2,2,3)) for ( i in 1:dim(a)[1] ){ for ( j in 1:dim(a)[2] ){ for ( k in 1:dim(a)[3] ){ a[i,j,k] &lt;- ifelse(a[i,j,k] %% 2, a[i,j,k], NA) } } } a Please write down the content of the variable a when snippet 3 is executed. B.2.1 Exercise III The following function genlist was found in the file list-maker.final.final2.R as part of an old undocumented project. Therefore its purpose is not known. Being a young and ambitious R developer you were asked to answer the following questions. genlist &lt;- function(x=sample(x=1:5, size=5)){ n &lt;- 0 for (i in (1:length(x))){ if (x[i] % 2 == 1){ n &lt;- n + 1 } } res &lt;- list(n=n, x=x, y=rev(x), z=sort(x)) return(res) } While executing and trying to load the function above the following message appears. Explain why the message is returned and correct the error in the provided source code. Error: unexpected input in: \" for (i in (1:length(x))){ if (x[i] % 2 == 1){\" Briefly describe what the function genlist does (line-by-line) and returns. Ensure that your description is precise and allows the reader to gain a deep understanding of how the values in the returned list are generated. Provide the results to the following function call based on your corrected code. genlist() genlist(1:7) Please come up with a single line of code that returns the type of each list element. Name the respective datatypes and answer the question if these can be different from values returned by class. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
