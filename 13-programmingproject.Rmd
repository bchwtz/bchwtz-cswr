# Programming Project {-}

```{r, echo=FALSE, fig.cap=NULL, out.width="100%"}
knitr::include_graphics("gfx/CH00-ProgrammingProject.png")
```


```{block2, type='rmdexercise'}
Please chose **one** of the provided topics and hand in the answers to the programming project by uploading a PDF in Ilias before the deadline expires. The PDF should contain the theoretical background, the complete and documented implementation, explanations and all code (in the appendix) that is required to reproduce your results. The PDF needs to be produced using RMarkdown. This Project is an **individual** task, will be graded and determines your final grade. The deadline for submitting your solution is 21. June 2020 - 23:59.

```


## Topic 1: Web Scraping {-}
Data scraping, data harvesting, or data extraction is the process of extracting information from websites in an automated fashion. The program that downloads the content and extracts the desired piece of information is usually referred to as bot or crawler. The scraping process consists of two parts: fetching and extracting. The first step, fetching, is the downloading of a page. Therefore, web crawling is the main component of web scraping, to fetch pages for later extraction and processing. When a page has been downloaded (and saved) the second step, the information extraction can take place. This involves taking some parts out of the downloaded page and prepare it in a way that it can be used for another purpose, e.g. an analysis.

The goal of this task is to scrape data such as cryptocurrency time series, job descriptions or car offers from a given website and calculate, present as well as interpret some descriptive statistics in an appealing way (for example using Shiny). The key learning is to develop a crawling algorithm that is capable of downloading content from the given site. Each of the following websites can be used only once in the course. If you are interested in a listed website please send me a message (benjamin.buchwitz@ku.de) stating the website and your identifier/matriculation number. Websites are assigned on a first-come-first-serve basis. However, you can also propose a website, that will then be added to the list. When coding please keep in mind that you can and should not crawl the entire content from your chosen page. Please limit the number of crawled instances to an absolute maximum of 100.000 entries (less is also ok - if in doubt talk to me).

| Site                              | Type                     | Student       | 
|:----------------------------------|:-------------------------|:-------------:|
| https://guide.michelin.co.jp/     | Restaurants              | wws22603      |
| https://www.rottentomatoes.com/   | Movies                   | wws23848      |
| https://www.stepstone.de/         | Jobs                     |       |
| https://www.mobile.de/            | Car / Motorbike          |       |
| https://www.kickstarter.com/      | Crowdfunding             |       |


<!--
**Key Steps:**

- Crawler-Function that downloads and saves the webpages in HTML format
- Scraper-Function that extracts information from the saved files
- Condense scraped information in an adequate data structure
- Calculate, present and interpret descriptive statistics
-->

**Getting started Literature:**

Simon Munzert, Christian Rubba, Peter Meißner, and Dominic Nyhuis. 2014. Automated Data Collection with R. A Practical Guide to Web Scraping and Text Mining. Chichester: John Wiley & Sons. Accompanying website: http://r-datacollection.com/.

---

## Topic 2: Forecasting {-}
In autoregression models, the variable of interest is forecasted using a linear combination of past values of the variable itself. The term autoregression indicates that it is a regression of the variable against itself. The $AR(p)$-Model for a time series $\{y_t\}$ is given by:

$$ y_t = c + \phi_1 \cdot y_{t-1} + \ldots + \phi_p \cdot y_{t-p} + \epsilon_t$$
If $\{y_t\}$ is instationary it may be preferable to estimate the model on the basis of the first differences of the time series which is given by $z_t = y_t - y_{t-1}$ (details in the referenced literature). The goal of this task is to compare the performance of the $AR(p)$ model on the 100.000 Series of the M4 Time Series Dataset. Therefore it is required to develop a function that estimates a model of a given order and calculates a required amount of forecasts. These forecasts should be compared against the real data from the M4 holdout by means of the Mean Absolute Scaled Error (MASE). Please report results for stationary and instationary $AR(p=1)$ to $AR(p=10)$ models (20 models in total) for the whole M4 dataset. Additionally you can also include an stationary and instationary $AR(p=0)$ model to model the data as a simple and trivial benchmark for the more compelx models (yielding 22 models in total).

<!--
**Key Steps:**

- Function that performs model estimation and generates forecasts for one specified model
- Forecasts for all 100.000 time series and for all models
- Evaluate and present accuracy for the forecasts in terms of MASE by model and yearly, quarterly, monthly, weekly, daily and hourly frequencies
-->

**Dataset:**

M4 Competition Website: https://www.mcompetitions.unic.ac.cy/the-dataset/

**Getting started Literature:**

Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. Accompanying website: http://OTexts.com/fpp2.

<!--
---

## Topic 3: Robust Standard Errors {-}
The assumption of homoskedasticity often does not hold when analyzing real-world datasets. As a consequence, the OLS estimator for the regression standard error is not consistent anymore. Therefore, a number of various different estimators have been proposed to overcome this problem. The idea of this task is to understand the implications of heteroskedasticity (only not in conjunction with autocorrelation) on the regression results and explore pathways that have been proposed to practically deal with the issue. 

**Key Steps:**

- Simulate multiple datasets (1 to 10 exogenous variable ) that violate the assumption of homoskedasticity
- Explain one concept to adjust standard errors to obtain a heteroskedasticity-consistent standard error estimate (e.g White Estimator; see recommended literature).
- Function that calculates adapted variance-covariance matrix on the basis of normal regression (alternatively you can write a complete regression function all by yourself)
- Compare, illustrate and interpret the standard and corrected regression results

**Getting started Literature:**

Zeileis, A. (2004). Econometric Computing with HC and HAC Covariance Matrix Estimators. Journal of Statistical Software, 11(10), 1 - 17. doi:http://dx.doi.org/10.18637/jss.v011.i10

---

## Topic 4: Backpropagation Algorithm {-}
Backpropagation is short for “backward propagation of errors” and is a mechanism used to update the weights of a neural network using gradient descent. The main idea is to calculate the gradient of the error function with respect to the neural network's weights while proceeding backward through the network. The goal of this task is to implement a simple and exemplarily illustrated version of the backpropagation algorithm for a simple feed-forward neural network and present the results of weight adaptions under different learning rates.

**Key Steps:**

- Understand Feed Forward Neural Network
- Understand the Idea of Backpropagation
- Create a simple Example Network
- Illustrate Backpropagation Step by Step for one fixed Learning Rate
- Illustrate Results when variating Learning Rate

**Getting started Literature:**

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press. Accompanying website: https://www.deeplearningbook.org/.

---

## Topic 5: Safe Withdrawal Simulation {-}
The question of safe withdrawal rates in retirement portfolios has become an active field of research in finance. The basic concept is as follows: After reaching retirement age, a person starts to withdraw money from the personal wealth portfolio in order to maintain her/his standard of living. However, the retiree should calculate which percentage of the wealth portfolio can be withdrawn without risking to run out of money or the necessity to cut down the standard of living in a later stage. To model this process retirees mainly need three parameters: 

- Length of retirement period (As we don't want to underestimate life expectancy we can assume a fixed value e.g. 30 years, however, life expectancy is a stochastic variable that can also be modeled based on mortality tables)
- Yearly portfolio returns (to model the portfolio development without withdrawals) 
- Yearly inflation rate (to model the necessary changes of withdrawals over the time span in order to remain the standard of living) 

One way to model these parameters are simulation/bootstrapping approaches. The idea behind these approaches is to simulate the withdrawal process several times while incorporating the uncertainty of the variables in the analysis. Given the portfolio return and inflation rate for every year of a multi-decade period, bootstrapping consists of sampling values for the parameters for each year (with replacement) in order to model the portfolio development during the retirement period. The portfolio succeeded if the portfolio value is never smaller than zero, as it was possible to maintain the real value of the withdrawals without running out of money. If the portfolio value falls below zero, the portfolio failed, as the anticipated withdrawals haven't been possible. Reproducing/simulating this process $B$ times for different starting withdrawals allows approximating the fraction of scenarios in which the portfolio succeeds and fails. Thereby, it is possible to observe the (obviously) inverse relationship between initial withdrawal rate and portfolio success probability in detail, which can be described and explained in detail.

**Key Steps:**

- Construct a portfolio of 50% stocks (DAX) and 50% riskless assets (government bonds with 1-year maturity).
- Making justified assumptions about parameter values and distributions for all uncertain values.
- Writing a function that simulates if the portfolio is successful, given the starting values.
- Replicate the simulation using at least $B=1.000.000$
- Present results (textually and graphically) and conclude.

**Getting started Literature:**

Bengen, W. P. (1994): Determining withdrawal rates using historical data. Journal of Financial Planning, Vol. 7, pp. 171-180.

Spitzer, J. J. et al. (2007): Guidelines for withdrawal rates and portfolio safety during retirement. Journal of Financial Planning, Vol. 20(10), pp. 52-59.

Cooley, P. L. (2003): A comparative analysis of retirement portfolio success rates: Simulation vs. overlapping periods. Financial Services Review, Vol. 12, pp. 115-128.

-->

## Topic 3: Algorithm Implementation & Data Analysis {-}
This topic is a more flexible choice. The basic idea is to select a dataset and a corresponding method to model and predict the outputs. The Dataset and the methodology is free to choose (in consultation with me via benjamin.buchwitz@ku.de). The goal of this task is to implement the method on your own (not using a package) and develop a deeper understanding of the method while performing an analysis with the selected dataset. I strongly recommend choosing a simple method which is often challenging (e.g. simple regression tree for Boston Housing or simple classification tree for Twitter Classification). Each dataset method pair can only be chosen once and the task is allocated on a first-come-first-serve basis.

| Dataset                           | Methodology              | Student       | 
|:----------------------------------|:-------------------------|:-------------:| 
| Red Wine Quality                  | tbd                      | wws16885      |
| Twitter Classification Dataset    | tbd                      | ww           |
| Boston Housing Dataset            | Regression Tree          |      |

<!--
**Key Steps:**

- Select an appropriate simple method
- Understand how to evaluate a prediction generated by that method
- Understand and implement the method in your own R function
- Split the dataset (80:20) and estimate the model on the large part (training 80% of data); use that model to predict outcomes for the smaller test part (20% of data).
- Report and interpret results
-->

**Getting started Literature:**

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.

## Topic 4: Custom Topic {-}
You are still free to suggest a custom topic that you are interested in. If you want to do so please reach out to me to clarify the details and get you listed below.

| Project Description                                          | Student       | 
|:-------------------------------------------------------------|--------------:| 
| COVID-19 Visualization + Forecasting (https://data.europa.eu)| wws22488      |
